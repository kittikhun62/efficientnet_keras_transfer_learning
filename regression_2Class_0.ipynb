{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPp7qjVBZ4vxkG/CHHT+f2T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/regression_2Class_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a13be2b9-3858-4f0f-b06e-c0926e29c803"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - 2 class เพิ่ม 4 paper.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "outputId": "48fbd7dc-6311-47a3-bf12-0aebe9064e9e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "825  826  1-s2.0-S2095268622000210-main   \n",
              "826  827  1-s2.0-S2095268622000210-main   \n",
              "827  828  1-s2.0-S2095268622000210-main   \n",
              "828  829  1-s2.0-S2095268622000210-main   \n",
              "829  830  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "825  Integration of preparation of K, Na-embedded a...   \n",
              "826  Integration of preparation of K, Na-embedded a...   \n",
              "827  Integration of preparation of K, Na-embedded a...   \n",
              "828  Integration of preparation of K, Na-embedded a...   \n",
              "829  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "825  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "826  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "827  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "828  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "829  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-800  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-800  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-800  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-800  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-800  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "825  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-800  301.70   \n",
              "826  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-800  301.70   \n",
              "827  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-800  301.70   \n",
              "828  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-800  301.70   \n",
              "829  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-800  301.70   \n",
              "\n",
              "     Size(mico)  Class_01  \n",
              "0            10         0  \n",
              "1            10         0  \n",
              "2            10         0  \n",
              "3            10         0  \n",
              "4            10         0  \n",
              "..          ...       ...  \n",
              "825          10         0  \n",
              "826          10         0  \n",
              "827          10         0  \n",
              "828          10         0  \n",
              "829          10         0  \n",
              "\n",
              "[830 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-634c06d1-0af7-4964-b715-50d5c720babd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "      <th>Class_01</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>825</th>\n",
              "      <td>826</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>826</th>\n",
              "      <td>827</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>827</th>\n",
              "      <td>828</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>828</th>\n",
              "      <td>829</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>829</th>\n",
              "      <td>830</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>830 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-634c06d1-0af7-4964-b715-50d5c720babd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-634c06d1-0af7-4964-b715-50d5c720babd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-634c06d1-0af7-4964-b715-50d5c720babd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "63556dc8-fcb7-4dac-c58d-6a2a055f9ad1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "7878636c-d44a-4342-b709-3fc8864cfe9c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-800','801-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c02938f-a908-4501-9158-88123162209f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# base_dir = '/content/drive/My Drive/new Regress'\n",
        "# os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# # Directories for our training,\n",
        "# # validation and test splits\n",
        "# train_dir = os.path.join(base_dir, 'train')\n",
        "# os.makedirs(train_dir, exist_ok=True)\n",
        "# validation_dir = os.path.join(base_dir, 'validation')\n",
        "# os.makedirs(validation_dir, exist_ok=True)\n",
        "# test_dir = os.path.join(base_dir, 'test')\n",
        "# os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "val = df[df['No'].between(629,729)]\n",
        "train = df[df['No'].between(1,628)]\n",
        "test = df[df['No'].between(730,830)] \n",
        "\n",
        "# #Path Train\n",
        "# T1_train = train[train['Class']=='0-800']\n",
        "# T1_path_train = T1_train['path_Picture'].tolist() \n",
        "# T2_train = train[train['Class']=='801-3200']\n",
        "# T2_path_train = T2_train['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "# #Path Validation\n",
        "# T1_val = val[val['Class']=='0-800']\n",
        "# T1_path_val = T1_val['path_Picture'].tolist() \n",
        "# T2_val = val[val['Class']=='801-3200']\n",
        "# T2_path_val = T2_val['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "\n",
        "# #Path Test\n",
        "# T1_test = test[test['Class']=='0-800']\n",
        "# T1_path_test = T1_test['path_Picture'].tolist() \n",
        "# T2_test = test[test['Class']=='801-3200']\n",
        "# T2_path_test = T2_test['path_Picture'].tolist() \n"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "r9N_9sFL1hYB"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/content/drive/My Drive/new project'\n",
        "os.chdir(DATA_PATH)\n",
        "train_dir = os.path.join(DATA_PATH, 'train')\n",
        "print(train_dir)\n",
        "validation_dir = os.path.join(DATA_PATH, 'validation')\n",
        "print(validation_dir)"
      ],
      "metadata": {
        "id": "XyxDPsEi6yYJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35831c46-bf0f-4a94-e13e-f9a6f2d653ba"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/new project/train\n",
            "/content/drive/My Drive/new project/validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "# print('total training 2 images:', len(os.listdir(train_2_dir)),'\\n')\n",
        "\n",
        "# print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "# print('total validation 2 images:', len(os.listdir(validation_2_dir)),'\\n')\n",
        "\n",
        "# print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "# print('total test 2 images:', len(os.listdir(test_2_dir)),'\\n')\n"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "tYeLut2ByGeC"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 1000 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 598  # จำนวนภาพ Train\n",
        "NUM_TEST = 101 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.3\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d327079-aab6-4972-8bbf-e3b19fc94565"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57c5e67b-625f-4f54-e11e-6f2997235635"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d_65 (Conv2D)             (None, 75, 75, 32)   864         ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_49 (BatchN  (None, 75, 75, 32)  128         ['conv2d_65[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_49 (Swish)               (None, 75, 75, 32)   0           ['batch_normalization_49[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_16 (Depthwise  (None, 75, 75, 32)  288         ['swish_49[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_50 (BatchN  (None, 75, 75, 32)  128         ['depthwise_conv2d_16[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_50 (Swish)               (None, 75, 75, 32)   0           ['batch_normalization_50[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_16 (Lambda)             (None, 1, 1, 32)     0           ['swish_50[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_66 (Conv2D)             (None, 1, 1, 8)      264         ['lambda_16[0][0]']              \n",
            "                                                                                                  \n",
            " swish_51 (Swish)               (None, 1, 1, 8)      0           ['conv2d_66[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_67 (Conv2D)             (None, 1, 1, 32)     288         ['swish_51[0][0]']               \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 1, 1, 32)     0           ['conv2d_67[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_16 (Multiply)         (None, 75, 75, 32)   0           ['activation_16[0][0]',          \n",
            "                                                                  'swish_50[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_68 (Conv2D)             (None, 75, 75, 16)   512         ['multiply_16[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_51 (BatchN  (None, 75, 75, 16)  64          ['conv2d_68[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_69 (Conv2D)             (None, 75, 75, 96)   1536        ['batch_normalization_51[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_52 (BatchN  (None, 75, 75, 96)  384         ['conv2d_69[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_52 (Swish)               (None, 75, 75, 96)   0           ['batch_normalization_52[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_17 (Depthwise  (None, 38, 38, 96)  864         ['swish_52[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_53 (BatchN  (None, 38, 38, 96)  384         ['depthwise_conv2d_17[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_53 (Swish)               (None, 38, 38, 96)   0           ['batch_normalization_53[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_17 (Lambda)             (None, 1, 1, 96)     0           ['swish_53[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_70 (Conv2D)             (None, 1, 1, 4)      388         ['lambda_17[0][0]']              \n",
            "                                                                                                  \n",
            " swish_54 (Swish)               (None, 1, 1, 4)      0           ['conv2d_70[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_71 (Conv2D)             (None, 1, 1, 96)     480         ['swish_54[0][0]']               \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 1, 1, 96)     0           ['conv2d_71[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_17 (Multiply)         (None, 38, 38, 96)   0           ['activation_17[0][0]',          \n",
            "                                                                  'swish_53[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_72 (Conv2D)             (None, 38, 38, 24)   2304        ['multiply_17[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_54 (BatchN  (None, 38, 38, 24)  96          ['conv2d_72[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_73 (Conv2D)             (None, 38, 38, 144)  3456        ['batch_normalization_54[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_55 (BatchN  (None, 38, 38, 144)  576        ['conv2d_73[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_55 (Swish)               (None, 38, 38, 144)  0           ['batch_normalization_55[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_18 (Depthwise  (None, 38, 38, 144)  1296       ['swish_55[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_56 (BatchN  (None, 38, 38, 144)  576        ['depthwise_conv2d_18[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_56 (Swish)               (None, 38, 38, 144)  0           ['batch_normalization_56[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_18 (Lambda)             (None, 1, 1, 144)    0           ['swish_56[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_74 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_18[0][0]']              \n",
            "                                                                                                  \n",
            " swish_57 (Swish)               (None, 1, 1, 6)      0           ['conv2d_74[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_75 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_57[0][0]']               \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 1, 1, 144)    0           ['conv2d_75[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_18 (Multiply)         (None, 38, 38, 144)  0           ['activation_18[0][0]',          \n",
            "                                                                  'swish_56[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_76 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_18[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_57 (BatchN  (None, 38, 38, 24)  96          ['conv2d_76[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_9 (DropConnect)   (None, 38, 38, 24)   0           ['batch_normalization_57[0][0]'] \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 38, 38, 24)   0           ['drop_connect_9[0][0]',         \n",
            "                                                                  'batch_normalization_54[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_77 (Conv2D)             (None, 38, 38, 144)  3456        ['add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_58 (BatchN  (None, 38, 38, 144)  576        ['conv2d_77[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_58 (Swish)               (None, 38, 38, 144)  0           ['batch_normalization_58[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_19 (Depthwise  (None, 19, 19, 144)  3600       ['swish_58[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_59 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_19[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_59 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_59[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_19 (Lambda)             (None, 1, 1, 144)    0           ['swish_59[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_78 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_19[0][0]']              \n",
            "                                                                                                  \n",
            " swish_60 (Swish)               (None, 1, 1, 6)      0           ['conv2d_78[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_79 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_60[0][0]']               \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 1, 1, 144)    0           ['conv2d_79[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_19 (Multiply)         (None, 19, 19, 144)  0           ['activation_19[0][0]',          \n",
            "                                                                  'swish_59[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_80 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_19[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_60 (BatchN  (None, 19, 19, 40)  160         ['conv2d_80[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_81 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_60[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_61 (BatchN  (None, 19, 19, 240)  960        ['conv2d_81[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_61 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_61[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_20 (Depthwise  (None, 19, 19, 240)  6000       ['swish_61[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_62 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_20[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_62 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_62[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_20 (Lambda)             (None, 1, 1, 240)    0           ['swish_62[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_82 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_20[0][0]']              \n",
            "                                                                                                  \n",
            " swish_63 (Swish)               (None, 1, 1, 10)     0           ['conv2d_82[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_83 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_63[0][0]']               \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 1, 1, 240)    0           ['conv2d_83[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_20 (Multiply)         (None, 19, 19, 240)  0           ['activation_20[0][0]',          \n",
            "                                                                  'swish_62[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_84 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_20[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_63 (BatchN  (None, 19, 19, 40)  160         ['conv2d_84[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_10 (DropConnect)  (None, 19, 19, 40)   0           ['batch_normalization_63[0][0]'] \n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, 19, 19, 40)   0           ['drop_connect_10[0][0]',        \n",
            "                                                                  'batch_normalization_60[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_85 (Conv2D)             (None, 19, 19, 240)  9600        ['add_10[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_64 (BatchN  (None, 19, 19, 240)  960        ['conv2d_85[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_64 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_64[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_21 (Depthwise  (None, 10, 10, 240)  2160       ['swish_64[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_65 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_21[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_65 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_65[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_21 (Lambda)             (None, 1, 1, 240)    0           ['swish_65[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_86 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_21[0][0]']              \n",
            "                                                                                                  \n",
            " swish_66 (Swish)               (None, 1, 1, 10)     0           ['conv2d_86[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_87 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_66[0][0]']               \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 1, 1, 240)    0           ['conv2d_87[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_21 (Multiply)         (None, 10, 10, 240)  0           ['activation_21[0][0]',          \n",
            "                                                                  'swish_65[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_88 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_21[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_66 (BatchN  (None, 10, 10, 80)  320         ['conv2d_88[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_89 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_66[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_67 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_89[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_67 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_67[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_22 (Depthwise  (None, 10, 10, 480)  4320       ['swish_67[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_68 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_22[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_68 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_68[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_22 (Lambda)             (None, 1, 1, 480)    0           ['swish_68[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_90 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_22[0][0]']              \n",
            "                                                                                                  \n",
            " swish_69 (Swish)               (None, 1, 1, 20)     0           ['conv2d_90[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_91 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_69[0][0]']               \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 1, 1, 480)    0           ['conv2d_91[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_22 (Multiply)         (None, 10, 10, 480)  0           ['activation_22[0][0]',          \n",
            "                                                                  'swish_68[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_92 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_22[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_69 (BatchN  (None, 10, 10, 80)  320         ['conv2d_92[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_11 (DropConnect)  (None, 10, 10, 80)   0           ['batch_normalization_69[0][0]'] \n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, 10, 10, 80)   0           ['drop_connect_11[0][0]',        \n",
            "                                                                  'batch_normalization_66[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_93 (Conv2D)             (None, 10, 10, 480)  38400       ['add_11[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_70 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_93[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_70 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_70[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_23 (Depthwise  (None, 10, 10, 480)  4320       ['swish_70[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_71 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_23[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_71 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_71[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_23 (Lambda)             (None, 1, 1, 480)    0           ['swish_71[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_94 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_23[0][0]']              \n",
            "                                                                                                  \n",
            " swish_72 (Swish)               (None, 1, 1, 20)     0           ['conv2d_94[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_95 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_72[0][0]']               \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 1, 1, 480)    0           ['conv2d_95[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_23 (Multiply)         (None, 10, 10, 480)  0           ['activation_23[0][0]',          \n",
            "                                                                  'swish_71[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_96 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_23[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_72 (BatchN  (None, 10, 10, 80)  320         ['conv2d_96[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_12 (DropConnect)  (None, 10, 10, 80)   0           ['batch_normalization_72[0][0]'] \n",
            "                                                                                                  \n",
            " add_12 (Add)                   (None, 10, 10, 80)   0           ['drop_connect_12[0][0]',        \n",
            "                                                                  'add_11[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_97 (Conv2D)             (None, 10, 10, 480)  38400       ['add_12[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_73 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_97[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_73 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_73[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_24 (Depthwise  (None, 10, 10, 480)  12000      ['swish_73[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_74 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_24[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_74 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_74[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_24 (Lambda)             (None, 1, 1, 480)    0           ['swish_74[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_98 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_24[0][0]']              \n",
            "                                                                                                  \n",
            " swish_75 (Swish)               (None, 1, 1, 20)     0           ['conv2d_98[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_99 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_75[0][0]']               \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 1, 1, 480)    0           ['conv2d_99[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_24 (Multiply)         (None, 10, 10, 480)  0           ['activation_24[0][0]',          \n",
            "                                                                  'swish_74[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_100 (Conv2D)            (None, 10, 10, 112)  53760       ['multiply_24[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_75 (BatchN  (None, 10, 10, 112)  448        ['conv2d_100[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_101 (Conv2D)            (None, 10, 10, 672)  75264       ['batch_normalization_75[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_76 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_101[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_76 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_76[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_25 (Depthwise  (None, 10, 10, 672)  16800      ['swish_76[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_77 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_25[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_77 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_77[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_25 (Lambda)             (None, 1, 1, 672)    0           ['swish_77[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_102 (Conv2D)            (None, 1, 1, 28)     18844       ['lambda_25[0][0]']              \n",
            "                                                                                                  \n",
            " swish_78 (Swish)               (None, 1, 1, 28)     0           ['conv2d_102[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_103 (Conv2D)            (None, 1, 1, 672)    19488       ['swish_78[0][0]']               \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, 1, 1, 672)    0           ['conv2d_103[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_25 (Multiply)         (None, 10, 10, 672)  0           ['activation_25[0][0]',          \n",
            "                                                                  'swish_77[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_104 (Conv2D)            (None, 10, 10, 112)  75264       ['multiply_25[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_78 (BatchN  (None, 10, 10, 112)  448        ['conv2d_104[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_13 (DropConnect)  (None, 10, 10, 112)  0           ['batch_normalization_78[0][0]'] \n",
            "                                                                                                  \n",
            " add_13 (Add)                   (None, 10, 10, 112)  0           ['drop_connect_13[0][0]',        \n",
            "                                                                  'batch_normalization_75[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_105 (Conv2D)            (None, 10, 10, 672)  75264       ['add_13[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_79 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_105[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_79 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_79[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_26 (Depthwise  (None, 10, 10, 672)  16800      ['swish_79[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_80 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_26[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_80 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_80[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_26 (Lambda)             (None, 1, 1, 672)    0           ['swish_80[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_106 (Conv2D)            (None, 1, 1, 28)     18844       ['lambda_26[0][0]']              \n",
            "                                                                                                  \n",
            " swish_81 (Swish)               (None, 1, 1, 28)     0           ['conv2d_106[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_107 (Conv2D)            (None, 1, 1, 672)    19488       ['swish_81[0][0]']               \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, 1, 1, 672)    0           ['conv2d_107[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_26 (Multiply)         (None, 10, 10, 672)  0           ['activation_26[0][0]',          \n",
            "                                                                  'swish_80[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_108 (Conv2D)            (None, 10, 10, 112)  75264       ['multiply_26[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_81 (BatchN  (None, 10, 10, 112)  448        ['conv2d_108[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_14 (DropConnect)  (None, 10, 10, 112)  0           ['batch_normalization_81[0][0]'] \n",
            "                                                                                                  \n",
            " add_14 (Add)                   (None, 10, 10, 112)  0           ['drop_connect_14[0][0]',        \n",
            "                                                                  'add_13[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_109 (Conv2D)            (None, 10, 10, 672)  75264       ['add_14[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_82 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_109[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_82 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_82[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_27 (Depthwise  (None, 5, 5, 672)   16800       ['swish_82[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_83 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_27[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_83 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_83[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_27 (Lambda)             (None, 1, 1, 672)    0           ['swish_83[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_110 (Conv2D)            (None, 1, 1, 28)     18844       ['lambda_27[0][0]']              \n",
            "                                                                                                  \n",
            " swish_84 (Swish)               (None, 1, 1, 28)     0           ['conv2d_110[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_111 (Conv2D)            (None, 1, 1, 672)    19488       ['swish_84[0][0]']               \n",
            "                                                                                                  \n",
            " activation_27 (Activation)     (None, 1, 1, 672)    0           ['conv2d_111[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_27 (Multiply)         (None, 5, 5, 672)    0           ['activation_27[0][0]',          \n",
            "                                                                  'swish_83[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_112 (Conv2D)            (None, 5, 5, 192)    129024      ['multiply_27[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_84 (BatchN  (None, 5, 5, 192)   768         ['conv2d_112[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_113 (Conv2D)            (None, 5, 5, 1152)   221184      ['batch_normalization_84[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_85 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_113[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_85 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_85[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_28 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_85[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_86 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_28[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_86 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_86[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_28 (Lambda)             (None, 1, 1, 1152)   0           ['swish_86[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_114 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_28[0][0]']              \n",
            "                                                                                                  \n",
            " swish_87 (Swish)               (None, 1, 1, 48)     0           ['conv2d_114[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_115 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_87[0][0]']               \n",
            "                                                                                                  \n",
            " activation_28 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_115[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_28 (Multiply)         (None, 5, 5, 1152)   0           ['activation_28[0][0]',          \n",
            "                                                                  'swish_86[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_116 (Conv2D)            (None, 5, 5, 192)    221184      ['multiply_28[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_87 (BatchN  (None, 5, 5, 192)   768         ['conv2d_116[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_15 (DropConnect)  (None, 5, 5, 192)    0           ['batch_normalization_87[0][0]'] \n",
            "                                                                                                  \n",
            " add_15 (Add)                   (None, 5, 5, 192)    0           ['drop_connect_15[0][0]',        \n",
            "                                                                  'batch_normalization_84[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_117 (Conv2D)            (None, 5, 5, 1152)   221184      ['add_15[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_88 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_117[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_88 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_88[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_29 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_88[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_89 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_29[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_89 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_89[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_29 (Lambda)             (None, 1, 1, 1152)   0           ['swish_89[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_118 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_29[0][0]']              \n",
            "                                                                                                  \n",
            " swish_90 (Swish)               (None, 1, 1, 48)     0           ['conv2d_118[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_119 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_90[0][0]']               \n",
            "                                                                                                  \n",
            " activation_29 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_119[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_29 (Multiply)         (None, 5, 5, 1152)   0           ['activation_29[0][0]',          \n",
            "                                                                  'swish_89[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_120 (Conv2D)            (None, 5, 5, 192)    221184      ['multiply_29[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_90 (BatchN  (None, 5, 5, 192)   768         ['conv2d_120[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_16 (DropConnect)  (None, 5, 5, 192)    0           ['batch_normalization_90[0][0]'] \n",
            "                                                                                                  \n",
            " add_16 (Add)                   (None, 5, 5, 192)    0           ['drop_connect_16[0][0]',        \n",
            "                                                                  'add_15[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_121 (Conv2D)            (None, 5, 5, 1152)   221184      ['add_16[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_91 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_121[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_91 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_91[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_30 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_91[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_92 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_30[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_92 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_92[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_30 (Lambda)             (None, 1, 1, 1152)   0           ['swish_92[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_122 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_30[0][0]']              \n",
            "                                                                                                  \n",
            " swish_93 (Swish)               (None, 1, 1, 48)     0           ['conv2d_122[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_123 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_93[0][0]']               \n",
            "                                                                                                  \n",
            " activation_30 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_123[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_30 (Multiply)         (None, 5, 5, 1152)   0           ['activation_30[0][0]',          \n",
            "                                                                  'swish_92[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_124 (Conv2D)            (None, 5, 5, 192)    221184      ['multiply_30[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_93 (BatchN  (None, 5, 5, 192)   768         ['conv2d_124[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_17 (DropConnect)  (None, 5, 5, 192)    0           ['batch_normalization_93[0][0]'] \n",
            "                                                                                                  \n",
            " add_17 (Add)                   (None, 5, 5, 192)    0           ['drop_connect_17[0][0]',        \n",
            "                                                                  'add_16[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_125 (Conv2D)            (None, 5, 5, 1152)   221184      ['add_17[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_94 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_125[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_94 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_94[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_31 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_94[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_95 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_31[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_95 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_95[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_31 (Lambda)             (None, 1, 1, 1152)   0           ['swish_95[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_126 (Conv2D)            (None, 1, 1, 48)     55344       ['lambda_31[0][0]']              \n",
            "                                                                                                  \n",
            " swish_96 (Swish)               (None, 1, 1, 48)     0           ['conv2d_126[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_127 (Conv2D)            (None, 1, 1, 1152)   56448       ['swish_96[0][0]']               \n",
            "                                                                                                  \n",
            " activation_31 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_127[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_31 (Multiply)         (None, 5, 5, 1152)   0           ['activation_31[0][0]',          \n",
            "                                                                  'swish_95[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_128 (Conv2D)            (None, 5, 5, 320)    368640      ['multiply_31[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_96 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_128[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_129 (Conv2D)            (None, 5, 5, 1280)   409600      ['batch_normalization_96[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_97 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_129[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_97 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_97[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(2, activation='softmax', name=\"fc_out\"))\n",
        "model.add(layers.Dense(1))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade7ea8c-a051-424e-9d6f-3812b7c4a4fe"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 215\n",
            "This is the number of trainable layers after freezing the conv base: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "046c1f32-2619-4b23-87da-6632ff70268c"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 2)                 2562      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,052,129\n",
            "Trainable params: 2,565\n",
            "Non-trainable params: 4,049,564\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe = train,\n",
        "        directory = train_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'BET',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)\n",
        "\n",
        "validation_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe = val,\n",
        "        directory = validation_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'BET',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0564cdba-dd2e-4fac-c113-9e4f44788ca1"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 628 validated image filenames.\n",
            "Found 101 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.compile(\n",
        "#     loss=rmse,\n",
        "#     optimizer=Adam(),\n",
        "#     metrics=[rmse]"
      ],
      "metadata": {
        "id": "gOSRISmJXWec"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mse',\n",
        "              optimizer=Adam(learning_rate=9e-3),\n",
        "              metrics=['mae'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b7ecd1d-09f9-4ba5-dd7d-143373b0596c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-65-0e0394aaa368>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37/37 [==============================] - 11s 131ms/step - loss: 1430541.6250 - mae: 960.0522 - val_loss: 543064.9375 - val_mae: 569.5405\n",
            "Epoch 2/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 1434987.8750 - mae: 965.2227 - val_loss: 538173.2500 - val_mae: 566.8487\n",
            "Epoch 3/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1453779.2500 - mae: 971.9819 - val_loss: 532856.3125 - val_mae: 559.7734\n",
            "Epoch 4/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1468546.6250 - mae: 978.6757 - val_loss: 515093.0000 - val_mae: 546.3167\n",
            "Epoch 5/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1410761.0000 - mae: 955.7294 - val_loss: 540118.9375 - val_mae: 567.1822\n",
            "Epoch 6/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 1432802.8750 - mae: 961.7482 - val_loss: 517112.2188 - val_mae: 546.6756\n",
            "Epoch 7/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1444587.5000 - mae: 970.2446 - val_loss: 533773.5625 - val_mae: 563.1956\n",
            "Epoch 8/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1450186.1250 - mae: 968.3970 - val_loss: 512237.3438 - val_mae: 543.6968\n",
            "Epoch 9/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 1451866.5000 - mae: 969.7452 - val_loss: 515651.5000 - val_mae: 545.0775\n",
            "Epoch 10/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 1468367.3750 - mae: 981.1523 - val_loss: 540976.8125 - val_mae: 570.6043\n",
            "Epoch 11/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1430483.3750 - mae: 962.2438 - val_loss: 527454.7500 - val_mae: 559.1988\n",
            "Epoch 12/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1467144.0000 - mae: 977.8729 - val_loss: 505332.0938 - val_mae: 539.3259\n",
            "Epoch 13/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1421729.8750 - mae: 960.4907 - val_loss: 534145.6875 - val_mae: 561.6556\n",
            "Epoch 14/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 1372957.5000 - mae: 940.1469 - val_loss: 516582.2500 - val_mae: 548.2303\n",
            "Epoch 15/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1422590.0000 - mae: 956.8776 - val_loss: 520032.2500 - val_mae: 549.8994\n",
            "Epoch 16/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1417359.6250 - mae: 952.9219 - val_loss: 527970.8125 - val_mae: 558.2508\n",
            "Epoch 17/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1425695.0000 - mae: 958.6942 - val_loss: 535753.4375 - val_mae: 565.7708\n",
            "Epoch 18/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 1445597.8750 - mae: 964.7097 - val_loss: 539052.8125 - val_mae: 566.8583\n",
            "Epoch 19/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 1396278.5000 - mae: 945.1375 - val_loss: 525756.2500 - val_mae: 556.0259\n",
            "Epoch 20/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1438399.6250 - mae: 964.4103 - val_loss: 520931.0312 - val_mae: 553.3350\n",
            "Epoch 21/1000\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 1419936.6250 - mae: 957.9180 - val_loss: 532706.4375 - val_mae: 562.5937\n",
            "Epoch 22/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1392040.7500 - mae: 942.5525 - val_loss: 489154.5000 - val_mae: 523.7437\n",
            "Epoch 23/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1410775.5000 - mae: 951.4963 - val_loss: 526919.6250 - val_mae: 555.1855\n",
            "Epoch 24/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1390133.6250 - mae: 942.8070 - val_loss: 501082.8438 - val_mae: 533.5778\n",
            "Epoch 25/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1425041.0000 - mae: 959.1428 - val_loss: 508857.9062 - val_mae: 541.3782\n",
            "Epoch 26/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1402114.6250 - mae: 951.4676 - val_loss: 516311.2188 - val_mae: 544.7945\n",
            "Epoch 27/1000\n",
            "37/37 [==============================] - 8s 223ms/step - loss: 1411709.8750 - mae: 954.0721 - val_loss: 499002.3438 - val_mae: 531.6247\n",
            "Epoch 28/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 1386668.8750 - mae: 937.9131 - val_loss: 489843.0000 - val_mae: 522.2540\n",
            "Epoch 29/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1420029.1250 - mae: 956.6259 - val_loss: 510074.2812 - val_mae: 540.5292\n",
            "Epoch 30/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 1400001.8750 - mae: 949.5747 - val_loss: 517778.5000 - val_mae: 548.3233\n",
            "Epoch 31/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1407646.2500 - mae: 949.3349 - val_loss: 513137.7500 - val_mae: 546.4885\n",
            "Epoch 32/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1392922.8750 - mae: 943.8743 - val_loss: 479009.3438 - val_mae: 515.5879\n",
            "Epoch 33/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 1381783.6250 - mae: 938.3928 - val_loss: 507320.8438 - val_mae: 538.2173\n",
            "Epoch 34/1000\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 1399789.8750 - mae: 945.7855 - val_loss: 527475.9375 - val_mae: 556.7960\n",
            "Epoch 35/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1375635.6250 - mae: 938.2890 - val_loss: 505897.4062 - val_mae: 536.6517\n",
            "Epoch 36/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1357534.5000 - mae: 933.9875 - val_loss: 504617.0312 - val_mae: 536.2007\n",
            "Epoch 37/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1363275.1250 - mae: 939.2632 - val_loss: 504557.1250 - val_mae: 535.6438\n",
            "Epoch 38/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 1417830.1250 - mae: 953.1805 - val_loss: 499813.1250 - val_mae: 532.9593\n",
            "Epoch 39/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1382671.3750 - mae: 940.1937 - val_loss: 523830.3438 - val_mae: 553.2683\n",
            "Epoch 40/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1395382.6250 - mae: 939.5058 - val_loss: 502481.4062 - val_mae: 533.7026\n",
            "Epoch 41/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1401869.6250 - mae: 945.4783 - val_loss: 530782.3125 - val_mae: 560.9946\n",
            "Epoch 42/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1396488.8750 - mae: 943.1465 - val_loss: 497013.7500 - val_mae: 530.0829\n",
            "Epoch 43/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1381589.6250 - mae: 936.5690 - val_loss: 508677.1562 - val_mae: 540.2050\n",
            "Epoch 44/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 1397916.7500 - mae: 945.2924 - val_loss: 503957.7812 - val_mae: 537.5342\n",
            "Epoch 45/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1396445.7500 - mae: 941.7169 - val_loss: 506664.1562 - val_mae: 538.8359\n",
            "Epoch 46/1000\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1390635.1250 - mae: 936.5766 - val_loss: 494371.1250 - val_mae: 528.0740\n",
            "Epoch 47/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1369770.6250 - mae: 937.3582 - val_loss: 501872.3438 - val_mae: 535.5909\n",
            "Epoch 48/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 1396774.7500 - mae: 944.2140 - val_loss: 484719.2500 - val_mae: 517.7722\n",
            "Epoch 49/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1399480.6250 - mae: 948.7947 - val_loss: 504551.5000 - val_mae: 536.6191\n",
            "Epoch 50/1000\n",
            "37/37 [==============================] - 6s 139ms/step - loss: 1396862.5000 - mae: 941.6597 - val_loss: 495616.5938 - val_mae: 527.2320\n",
            "Epoch 51/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1376167.5000 - mae: 933.1212 - val_loss: 478723.7500 - val_mae: 514.0663\n",
            "Epoch 52/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1358865.3750 - mae: 928.6484 - val_loss: 486231.6562 - val_mae: 521.8696\n",
            "Epoch 53/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 1348201.2500 - mae: 921.8052 - val_loss: 509904.8438 - val_mae: 542.1982\n",
            "Epoch 54/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1364869.1250 - mae: 930.3314 - val_loss: 484898.3750 - val_mae: 520.5906\n",
            "Epoch 55/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 1376675.3750 - mae: 937.3569 - val_loss: 488056.2188 - val_mae: 517.3118\n",
            "Epoch 56/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1383211.2500 - mae: 936.2946 - val_loss: 495538.8438 - val_mae: 525.3843\n",
            "Epoch 57/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 1382499.1250 - mae: 939.4120 - val_loss: 478750.1250 - val_mae: 512.2232\n",
            "Epoch 58/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1379250.3750 - mae: 938.6381 - val_loss: 482042.0000 - val_mae: 513.3225\n",
            "Epoch 59/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1371922.2500 - mae: 930.7819 - val_loss: 505758.5000 - val_mae: 538.8608\n",
            "Epoch 60/1000\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 1339750.0000 - mae: 916.7262 - val_loss: 504962.7188 - val_mae: 537.3713\n",
            "Epoch 61/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1375669.8750 - mae: 933.5790 - val_loss: 508349.6250 - val_mae: 539.5978\n",
            "Epoch 62/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1380594.1250 - mae: 932.0491 - val_loss: 491568.2812 - val_mae: 525.8837\n",
            "Epoch 63/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1360714.5000 - mae: 925.9142 - val_loss: 482843.2812 - val_mae: 516.7938\n",
            "Epoch 64/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1358291.0000 - mae: 928.4433 - val_loss: 494201.8750 - val_mae: 526.6367\n",
            "Epoch 65/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1384696.2500 - mae: 937.8248 - val_loss: 464772.8438 - val_mae: 497.9667\n",
            "Epoch 66/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1386165.0000 - mae: 937.9088 - val_loss: 472782.0312 - val_mae: 506.1135\n",
            "Epoch 67/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1359436.6250 - mae: 931.2711 - val_loss: 484135.5000 - val_mae: 516.2292\n",
            "Epoch 68/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 1366340.7500 - mae: 928.9524 - val_loss: 471471.5000 - val_mae: 504.8171\n",
            "Epoch 69/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1333319.0000 - mae: 912.2015 - val_loss: 466891.8750 - val_mae: 502.4264\n",
            "Epoch 70/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 1357433.0000 - mae: 921.8033 - val_loss: 486110.3750 - val_mae: 516.3335\n",
            "Epoch 71/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1367394.3750 - mae: 929.4114 - val_loss: 485425.6562 - val_mae: 515.6700\n",
            "Epoch 72/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1365513.5000 - mae: 930.4071 - val_loss: 500634.1562 - val_mae: 531.6386\n",
            "Epoch 73/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 1345141.8750 - mae: 916.4891 - val_loss: 492081.8750 - val_mae: 523.1240\n",
            "Epoch 74/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1347994.0000 - mae: 920.9346 - val_loss: 478851.4062 - val_mae: 511.3572\n",
            "Epoch 75/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1357127.0000 - mae: 924.5835 - val_loss: 474867.9062 - val_mae: 508.7654\n",
            "Epoch 76/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1364089.5000 - mae: 929.4507 - val_loss: 474243.0000 - val_mae: 508.4049\n",
            "Epoch 77/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1367619.2500 - mae: 929.6377 - val_loss: 493311.6250 - val_mae: 526.6743\n",
            "Epoch 78/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1347809.2500 - mae: 920.3193 - val_loss: 492665.6562 - val_mae: 526.3163\n",
            "Epoch 79/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1346992.7500 - mae: 920.6249 - val_loss: 464403.8750 - val_mae: 498.0224\n",
            "Epoch 80/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1359465.8750 - mae: 924.2072 - val_loss: 483466.4062 - val_mae: 516.8555\n",
            "Epoch 81/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1322985.6250 - mae: 906.8561 - val_loss: 494578.0000 - val_mae: 526.4255\n",
            "Epoch 82/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1353787.8750 - mae: 923.3826 - val_loss: 482153.0000 - val_mae: 515.5833\n",
            "Epoch 83/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1378905.8750 - mae: 935.0339 - val_loss: 481422.3438 - val_mae: 514.3604\n",
            "Epoch 84/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1342949.3750 - mae: 916.7530 - val_loss: 484703.1250 - val_mae: 516.0232\n",
            "Epoch 85/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1306970.1250 - mae: 903.0455 - val_loss: 480133.4062 - val_mae: 513.3633\n",
            "Epoch 86/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1323033.2500 - mae: 908.8003 - val_loss: 471631.5938 - val_mae: 503.7154\n",
            "Epoch 87/1000\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 1334257.0000 - mae: 918.2048 - val_loss: 486608.0938 - val_mae: 520.5298\n",
            "Epoch 88/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 1333463.1250 - mae: 911.6407 - val_loss: 470365.4062 - val_mae: 502.7151\n",
            "Epoch 89/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1368585.2500 - mae: 927.5844 - val_loss: 477476.1250 - val_mae: 510.5099\n",
            "Epoch 90/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1334918.7500 - mae: 910.9174 - val_loss: 484601.3438 - val_mae: 518.5987\n",
            "Epoch 91/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1335059.0000 - mae: 916.9862 - val_loss: 483913.9062 - val_mae: 517.6755\n",
            "Epoch 92/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 1320892.3750 - mae: 908.1512 - val_loss: 479450.3438 - val_mae: 510.9082\n",
            "Epoch 93/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1331056.3750 - mae: 911.8130 - val_loss: 463209.5938 - val_mae: 497.1759\n",
            "Epoch 94/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1309529.8750 - mae: 903.7932 - val_loss: 466496.0000 - val_mae: 498.8518\n",
            "Epoch 95/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 1325299.6250 - mae: 909.9349 - val_loss: 465821.3750 - val_mae: 497.9146\n",
            "Epoch 96/1000\n",
            "37/37 [==============================] - 8s 225ms/step - loss: 1348705.7500 - mae: 920.1953 - val_loss: 476789.9062 - val_mae: 508.0366\n",
            "Epoch 97/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1344130.1250 - mae: 920.6762 - val_loss: 449071.7188 - val_mae: 484.1072\n",
            "Epoch 98/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1339883.3750 - mae: 914.3439 - val_loss: 467712.5000 - val_mae: 502.9524\n",
            "Epoch 99/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 1321393.1250 - mae: 907.0848 - val_loss: 478597.0938 - val_mae: 512.5146\n",
            "Epoch 100/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1317101.1250 - mae: 906.7614 - val_loss: 489472.7500 - val_mae: 522.6186\n",
            "Epoch 101/1000\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 1326357.1250 - mae: 904.8739 - val_loss: 465795.3750 - val_mae: 501.0429\n",
            "Epoch 102/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1315228.5000 - mae: 906.4933 - val_loss: 476683.6250 - val_mae: 511.4347\n",
            "Epoch 103/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1292981.7500 - mae: 898.2529 - val_loss: 460746.9062 - val_mae: 493.0558\n",
            "Epoch 104/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1332302.7500 - mae: 912.3681 - val_loss: 467704.9062 - val_mae: 500.8484\n",
            "Epoch 105/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1322803.3750 - mae: 908.6147 - val_loss: 459487.8750 - val_mae: 492.0412\n",
            "Epoch 106/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1303788.6250 - mae: 902.4445 - val_loss: 466432.7812 - val_mae: 499.8413\n",
            "Epoch 107/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 1311872.3750 - mae: 903.7591 - val_loss: 450474.4062 - val_mae: 486.1414\n",
            "Epoch 108/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1309208.1250 - mae: 900.9849 - val_loss: 453707.5312 - val_mae: 487.7909\n",
            "Epoch 109/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1336493.3750 - mae: 917.4947 - val_loss: 460776.8438 - val_mae: 490.9237\n",
            "Epoch 110/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1331477.3750 - mae: 912.9064 - val_loss: 456297.1250 - val_mae: 488.5225\n",
            "Epoch 111/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 1318956.7500 - mae: 906.3627 - val_loss: 467061.6250 - val_mae: 498.6366\n",
            "Epoch 112/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1288069.5000 - mae: 894.5176 - val_loss: 443648.8438 - val_mae: 476.4638\n",
            "Epoch 113/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 1303944.8750 - mae: 900.5724 - val_loss: 480831.1250 - val_mae: 514.8194\n",
            "Epoch 114/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1307034.2500 - mae: 902.0767 - val_loss: 437964.5000 - val_mae: 472.7997\n",
            "Epoch 115/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1323501.1250 - mae: 910.0797 - val_loss: 464517.7500 - val_mae: 496.0792\n",
            "Epoch 116/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1336595.6250 - mae: 910.3895 - val_loss: 448709.8750 - val_mae: 482.6410\n",
            "Epoch 117/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1296023.5000 - mae: 895.6008 - val_loss: 459401.3750 - val_mae: 492.7575\n",
            "Epoch 118/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1286734.0000 - mae: 892.9623 - val_loss: 462622.0000 - val_mae: 494.4335\n",
            "Epoch 119/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1293360.8750 - mae: 895.8268 - val_loss: 435587.5000 - val_mae: 469.9740\n",
            "Epoch 120/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1299866.3750 - mae: 895.1009 - val_loss: 461336.2500 - val_mae: 492.5928\n",
            "Epoch 121/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 1321124.5000 - mae: 905.4343 - val_loss: 468139.4062 - val_mae: 500.6847\n",
            "Epoch 122/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1295341.0000 - mae: 894.4412 - val_loss: 448846.6250 - val_mae: 480.8365\n",
            "Epoch 123/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1294101.6250 - mae: 892.8961 - val_loss: 455629.6562 - val_mae: 488.6450\n",
            "Epoch 124/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 1314214.1250 - mae: 904.9908 - val_loss: 447621.6562 - val_mae: 479.8316\n",
            "Epoch 125/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 1305942.6250 - mae: 901.3025 - val_loss: 447010.6250 - val_mae: 479.1944\n",
            "Epoch 126/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1300026.1250 - mae: 894.3568 - val_loss: 439014.4062 - val_mae: 469.5448\n",
            "Epoch 127/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 1253213.6250 - mae: 875.3176 - val_loss: 430802.0938 - val_mae: 464.8550\n",
            "Epoch 128/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1275858.3750 - mae: 883.2548 - val_loss: 445190.2500 - val_mae: 477.2913\n",
            "Epoch 129/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 1313292.1250 - mae: 901.7979 - val_loss: 451898.1250 - val_mae: 484.8116\n",
            "Epoch 130/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1289713.0000 - mae: 889.8743 - val_loss: 443966.5000 - val_mae: 475.4620\n",
            "Epoch 131/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 1295161.6250 - mae: 893.0674 - val_loss: 450682.1250 - val_mae: 483.8292\n",
            "Epoch 132/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1274933.8750 - mae: 885.0397 - val_loss: 438954.2500 - val_mae: 472.6299\n",
            "Epoch 133/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1277324.8750 - mae: 886.4414 - val_loss: 431062.5312 - val_mae: 463.8633\n",
            "Epoch 134/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1261296.3750 - mae: 878.6446 - val_loss: 437754.5938 - val_mae: 471.9942\n",
            "Epoch 135/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1287438.6250 - mae: 889.5650 - val_loss: 437138.3438 - val_mae: 471.6670\n",
            "Epoch 136/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1276482.6250 - mae: 890.8049 - val_loss: 458640.2500 - val_mae: 492.7443\n",
            "Epoch 137/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1267284.5000 - mae: 882.0663 - val_loss: 458008.7812 - val_mae: 491.9898\n",
            "Epoch 138/1000\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 1278813.1250 - mae: 886.7571 - val_loss: 453617.9062 - val_mae: 489.8390\n",
            "Epoch 139/1000\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 1255854.3750 - mae: 875.7209 - val_loss: 449550.0938 - val_mae: 482.9121\n",
            "Epoch 140/1000\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 1271412.2500 - mae: 881.2513 - val_loss: 434165.1250 - val_mae: 470.2459\n",
            "Epoch 141/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1294439.3750 - mae: 891.8888 - val_loss: 448328.0938 - val_mae: 482.4265\n",
            "Epoch 142/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1311391.6250 - mae: 903.8504 - val_loss: 432950.7500 - val_mae: 469.2937\n",
            "Epoch 143/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1232615.0000 - mae: 869.8701 - val_loss: 447089.9062 - val_mae: 481.6286\n",
            "Epoch 144/1000\n",
            "37/37 [==============================] - 10s 243ms/step - loss: 1263767.6250 - mae: 880.8161 - val_loss: 431764.9688 - val_mae: 468.6830\n",
            "Epoch 145/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1258264.1250 - mae: 878.6400 - val_loss: 445857.6250 - val_mae: 480.8849\n",
            "Epoch 146/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 1275030.1250 - mae: 885.5908 - val_loss: 452412.2188 - val_mae: 489.2040\n",
            "Epoch 147/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1255973.6250 - mae: 873.0981 - val_loss: 426626.0000 - val_mae: 461.3566\n",
            "Epoch 148/1000\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 1273271.0000 - mae: 881.9684 - val_loss: 458317.3438 - val_mae: 497.0533\n",
            "Epoch 149/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1246500.0000 - mae: 874.5636 - val_loss: 439697.0312 - val_mae: 477.7328\n",
            "Epoch 150/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1269495.5000 - mae: 883.8303 - val_loss: 439086.6250 - val_mae: 477.4287\n",
            "Epoch 151/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 1283446.7500 - mae: 885.9861 - val_loss: 442232.2812 - val_mae: 479.0541\n",
            "Epoch 152/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1234401.3750 - mae: 865.1425 - val_loss: 423698.7188 - val_mae: 459.8137\n",
            "Epoch 153/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 1238434.5000 - mae: 867.4484 - val_loss: 448096.4062 - val_mae: 486.8746\n",
            "Epoch 154/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1295606.6250 - mae: 891.4725 - val_loss: 440415.4062 - val_mae: 478.1112\n",
            "Epoch 155/1000\n",
            "37/37 [==============================] - 6s 135ms/step - loss: 1259728.7500 - mae: 876.8036 - val_loss: 425275.5938 - val_mae: 465.2769\n",
            "Epoch 156/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 1259357.3750 - mae: 874.9539 - val_loss: 428422.9062 - val_mae: 466.9862\n",
            "Epoch 157/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1268454.5000 - mae: 878.4958 - val_loss: 434865.0938 - val_mae: 475.1136\n",
            "Epoch 158/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1234781.0000 - mae: 867.5217 - val_loss: 445012.8438 - val_mae: 485.0517\n",
            "Epoch 159/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1272315.7500 - mae: 880.5612 - val_loss: 401468.2500 - val_mae: 443.1268\n",
            "Epoch 160/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1249291.3750 - mae: 869.1526 - val_loss: 426081.4688 - val_mae: 465.7076\n",
            "Epoch 161/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1198445.7500 - mae: 849.9800 - val_loss: 407344.8438 - val_mae: 450.8488\n",
            "Epoch 162/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1237252.2500 - mae: 863.3676 - val_loss: 435623.1250 - val_mae: 475.5530\n",
            "Epoch 163/1000\n",
            "37/37 [==============================] - 10s 239ms/step - loss: 1249201.5000 - mae: 867.1078 - val_loss: 420636.5000 - val_mae: 462.8667\n",
            "Epoch 164/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1250111.1250 - mae: 869.8242 - val_loss: 423768.6250 - val_mae: 464.4377\n",
            "Epoch 165/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 1273642.3750 - mae: 884.5016 - val_loss: 405090.6250 - val_mae: 449.7327\n",
            "Epoch 166/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1249854.5000 - mae: 872.4285 - val_loss: 418880.6562 - val_mae: 461.9347\n",
            "Epoch 167/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1242702.5000 - mae: 870.8795 - val_loss: 415091.8750 - val_mae: 455.2123\n",
            "Epoch 168/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1240946.8750 - mae: 865.6450 - val_loss: 428352.4062 - val_mae: 471.3965\n",
            "Epoch 169/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 1269010.3750 - mae: 877.5280 - val_loss: 431467.3438 - val_mae: 473.3116\n",
            "Epoch 170/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1274135.8750 - mae: 881.9498 - val_loss: 434052.0312 - val_mae: 479.4001\n",
            "Epoch 171/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1253975.5000 - mae: 871.8712 - val_loss: 444002.7188 - val_mae: 489.0565\n",
            "Epoch 172/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1253028.6250 - mae: 872.8945 - val_loss: 432845.1562 - val_mae: 478.4964\n",
            "Epoch 173/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1233967.5000 - mae: 867.9366 - val_loss: 393754.2500 - val_mae: 438.8232\n",
            "Epoch 174/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1240884.1250 - mae: 867.1395 - val_loss: 414288.8438 - val_mae: 459.6146\n",
            "Epoch 175/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1247599.1250 - mae: 866.3282 - val_loss: 417391.8750 - val_mae: 461.3646\n",
            "Epoch 176/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1248271.5000 - mae: 869.9628 - val_loss: 423656.4062 - val_mae: 470.0611\n",
            "Epoch 177/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1227383.5000 - mae: 859.9553 - val_loss: 416238.4688 - val_mae: 461.0833\n",
            "Epoch 178/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 1204309.3750 - mae: 847.4531 - val_loss: 422496.2188 - val_mae: 469.4724\n",
            "Epoch 179/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1220599.3750 - mae: 856.9047 - val_loss: 421929.7812 - val_mae: 469.7404\n",
            "Epoch 180/1000\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 1259124.2500 - mae: 877.9563 - val_loss: 428117.9688 - val_mae: 477.8081\n",
            "Epoch 181/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1226401.7500 - mae: 857.1974 - val_loss: 403548.4062 - val_mae: 450.9844\n",
            "Epoch 182/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1208410.0000 - mae: 852.7162 - val_loss: 429999.4688 - val_mae: 479.4210\n",
            "Epoch 183/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1223358.1250 - mae: 858.4249 - val_loss: 426327.6562 - val_mae: 477.4462\n",
            "Epoch 184/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1212070.8750 - mae: 853.5870 - val_loss: 419033.3438 - val_mae: 469.3925\n",
            "Epoch 185/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1232288.7500 - mae: 863.5659 - val_loss: 425174.0938 - val_mae: 477.6746\n",
            "Epoch 186/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 1208028.7500 - mae: 851.0153 - val_loss: 411180.4062 - val_mae: 461.3646\n",
            "Epoch 187/1000\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 1246223.6250 - mae: 866.2381 - val_loss: 434360.8438 - val_mae: 487.3602\n",
            "Epoch 188/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1196553.6250 - mae: 849.8541 - val_loss: 399707.5312 - val_mae: 451.0774\n",
            "Epoch 189/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 1200960.0000 - mae: 846.9851 - val_loss: 408929.5000 - val_mae: 461.2812\n",
            "Epoch 190/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1242440.1250 - mae: 865.8429 - val_loss: 408934.3750 - val_mae: 461.0833\n",
            "Epoch 191/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 1234047.0000 - mae: 858.5849 - val_loss: 404753.2500 - val_mae: 459.6145\n",
            "Epoch 192/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1217288.8750 - mae: 857.5017 - val_loss: 393893.1562 - val_mae: 449.3807\n",
            "Epoch 193/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 1205350.3750 - mae: 847.3737 - val_loss: 393363.3438 - val_mae: 449.3938\n",
            "Epoch 194/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1213273.5000 - mae: 855.7095 - val_loss: 406710.8750 - val_mae: 461.3646\n",
            "Epoch 195/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1210275.5000 - mae: 847.7889 - val_loss: 422996.7188 - val_mae: 479.1588\n",
            "Epoch 196/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1190750.0000 - mae: 842.1463 - val_loss: 418786.5000 - val_mae: 477.1011\n",
            "Epoch 197/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 1191816.0000 - mae: 837.7994 - val_loss: 401415.7812 - val_mae: 459.0521\n",
            "Epoch 198/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1205918.8750 - mae: 846.1251 - val_loss: 384099.5312 - val_mae: 441.3366\n",
            "Epoch 199/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1191946.3750 - mae: 846.5928 - val_loss: 414161.0312 - val_mae: 471.5058\n",
            "Epoch 200/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1222617.7500 - mae: 856.6519 - val_loss: 420163.7188 - val_mae: 479.5892\n",
            "Epoch 201/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1182916.7500 - mae: 839.8572 - val_loss: 419591.9062 - val_mae: 479.5627\n",
            "Epoch 202/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1187719.2500 - mae: 842.6634 - val_loss: 415393.0000 - val_mae: 477.2240\n",
            "Epoch 203/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1164925.8750 - mae: 834.1423 - val_loss: 381488.0000 - val_mae: 441.4695\n",
            "Epoch 204/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1194760.1250 - mae: 839.0494 - val_loss: 414265.1250 - val_mae: 477.4518\n",
            "Epoch 205/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1235347.2500 - mae: 865.5825 - val_loss: 393440.7812 - val_mae: 457.0208\n",
            "Epoch 206/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 1188921.0000 - mae: 840.9382 - val_loss: 376324.6250 - val_mae: 439.5179\n",
            "Epoch 207/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1198196.3750 - mae: 844.5240 - val_loss: 402456.0312 - val_mae: 467.0560\n",
            "Epoch 208/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 1187129.1250 - mae: 843.5197 - val_loss: 401901.9062 - val_mae: 467.0426\n",
            "Epoch 209/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1173831.7500 - mae: 835.1657 - val_loss: 397921.9688 - val_mae: 461.0000\n",
            "Epoch 210/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1218138.7500 - mae: 853.9547 - val_loss: 404404.8750 - val_mae: 469.0475\n",
            "Epoch 211/1000\n",
            "37/37 [==============================] - 10s 247ms/step - loss: 1202784.1250 - mae: 846.0967 - val_loss: 387369.5000 - val_mae: 451.3828\n",
            "Epoch 212/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1188934.0000 - mae: 842.7098 - val_loss: 393303.2500 - val_mae: 459.6146\n",
            "Epoch 213/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 1189222.3750 - mae: 836.6110 - val_loss: 382753.5000 - val_mae: 449.6593\n",
            "Epoch 214/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1191336.0000 - mae: 843.8438 - val_loss: 389383.7500 - val_mae: 453.7355\n",
            "Epoch 215/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 1221186.6250 - mae: 851.6910 - val_loss: 388276.5312 - val_mae: 453.3841\n",
            "Epoch 216/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 1203057.8750 - mae: 846.4449 - val_loss: 381137.5000 - val_mae: 449.4186\n",
            "Epoch 217/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1215239.2500 - mae: 851.6284 - val_loss: 386981.9688 - val_mae: 457.3021\n",
            "Epoch 218/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1196256.6250 - mae: 840.9207 - val_loss: 393592.1562 - val_mae: 461.0833\n",
            "Epoch 219/1000\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 1200934.2500 - mae: 847.5205 - val_loss: 393067.9062 - val_mae: 461.3646\n",
            "Epoch 220/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1215828.7500 - mae: 850.8007 - val_loss: 398843.3750 - val_mae: 468.9128\n",
            "Epoch 221/1000\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 1175045.1250 - mae: 836.0142 - val_loss: 395581.0312 - val_mae: 463.6771\n",
            "Epoch 222/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1207114.8750 - mae: 846.8906 - val_loss: 391449.8438 - val_mae: 461.3646\n",
            "Epoch 223/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1161021.0000 - mae: 827.7090 - val_loss: 403504.0938 - val_mae: 476.3813\n",
            "Epoch 224/1000\n",
            "37/37 [==============================] - 10s 241ms/step - loss: 1194055.6250 - mae: 840.1478 - val_loss: 400244.0938 - val_mae: 470.8912\n",
            "Epoch 225/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1199353.8750 - mae: 842.9756 - val_loss: 396164.7188 - val_mae: 469.1278\n",
            "Epoch 226/1000\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 1187903.6250 - mae: 836.6942 - val_loss: 395589.6250 - val_mae: 468.8331\n",
            "Epoch 227/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1191865.7500 - mae: 845.1879 - val_loss: 395054.3750 - val_mae: 468.8199\n",
            "Epoch 228/1000\n",
            "37/37 [==============================] - 6s 140ms/step - loss: 1181638.3750 - mae: 839.8353 - val_loss: 398052.7812 - val_mae: 470.8376\n",
            "Epoch 229/1000\n",
            "37/37 [==============================] - 8s 225ms/step - loss: 1162064.6250 - mae: 825.7062 - val_loss: 387737.9062 - val_mae: 461.3646\n",
            "Epoch 230/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 1178206.2500 - mae: 832.1552 - val_loss: 393450.3438 - val_mae: 469.0608\n",
            "Epoch 231/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1216576.3750 - mae: 851.9404 - val_loss: 399138.6562 - val_mae: 476.7301\n",
            "Epoch 232/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 1179176.2500 - mae: 838.5009 - val_loss: 398563.5938 - val_mae: 476.4223\n",
            "Epoch 233/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1162749.2500 - mae: 830.5334 - val_loss: 382092.1562 - val_mae: 459.3333\n",
            "Epoch 234/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1187433.0000 - mae: 836.9753 - val_loss: 378033.7812 - val_mae: 457.3021\n",
            "Epoch 235/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1183586.3750 - mae: 836.6412 - val_loss: 384585.8438 - val_mae: 461.3646\n",
            "Epoch 236/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1168099.0000 - mae: 827.7959 - val_loss: 367336.5312 - val_mae: 447.9356\n",
            "Epoch 237/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 1159577.3750 - mae: 825.0157 - val_loss: 386141.2812 - val_mae: 466.3743\n",
            "Epoch 238/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1173714.2500 - mae: 837.0778 - val_loss: 395318.0312 - val_mae: 476.2635\n",
            "Epoch 239/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1180400.8750 - mae: 831.6735 - val_loss: 398329.8750 - val_mae: 478.5493\n",
            "Epoch 240/1000\n",
            "37/37 [==============================] - 6s 137ms/step - loss: 1160304.1250 - mae: 827.1077 - val_loss: 375865.5938 - val_mae: 453.8011\n",
            "Epoch 241/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1192840.0000 - mae: 839.6646 - val_loss: 381488.5000 - val_mae: 461.6458\n",
            "Epoch 242/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1177851.5000 - mae: 834.1442 - val_loss: 371344.9688 - val_mae: 452.0785\n",
            "Epoch 243/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 1188721.2500 - mae: 834.6774 - val_loss: 376944.9688 - val_mae: 459.6145\n",
            "Epoch 244/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1145189.1250 - mae: 818.4979 - val_loss: 383426.4688 - val_mae: 463.3958\n",
            "Epoch 245/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 1136731.5000 - mae: 814.6272 - val_loss: 373363.3750 - val_mae: 454.1492\n",
            "Epoch 246/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1187479.6250 - mae: 836.2927 - val_loss: 372870.5938 - val_mae: 454.1623\n",
            "Epoch 247/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1160847.3750 - mae: 827.3978 - val_loss: 371762.7188 - val_mae: 453.5298\n",
            "Epoch 248/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 1189270.7500 - mae: 837.2789 - val_loss: 387392.7812 - val_mae: 470.5716\n",
            "Epoch 249/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 1156099.7500 - mae: 823.2272 - val_loss: 383414.9062 - val_mae: 468.8084\n",
            "Epoch 250/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1179986.7500 - mae: 835.0519 - val_loss: 382886.2188 - val_mae: 468.7949\n",
            "Epoch 251/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1150173.5000 - mae: 820.5162 - val_loss: 382323.2500 - val_mae: 468.5002\n",
            "Epoch 252/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 1154771.6250 - mae: 821.6746 - val_loss: 385349.3438 - val_mae: 470.7999\n",
            "Epoch 253/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1140745.2500 - mae: 813.3591 - val_loss: 362424.2500 - val_mae: 450.1925\n",
            "Epoch 254/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1139520.8750 - mae: 817.1143 - val_loss: 378293.2500 - val_mae: 463.1146\n",
            "Epoch 255/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 1146396.3750 - mae: 815.2419 - val_loss: 383796.7500 - val_mae: 470.7602\n",
            "Epoch 256/1000\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 1159116.1250 - mae: 823.7969 - val_loss: 366927.7812 - val_mae: 457.5833\n",
            "Epoch 257/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1109129.0000 - mae: 802.2184 - val_loss: 376820.8438 - val_mae: 463.3958\n",
            "Epoch 258/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 1148087.1250 - mae: 820.2186 - val_loss: 369418.9062 - val_mae: 459.6146\n",
            "Epoch 259/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1135914.0000 - mae: 812.2974 - val_loss: 372351.7812 - val_mae: 461.3645\n",
            "Epoch 260/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1157589.8750 - mae: 823.0790 - val_loss: 381155.2500 - val_mae: 470.1317\n",
            "Epoch 261/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 1165711.7500 - mae: 829.7594 - val_loss: 368944.4062 - val_mae: 456.3919\n",
            "Epoch 262/1000\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 1173095.0000 - mae: 831.5569 - val_loss: 370319.9688 - val_mae: 461.2812\n",
            "Epoch 263/1000\n",
            "37/37 [==============================] - 5s 101ms/step - loss: 1129448.6250 - mae: 812.3658 - val_loss: 376192.6562 - val_mae: 468.3418\n",
            "Epoch 264/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1151786.7500 - mae: 822.9486 - val_loss: 357067.9688 - val_mae: 450.0566\n",
            "Epoch 265/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1125245.5000 - mae: 808.6252 - val_loss: 384499.0312 - val_mae: 477.5786\n",
            "Epoch 266/1000\n",
            "37/37 [==============================] - 10s 236ms/step - loss: 1137827.6250 - mae: 813.6060 - val_loss: 365374.3750 - val_mae: 459.0521\n",
            "Epoch 267/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1154452.5000 - mae: 820.6015 - val_loss: 364923.4062 - val_mae: 459.3333\n",
            "Epoch 268/1000\n",
            "37/37 [==============================] - 9s 245ms/step - loss: 1152226.5000 - mae: 825.2701 - val_loss: 358643.0938 - val_mae: 452.4221\n",
            "Epoch 269/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1121303.8750 - mae: 806.5602 - val_loss: 369773.5938 - val_mae: 466.5127\n",
            "Epoch 270/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1124710.6250 - mae: 809.5464 - val_loss: 362855.5000 - val_mae: 458.6875\n",
            "Epoch 271/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1146235.2500 - mae: 817.6652 - val_loss: 360597.5000 - val_mae: 454.2120\n",
            "Epoch 272/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1133946.7500 - mae: 813.1171 - val_loss: 375140.6562 - val_mae: 470.5353\n",
            "Epoch 273/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1143504.3750 - mae: 821.0465 - val_loss: 365412.3750 - val_mae: 461.3646\n",
            "Epoch 274/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1119297.8750 - mae: 804.4043 - val_loss: 368354.1562 - val_mae: 463.3958\n",
            "Epoch 275/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1148540.5000 - mae: 819.0756 - val_loss: 366774.9062 - val_mae: 466.4330\n",
            "Epoch 276/1000\n",
            "37/37 [==============================] - 10s 244ms/step - loss: 1105978.2500 - mae: 795.7953 - val_loss: 375494.1250 - val_mae: 475.8185\n",
            "Epoch 277/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1128629.1250 - mae: 810.5919 - val_loss: 363473.8750 - val_mae: 461.3646\n",
            "Epoch 278/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1141107.2500 - mae: 813.3174 - val_loss: 377813.0938 - val_mae: 477.2344\n",
            "Epoch 279/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1148866.8750 - mae: 819.5944 - val_loss: 359127.9062 - val_mae: 459.6146\n",
            "Epoch 280/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1138608.2500 - mae: 813.7829 - val_loss: 373328.5312 - val_mae: 474.8685\n",
            "Epoch 281/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1125796.7500 - mae: 805.7713 - val_loss: 363814.0312 - val_mae: 466.3534\n",
            "Epoch 282/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1093708.7500 - mae: 796.0443 - val_loss: 372460.4688 - val_mae: 475.6594\n",
            "Epoch 283/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 1115642.7500 - mae: 807.1069 - val_loss: 362791.6562 - val_mae: 466.0458\n",
            "Epoch 284/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1102545.2500 - mae: 797.5305 - val_loss: 356735.9062 - val_mae: 459.6146\n",
            "Epoch 285/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1122561.2500 - mae: 806.2133 - val_loss: 365219.4062 - val_mae: 468.0506\n",
            "Epoch 286/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 1127199.1250 - mae: 808.7172 - val_loss: 355793.7188 - val_mae: 459.6146\n",
            "Epoch 287/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1133712.3750 - mae: 810.2844 - val_loss: 364187.5000 - val_mae: 467.7427\n",
            "Epoch 288/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1102380.5000 - mae: 798.8622 - val_loss: 367139.5938 - val_mae: 470.0420\n",
            "Epoch 289/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1151459.6250 - mae: 817.4340 - val_loss: 363200.9062 - val_mae: 467.7159\n",
            "Epoch 290/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 1115943.6250 - mae: 807.7741 - val_loss: 344840.0625 - val_mae: 450.4015\n",
            "Epoch 291/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1104216.5000 - mae: 798.8713 - val_loss: 351205.3438 - val_mae: 454.7584\n",
            "Epoch 292/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1095903.2500 - mae: 796.9958 - val_loss: 352837.3438 - val_mae: 459.0520\n",
            "Epoch 293/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1119595.3750 - mae: 803.8020 - val_loss: 352413.8438 - val_mae: 459.3333\n",
            "Epoch 294/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 1134074.3750 - mae: 815.2444 - val_loss: 346441.3438 - val_mae: 452.7669\n",
            "Epoch 295/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1114345.7500 - mae: 801.6328 - val_loss: 348761.6562 - val_mae: 454.1656\n",
            "Epoch 296/1000\n",
            "37/37 [==============================] - 5s 121ms/step - loss: 1097714.2500 - mae: 795.4766 - val_loss: 356535.6250 - val_mae: 466.1544\n",
            "Epoch 297/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1081423.8750 - mae: 787.3473 - val_loss: 364864.7500 - val_mae: 474.4188\n",
            "Epoch 298/1000\n",
            "37/37 [==============================] - 6s 144ms/step - loss: 1136254.0000 - mae: 811.6494 - val_loss: 362329.3438 - val_mae: 470.1907\n",
            "Epoch 299/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1100998.6250 - mae: 792.8270 - val_loss: 358542.2500 - val_mae: 468.4277\n",
            "Epoch 300/1000\n",
            "37/37 [==============================] - 10s 244ms/step - loss: 1114542.0000 - mae: 801.4503 - val_loss: 357995.1562 - val_mae: 468.1328\n",
            "Epoch 301/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1116737.2500 - mae: 803.9819 - val_loss: 362892.4688 - val_mae: 474.3121\n",
            "Epoch 302/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 1071778.2500 - mae: 786.3169 - val_loss: 360344.4062 - val_mae: 469.8562\n",
            "Epoch 303/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1084381.6250 - mae: 787.2599 - val_loss: 361926.1250 - val_mae: 474.2596\n",
            "Epoch 304/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1119964.8750 - mae: 802.9340 - val_loss: 356095.9062 - val_mae: 468.0799\n",
            "Epoch 305/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1091697.0000 - mae: 790.1197 - val_loss: 350193.2188 - val_mae: 461.3646\n",
            "Epoch 306/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 1109586.7500 - mae: 797.4708 - val_loss: 349739.3438 - val_mae: 461.3646\n",
            "Epoch 307/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1095012.6250 - mae: 794.6418 - val_loss: 342581.9062 - val_mae: 457.3021\n",
            "Epoch 308/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 1105681.1250 - mae: 795.3604 - val_loss: 345469.0625 - val_mae: 459.3333\n",
            "Epoch 309/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1111681.7500 - mae: 802.3679 - val_loss: 339605.5000 - val_mae: 452.6843\n",
            "Epoch 310/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1099800.1250 - mae: 792.7994 - val_loss: 332491.6562 - val_mae: 448.6349\n",
            "Epoch 311/1000\n",
            "37/37 [==============================] - 6s 140ms/step - loss: 1108328.1250 - mae: 797.5886 - val_loss: 340757.0625 - val_mae: 457.3021\n",
            "Epoch 312/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 1081784.0000 - mae: 789.9392 - val_loss: 340253.2812 - val_mae: 457.0208\n",
            "Epoch 313/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1122387.3750 - mae: 806.7604 - val_loss: 334495.1562 - val_mae: 450.7061\n",
            "Epoch 314/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 1108446.0000 - mae: 801.5714 - val_loss: 359459.7812 - val_mae: 475.9157\n",
            "Epoch 315/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 1112260.0000 - mae: 800.8810 - val_loss: 364839.4688 - val_mae: 482.8228\n",
            "Epoch 316/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1101412.6250 - mae: 790.8654 - val_loss: 339880.7500 - val_mae: 455.0895\n",
            "Epoch 317/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1025083.5000 - mae: 759.3936 - val_loss: 342711.3750 - val_mae: 456.8524\n",
            "Epoch 318/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1108037.1250 - mae: 795.6970 - val_loss: 344172.1250 - val_mae: 461.0833\n",
            "Epoch 319/1000\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 1102127.7500 - mae: 795.2374 - val_loss: 354240.9688 - val_mae: 473.8357\n",
            "Epoch 320/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 1106191.5000 - mae: 798.1098 - val_loss: 348678.3438 - val_mae: 468.1496\n",
            "Epoch 321/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 1102355.2500 - mae: 795.1993 - val_loss: 353298.0000 - val_mae: 473.7829\n",
            "Epoch 322/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1055754.7500 - mae: 774.7904 - val_loss: 356269.6562 - val_mae: 476.3510\n",
            "Epoch 323/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 1048403.3125 - mae: 772.3314 - val_loss: 335356.0000 - val_mae: 457.0208\n",
            "Epoch 324/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1067653.6250 - mae: 781.1901 - val_loss: 338274.2812 - val_mae: 459.3333\n",
            "Epoch 325/1000\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 1066999.5000 - mae: 782.8112 - val_loss: 346349.2188 - val_mae: 467.8029\n",
            "Epoch 326/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1097477.1250 - mae: 794.7030 - val_loss: 325654.0625 - val_mae: 449.1267\n",
            "Epoch 327/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1074793.8750 - mae: 786.4103 - val_loss: 336960.0312 - val_mae: 459.3333\n",
            "Epoch 328/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1075635.6250 - mae: 784.0778 - val_loss: 344935.0938 - val_mae: 467.4825\n",
            "Epoch 329/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1075106.1250 - mae: 781.8604 - val_loss: 330966.9688 - val_mae: 453.2285\n",
            "Epoch 330/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1101554.3750 - mae: 801.3029 - val_loss: 344089.2812 - val_mae: 467.7372\n",
            "Epoch 331/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 1078063.2500 - mae: 782.2061 - val_loss: 343564.7812 - val_mae: 467.4425\n",
            "Epoch 332/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1098563.3750 - mae: 793.3417 - val_loss: 343181.2188 - val_mae: 467.7106\n",
            "Epoch 333/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 1063154.1250 - mae: 775.1547 - val_loss: 334252.3750 - val_mae: 459.0521\n",
            "Epoch 334/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 1060303.5000 - mae: 775.6942 - val_loss: 333939.4688 - val_mae: 459.6146\n",
            "Epoch 335/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1065859.2500 - mae: 779.7689 - val_loss: 336779.8438 - val_mae: 461.6458\n",
            "Epoch 336/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 1087818.6250 - mae: 788.9734 - val_loss: 341310.3438 - val_mae: 467.3763\n",
            "Epoch 337/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1060402.5000 - mae: 773.5631 - val_loss: 332498.3438 - val_mae: 459.0521\n",
            "Epoch 338/1000\n",
            "37/37 [==============================] - 5s 102ms/step - loss: 1056658.3750 - mae: 775.2500 - val_loss: 345565.3750 - val_mae: 473.8977\n",
            "Epoch 339/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1077704.0000 - mae: 784.4299 - val_loss: 340097.2188 - val_mae: 467.8990\n",
            "Epoch 340/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 1086537.2500 - mae: 786.6018 - val_loss: 337781.8438 - val_mae: 463.3958\n",
            "Epoch 341/1000\n",
            "37/37 [==============================] - 5s 120ms/step - loss: 1050196.6250 - mae: 773.9221 - val_loss: 325779.9375 - val_mae: 453.1068\n",
            "Epoch 342/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 1068712.7500 - mae: 783.4152 - val_loss: 318849.4062 - val_mae: 449.0575\n",
            "Epoch 343/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1063113.0000 - mae: 778.9559 - val_loss: 343349.5312 - val_mae: 474.0462\n",
            "Epoch 344/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1074372.2500 - mae: 782.4140 - val_loss: 337744.0000 - val_mae: 467.2704\n",
            "Epoch 345/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 1074762.0000 - mae: 781.5089 - val_loss: 329119.5938 - val_mae: 459.3333\n",
            "Epoch 346/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1074269.8750 - mae: 783.1263 - val_loss: 336924.3125 - val_mae: 467.5249\n",
            "Epoch 347/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1033042.0000 - mae: 769.3803 - val_loss: 328267.2188 - val_mae: 459.3333\n",
            "Epoch 348/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 1054733.3750 - mae: 771.4757 - val_loss: 336131.5312 - val_mae: 467.7802\n",
            "Epoch 349/1000\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 1047695.5625 - mae: 769.3729 - val_loss: 330734.4062 - val_mae: 461.6458\n",
            "Epoch 350/1000\n",
            "37/37 [==============================] - 9s 245ms/step - loss: 1053127.1250 - mae: 776.2980 - val_loss: 328525.9062 - val_mae: 457.2876\n",
            "Epoch 351/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1052428.3750 - mae: 771.0237 - val_loss: 342949.8438 - val_mae: 475.5867\n",
            "Epoch 352/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 1039691.1250 - mae: 769.5319 - val_loss: 323018.7500 - val_mae: 457.5833\n",
            "Epoch 353/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1071170.8750 - mae: 783.0363 - val_loss: 332209.4062 - val_mae: 463.3958\n",
            "Epoch 354/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 1047799.0625 - mae: 771.6170 - val_loss: 333397.4375 - val_mae: 467.1391\n",
            "Epoch 355/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1052545.3750 - mae: 769.6611 - val_loss: 345915.9062 - val_mae: 480.9610\n",
            "Epoch 356/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1063047.7500 - mae: 776.4279 - val_loss: 327638.4688 - val_mae: 461.0833\n",
            "Epoch 357/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 1048600.8750 - mae: 771.7476 - val_loss: 328886.4688 - val_mae: 465.0682\n",
            "Epoch 358/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1075857.3750 - mae: 785.4373 - val_loss: 326880.0000 - val_mae: 461.3646\n",
            "Epoch 359/1000\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 1058267.7500 - mae: 772.0027 - val_loss: 323329.6250 - val_mae: 459.6146\n",
            "Epoch 360/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1056776.0000 - mae: 774.7646 - val_loss: 329258.0312 - val_mae: 463.3958\n",
            "Epoch 361/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1065499.8750 - mae: 779.8566 - val_loss: 325701.9375 - val_mae: 461.6458\n",
            "Epoch 362/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 1075415.0000 - mae: 786.6761 - val_loss: 325129.9375 - val_mae: 461.0833\n",
            "Epoch 363/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1057371.6250 - mae: 776.4243 - val_loss: 329452.7188 - val_mae: 466.7389\n",
            "Epoch 364/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 1028456.5000 - mae: 759.9749 - val_loss: 313170.6562 - val_mae: 451.3785\n",
            "Epoch 365/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1017984.7500 - mae: 756.9355 - val_loss: 320759.4688 - val_mae: 459.3333\n",
            "Epoch 366/1000\n",
            "37/37 [==============================] - 10s 243ms/step - loss: 1055059.6250 - mae: 775.7390 - val_loss: 317239.7500 - val_mae: 457.5833\n",
            "Epoch 367/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1037363.6250 - mae: 768.8640 - val_loss: 315252.7500 - val_mae: 453.7305\n",
            "Epoch 368/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 1023686.6250 - mae: 758.8206 - val_loss: 311665.3438 - val_mae: 451.7125\n",
            "Epoch 369/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 1016357.9375 - mae: 757.6179 - val_loss: 322241.6875 - val_mae: 461.0833\n",
            "Epoch 370/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1027325.0000 - mae: 763.7193 - val_loss: 331229.1562 - val_mae: 472.4910\n",
            "Epoch 371/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 1050814.0000 - mae: 773.5341 - val_loss: 338793.8438 - val_mae: 480.6090\n",
            "Epoch 372/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1042695.5000 - mae: 767.5382 - val_loss: 317835.0938 - val_mae: 459.0521\n",
            "Epoch 373/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1046725.0625 - mae: 772.0621 - val_loss: 325389.9062 - val_mae: 467.1694\n",
            "Epoch 374/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 1027231.5625 - mae: 763.3406 - val_loss: 321725.7812 - val_mae: 464.8439\n",
            "Epoch 375/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 1044624.7500 - mae: 769.2964 - val_loss: 327811.8750 - val_mae: 469.4557\n",
            "Epoch 376/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1045962.3750 - mae: 768.3024 - val_loss: 324062.8750 - val_mae: 466.8486\n",
            "Epoch 377/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1032925.5625 - mae: 759.9629 - val_loss: 331466.3750 - val_mae: 474.6181\n",
            "Epoch 378/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1023660.6875 - mae: 759.3628 - val_loss: 326398.5312 - val_mae: 468.8535\n",
            "Epoch 379/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 1042492.8125 - mae: 767.0461 - val_loss: 315095.2188 - val_mae: 459.3333\n",
            "Epoch 380/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1008974.0000 - mae: 755.7346 - val_loss: 317849.5938 - val_mae: 461.3646\n",
            "Epoch 381/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1048649.6250 - mae: 775.5141 - val_loss: 321987.3438 - val_mae: 466.7824\n",
            "Epoch 382/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1010896.0000 - mae: 755.6226 - val_loss: 321673.8125 - val_mae: 467.0509\n",
            "Epoch 383/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1040226.3125 - mae: 767.9005 - val_loss: 316662.4375 - val_mae: 461.3646\n",
            "Epoch 384/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1022895.8125 - mae: 756.8304 - val_loss: 325365.8438 - val_mae: 472.4034\n",
            "Epoch 385/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1027257.5000 - mae: 758.4181 - val_loss: 325028.4688 - val_mae: 472.6582\n",
            "Epoch 386/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1045735.0625 - mae: 770.4665 - val_loss: 319942.2188 - val_mae: 466.7164\n",
            "Epoch 387/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1031786.6875 - mae: 764.6541 - val_loss: 311849.6875 - val_mae: 459.0521\n",
            "Epoch 388/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1028776.5625 - mae: 761.5983 - val_loss: 322264.4062 - val_mae: 468.7212\n",
            "Epoch 389/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1036647.7500 - mae: 768.8558 - val_loss: 311057.5000 - val_mae: 459.0521\n",
            "Epoch 390/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1036953.2500 - mae: 770.2576 - val_loss: 303180.5312 - val_mae: 452.0028\n",
            "Epoch 391/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1035259.5625 - mae: 763.9799 - val_loss: 322427.1875 - val_mae: 472.2173\n",
            "Epoch 392/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 1051282.2500 - mae: 775.5396 - val_loss: 313159.8750 - val_mae: 461.6458\n",
            "Epoch 393/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 988735.4375 - mae: 749.4000 - val_loss: 301944.7500 - val_mae: 451.7620\n",
            "Epoch 394/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1026873.5000 - mae: 762.0214 - val_loss: 316601.2500 - val_mae: 466.3289\n",
            "Epoch 395/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 1008287.6875 - mae: 751.3090 - val_loss: 313085.8750 - val_mae: 464.2845\n",
            "Epoch 396/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1025309.8750 - mae: 760.5359 - val_loss: 307067.6562 - val_mae: 455.8636\n",
            "Epoch 397/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 1021403.0625 - mae: 756.4756 - val_loss: 303662.4688 - val_mae: 454.1270\n",
            "Epoch 398/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1013916.1250 - mae: 752.1165 - val_loss: 318223.8438 - val_mae: 468.5894\n",
            "Epoch 399/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 984642.4375 - mae: 738.1694 - val_loss: 313410.9688 - val_mae: 463.1146\n",
            "Epoch 400/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 1015878.3750 - mae: 754.6629 - val_loss: 306896.3125 - val_mae: 459.3333\n",
            "Epoch 401/1000\n",
            "37/37 [==============================] - 9s 245ms/step - loss: 1013464.3750 - mae: 754.3380 - val_loss: 318356.4688 - val_mae: 471.9553\n",
            "Epoch 402/1000\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 1004333.4375 - mae: 749.3365 - val_loss: 306051.8750 - val_mae: 459.0521\n",
            "Epoch 403/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 982038.0625 - mae: 743.4659 - val_loss: 313344.7500 - val_mae: 467.0556\n",
            "Epoch 404/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1021670.0625 - mae: 759.9462 - val_loss: 301012.2500 - val_mae: 453.9368\n",
            "Epoch 405/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 988911.0000 - mae: 741.8764 - val_loss: 312378.5312 - val_mae: 466.4667\n",
            "Epoch 406/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 1037289.9375 - mae: 764.6935 - val_loss: 316333.4062 - val_mae: 471.8231\n",
            "Epoch 407/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 1008493.4375 - mae: 751.2670 - val_loss: 311771.4688 - val_mae: 467.0026\n",
            "Epoch 408/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1011180.5625 - mae: 751.9547 - val_loss: 299637.4688 - val_mae: 454.2708\n",
            "Epoch 409/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1012316.1250 - mae: 754.0639 - val_loss: 310819.0312 - val_mae: 466.4140\n",
            "Epoch 410/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 983630.3750 - mae: 742.9703 - val_loss: 310429.4688 - val_mae: 466.4008\n",
            "Epoch 411/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 994931.6250 - mae: 747.3395 - val_loss: 310138.2500 - val_mae: 466.6689\n",
            "Epoch 412/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 1022793.2500 - mae: 757.0138 - val_loss: 305362.0312 - val_mae: 461.0833\n",
            "Epoch 413/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 996589.6875 - mae: 746.5836 - val_loss: 299021.4062 - val_mae: 457.5833\n",
            "Epoch 414/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 994714.3750 - mae: 744.7123 - val_loss: 308879.7188 - val_mae: 466.3478\n",
            "Epoch 415/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 1023591.7500 - mae: 762.1849 - val_loss: 316004.9375 - val_mae: 474.1794\n",
            "Epoch 416/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1000868.8750 - mae: 748.5413 - val_loss: 315231.2812 - val_mae: 473.0280\n",
            "Epoch 417/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 991718.7500 - mae: 742.2172 - val_loss: 315030.0000 - val_mae: 473.5645\n",
            "Epoch 418/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1014004.1875 - mae: 754.0655 - val_loss: 296998.7188 - val_mae: 457.0208\n",
            "Epoch 419/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 968283.8125 - mae: 735.6746 - val_loss: 307068.5312 - val_mae: 466.5634\n",
            "Epoch 420/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1003300.4375 - mae: 749.1781 - val_loss: 306784.6875 - val_mae: 466.8315\n",
            "Epoch 421/1000\n",
            "37/37 [==============================] - 6s 138ms/step - loss: 1003155.5000 - mae: 749.0972 - val_loss: 298965.4062 - val_mae: 459.0521\n",
            "Epoch 422/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 974602.2500 - mae: 737.9406 - val_loss: 304809.0000 - val_mae: 463.3958\n",
            "Epoch 423/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 988775.3125 - mae: 743.5397 - val_loss: 305666.3438 - val_mae: 466.7924\n",
            "Epoch 424/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 987648.2500 - mae: 742.2437 - val_loss: 309357.3125 - val_mae: 471.6315\n",
            "Epoch 425/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 994035.4375 - mae: 744.9448 - val_loss: 304722.9688 - val_mae: 466.2037\n",
            "Epoch 426/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 1010280.8125 - mae: 750.6863 - val_loss: 297262.8750 - val_mae: 459.3333\n",
            "Epoch 427/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 989260.3750 - mae: 740.4606 - val_loss: 307120.0938 - val_mae: 468.4902\n",
            "Epoch 428/1000\n",
            "37/37 [==============================] - 5s 121ms/step - loss: 968616.6250 - mae: 739.6748 - val_loss: 292438.6250 - val_mae: 454.2520\n",
            "Epoch 429/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 978811.9375 - mae: 738.4901 - val_loss: 299128.5312 - val_mae: 461.0833\n",
            "Epoch 430/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 1014461.8125 - mae: 757.1987 - val_loss: 295740.3438 - val_mae: 459.0521\n",
            "Epoch 431/1000\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 968336.7500 - mae: 734.9235 - val_loss: 298506.6250 - val_mae: 461.3646\n",
            "Epoch 432/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 964670.4375 - mae: 731.6908 - val_loss: 302331.1562 - val_mae: 466.6746\n",
            "Epoch 433/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 998037.5000 - mae: 744.3351 - val_loss: 312916.2812 - val_mae: 478.1610\n",
            "Epoch 434/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 991367.6250 - mae: 744.3677 - val_loss: 297436.3438 - val_mae: 461.3646\n",
            "Epoch 435/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 982432.0000 - mae: 739.1105 - val_loss: 293127.6875 - val_mae: 456.6569\n",
            "Epoch 436/1000\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 975530.0625 - mae: 734.5236 - val_loss: 293605.5625 - val_mae: 459.0521\n",
            "Epoch 437/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 946404.1875 - mae: 724.0614 - val_loss: 297298.0625 - val_mae: 464.0155\n",
            "Epoch 438/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 965542.0625 - mae: 728.0645 - val_loss: 292130.5000 - val_mae: 456.6951\n",
            "Epoch 439/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 971532.5625 - mae: 734.7968 - val_loss: 288305.7188 - val_mae: 454.3122\n",
            "Epoch 440/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 978990.6875 - mae: 735.3134 - val_loss: 302241.0312 - val_mae: 468.0393\n",
            "Epoch 441/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 969860.7500 - mae: 730.6422 - val_loss: 302861.2500 - val_mae: 470.9068\n",
            "Epoch 442/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 985733.1875 - mae: 739.9644 - val_loss: 294533.7188 - val_mae: 461.0833\n",
            "Epoch 443/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 978184.6875 - mae: 736.2576 - val_loss: 288393.1562 - val_mae: 457.5833\n",
            "Epoch 444/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 975603.6250 - mae: 739.1039 - val_loss: 300769.9062 - val_mae: 467.9862\n",
            "Epoch 445/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 997872.5625 - mae: 748.0001 - val_loss: 289752.2812 - val_mae: 456.7871\n",
            "Epoch 446/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 967285.9375 - mae: 728.3406 - val_loss: 289411.4062 - val_mae: 456.8004\n",
            "Epoch 447/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 993052.6875 - mae: 744.1469 - val_loss: 292883.5938 - val_mae: 461.3646\n",
            "Epoch 448/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 975312.0000 - mae: 737.4632 - val_loss: 292431.2188 - val_mae: 461.0833\n",
            "Epoch 449/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 986887.8125 - mae: 742.9869 - val_loss: 299065.1250 - val_mae: 468.2013\n",
            "Epoch 450/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 958818.1250 - mae: 730.9115 - val_loss: 288877.4062 - val_mae: 459.3333\n",
            "Epoch 451/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 981316.8750 - mae: 739.4453 - val_loss: 298352.1562 - val_mae: 468.1751\n",
            "Epoch 452/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 980888.8125 - mae: 741.5306 - val_loss: 292050.1562 - val_mae: 464.0994\n",
            "Epoch 453/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 965130.5000 - mae: 732.5558 - val_loss: 284097.0938 - val_mae: 454.8617\n",
            "Epoch 454/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 973397.0625 - mae: 734.6662 - val_loss: 294304.0938 - val_mae: 466.1038\n",
            "Epoch 455/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 953305.8125 - mae: 727.6484 - val_loss: 284082.3438 - val_mae: 457.0208\n",
            "Epoch 456/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 951090.0000 - mae: 724.5567 - val_loss: 289774.1250 - val_mae: 461.3646\n",
            "Epoch 457/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 959596.1250 - mae: 727.8477 - val_loss: 286481.8438 - val_mae: 459.3333\n",
            "Epoch 458/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 953859.6250 - mae: 728.3611 - val_loss: 286257.4062 - val_mae: 459.6146\n",
            "Epoch 459/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 939367.0625 - mae: 720.8619 - val_loss: 299302.2500 - val_mae: 472.7428\n",
            "Epoch 460/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 937838.1250 - mae: 723.8668 - val_loss: 284764.6562 - val_mae: 456.9854\n",
            "Epoch 461/1000\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 940450.7500 - mae: 718.1827 - val_loss: 291164.6250 - val_mae: 463.6771\n",
            "Epoch 462/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 960410.8125 - mae: 727.1210 - val_loss: 294374.1875 - val_mae: 467.7493\n",
            "Epoch 463/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 990492.5000 - mae: 744.7209 - val_loss: 287314.5312 - val_mae: 461.0833\n",
            "Epoch 464/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 949373.8750 - mae: 718.3366 - val_loss: 287099.5938 - val_mae: 461.3646\n",
            "Epoch 465/1000\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 956035.3125 - mae: 726.9781 - val_loss: 294102.5938 - val_mae: 470.2726\n",
            "Epoch 466/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 951280.7500 - mae: 725.8303 - val_loss: 290036.0312 - val_mae: 465.6648\n",
            "Epoch 467/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 944721.1875 - mae: 723.9387 - val_loss: 296223.5938 - val_mae: 471.9702\n",
            "Epoch 468/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 974593.7500 - mae: 735.0871 - val_loss: 282957.0625 - val_mae: 459.6146\n",
            "Epoch 469/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 937634.8750 - mae: 715.1257 - val_loss: 282408.2188 - val_mae: 459.0521\n",
            "Epoch 470/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 929984.8750 - mae: 718.4528 - val_loss: 278641.7500 - val_mae: 455.0851\n",
            "Epoch 471/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 972291.6250 - mae: 737.7066 - val_loss: 281988.3125 - val_mae: 459.6146\n",
            "Epoch 472/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 944781.4375 - mae: 717.8715 - val_loss: 288124.8438 - val_mae: 465.8677\n",
            "Epoch 473/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 960600.1875 - mae: 728.2940 - val_loss: 284259.5938 - val_mae: 461.6458\n",
            "Epoch 474/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 957704.6250 - mae: 725.5131 - val_loss: 287338.7812 - val_mae: 465.5602\n",
            "Epoch 475/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 944232.6875 - mae: 718.6275 - val_loss: 290515.5938 - val_mae: 469.7301\n",
            "Epoch 476/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 918486.8125 - mae: 706.9764 - val_loss: 283305.6562 - val_mae: 461.6458\n",
            "Epoch 477/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 944044.6875 - mae: 718.3752 - val_loss: 289259.4688 - val_mae: 467.5526\n",
            "Epoch 478/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 937785.6875 - mae: 718.9324 - val_loss: 286139.2812 - val_mae: 465.7897\n",
            "Epoch 479/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 943803.0625 - mae: 720.9303 - val_loss: 282108.0312 - val_mae: 461.0833\n",
            "Epoch 480/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 921918.3750 - mae: 709.2302 - val_loss: 291820.5312 - val_mae: 471.9116\n",
            "Epoch 481/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 949388.8125 - mae: 723.5220 - val_loss: 288353.4062 - val_mae: 469.2924\n",
            "Epoch 482/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 945103.7500 - mae: 720.3652 - val_loss: 274954.9688 - val_mae: 455.2417\n",
            "Epoch 483/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 942433.1250 - mae: 721.2746 - val_loss: 274636.9062 - val_mae: 455.2554\n",
            "Epoch 484/1000\n",
            "37/37 [==============================] - 5s 100ms/step - loss: 944924.8125 - mae: 718.5562 - val_loss: 277753.0938 - val_mae: 459.3333\n",
            "Epoch 485/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 935814.8125 - mae: 712.7645 - val_loss: 274047.1562 - val_mae: 455.2810\n",
            "Epoch 486/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 926763.5000 - mae: 709.7804 - val_loss: 280755.3750 - val_mae: 463.9352\n",
            "Epoch 487/1000\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 946401.0000 - mae: 720.3730 - val_loss: 286454.1250 - val_mae: 469.4171\n",
            "Epoch 488/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 939710.2500 - mae: 716.0372 - val_loss: 285763.0312 - val_mae: 467.6905\n",
            "Epoch 489/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 938514.5625 - mae: 716.3668 - val_loss: 279085.2188 - val_mae: 461.3646\n",
            "Epoch 490/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 942998.6250 - mae: 719.2048 - val_loss: 275305.7812 - val_mae: 457.0965\n",
            "Epoch 491/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 901068.5000 - mae: 698.7922 - val_loss: 291692.6562 - val_mae: 476.1620\n",
            "Epoch 492/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 928530.6250 - mae: 711.0596 - val_loss: 291349.5938 - val_mae: 476.1228\n",
            "Epoch 493/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 943145.1875 - mae: 720.4041 - val_loss: 283903.1250 - val_mae: 467.0626\n",
            "Epoch 494/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 908181.4375 - mae: 702.6470 - val_loss: 280717.8125 - val_mae: 465.0183\n",
            "Epoch 495/1000\n",
            "37/37 [==============================] - 10s 243ms/step - loss: 948630.3125 - mae: 727.8470 - val_loss: 277235.7188 - val_mae: 461.3646\n",
            "Epoch 496/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 941676.9375 - mae: 715.7690 - val_loss: 271210.8750 - val_mae: 457.3021\n",
            "Epoch 497/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 944784.0000 - mae: 723.9536 - val_loss: 286385.8750 - val_mae: 472.0316\n",
            "Epoch 498/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 909457.1875 - mae: 703.4394 - val_loss: 276328.4062 - val_mae: 461.3646\n",
            "Epoch 499/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 934859.8125 - mae: 716.0030 - val_loss: 279260.7500 - val_mae: 465.2343\n",
            "Epoch 500/1000\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 922891.5000 - mae: 708.6776 - val_loss: 279076.1562 - val_mae: 465.5027\n",
            "Epoch 501/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 936620.8125 - mae: 719.3964 - val_loss: 269221.1562 - val_mae: 455.2085\n",
            "Epoch 502/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 913985.6250 - mae: 709.6196 - val_loss: 268941.8125 - val_mae: 455.2212\n",
            "Epoch 503/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 919976.8125 - mae: 708.9666 - val_loss: 280861.7500 - val_mae: 467.2136\n",
            "Epoch 504/1000\n",
            "37/37 [==============================] - 5s 100ms/step - loss: 936004.3125 - mae: 715.7828 - val_loss: 283413.6875 - val_mae: 470.9221\n",
            "Epoch 505/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 915587.6875 - mae: 703.9240 - val_loss: 283421.5312 - val_mae: 470.9797\n",
            "Epoch 506/1000\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 919638.1875 - mae: 706.1804 - val_loss: 273934.3125 - val_mae: 461.3646\n",
            "Epoch 507/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 911306.0000 - mae: 706.2687 - val_loss: 279754.2188 - val_mae: 467.4431\n",
            "Epoch 508/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 923857.1875 - mae: 712.9841 - val_loss: 282585.7188 - val_mae: 471.1827\n",
            "Epoch 509/1000\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 950266.9375 - mae: 723.4167 - val_loss: 261298.2344 - val_mae: 451.2499\n",
            "Epoch 510/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 932060.5625 - mae: 715.1581 - val_loss: 273174.0312 - val_mae: 463.3413\n",
            "Epoch 511/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 905801.3750 - mae: 704.3973 - val_loss: 269632.0312 - val_mae: 459.3333\n",
            "Epoch 512/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 923978.8125 - mae: 713.5447 - val_loss: 272572.7188 - val_mae: 463.3152\n",
            "Epoch 513/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 901360.3125 - mae: 701.6865 - val_loss: 271869.3438 - val_mae: 461.3646\n",
            "Epoch 514/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 933501.3750 - mae: 718.4347 - val_loss: 265550.8125 - val_mae: 455.3778\n",
            "Epoch 515/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 931170.1250 - mae: 711.6196 - val_loss: 271275.5938 - val_mae: 461.3646\n",
            "Epoch 516/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 925510.5000 - mae: 711.9183 - val_loss: 274175.9688 - val_mae: 465.2939\n",
            "Epoch 517/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 910082.5000 - mae: 702.9988 - val_loss: 264482.7812 - val_mae: 453.8130\n",
            "Epoch 518/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 898958.0625 - mae: 701.5895 - val_loss: 270655.3125 - val_mae: 462.9558\n",
            "Epoch 519/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 928812.5000 - mae: 713.2510 - val_loss: 270613.7188 - val_mae: 463.5049\n",
            "Epoch 520/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 900970.1250 - mae: 699.5726 - val_loss: 272644.1250 - val_mae: 463.3958\n",
            "Epoch 521/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 904136.0000 - mae: 702.2982 - val_loss: 266120.4688 - val_mae: 457.4166\n",
            "Epoch 522/1000\n",
            "37/37 [==============================] - 5s 120ms/step - loss: 905924.9375 - mae: 699.3847 - val_loss: 275065.9062 - val_mae: 466.9661\n",
            "Epoch 523/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 918300.0000 - mae: 706.9802 - val_loss: 271986.1562 - val_mae: 464.9222\n",
            "Epoch 524/1000\n",
            "37/37 [==============================] - 5s 121ms/step - loss: 935997.5625 - mae: 719.7289 - val_loss: 271686.3750 - val_mae: 464.9089\n",
            "Epoch 525/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 901832.8750 - mae: 702.7862 - val_loss: 268740.4688 - val_mae: 463.1460\n",
            "Epoch 526/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 866003.5625 - mae: 684.2440 - val_loss: 265237.2812 - val_mae: 459.0521\n",
            "Epoch 527/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 903722.7500 - mae: 700.5400 - val_loss: 279606.4062 - val_mae: 474.1938\n",
            "Epoch 528/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 911018.2500 - mae: 707.7860 - val_loss: 270660.2500 - val_mae: 465.1384\n",
            "Epoch 529/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 886373.5000 - mae: 689.9204 - val_loss: 270105.0000 - val_mae: 463.3958\n",
            "Epoch 530/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 873011.6250 - mae: 687.4341 - val_loss: 267051.1562 - val_mae: 461.3646\n",
            "Epoch 531/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 911638.8125 - mae: 705.0919 - val_loss: 269672.3438 - val_mae: 464.8187\n",
            "Epoch 532/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 890605.2500 - mae: 692.8123 - val_loss: 260717.4375 - val_mae: 455.6107\n",
            "Epoch 533/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 904536.6875 - mae: 699.6190 - val_loss: 263469.4688 - val_mae: 459.3333\n",
            "Epoch 534/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 894033.9375 - mae: 691.6615 - val_loss: 265965.9688 - val_mae: 461.3646\n",
            "Epoch 535/1000\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 878109.0000 - mae: 690.6785 - val_loss: 257317.9531 - val_mae: 453.8992\n",
            "Epoch 536/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 909211.7500 - mae: 704.1507 - val_loss: 276844.9375 - val_mae: 473.8477\n",
            "Epoch 537/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 908657.8750 - mae: 704.8934 - val_loss: 256667.5156 - val_mae: 453.6439\n",
            "Epoch 538/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 898794.6875 - mae: 698.1752 - val_loss: 267686.3750 - val_mae: 464.7281\n",
            "Epoch 539/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 890226.3125 - mae: 693.5906 - val_loss: 267353.2812 - val_mae: 463.3958\n",
            "Epoch 540/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 891368.6875 - mae: 696.0645 - val_loss: 267224.4688 - val_mae: 463.6771\n",
            "Epoch 541/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 896983.1250 - mae: 696.4209 - val_loss: 266844.1562 - val_mae: 464.6891\n",
            "Epoch 542/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 909732.4375 - mae: 704.1448 - val_loss: 260904.1250 - val_mae: 459.0521\n",
            "Epoch 543/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 873144.9375 - mae: 694.1522 - val_loss: 263356.6250 - val_mae: 459.8163\n",
            "Epoch 544/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 880074.5625 - mae: 690.8959 - val_loss: 269038.4375 - val_mae: 468.4980\n",
            "Epoch 545/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 909791.1250 - mae: 703.5966 - val_loss: 262992.1875 - val_mae: 462.6057\n",
            "Epoch 546/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 894758.6250 - mae: 697.6681 - val_loss: 268320.6562 - val_mae: 468.1644\n",
            "Epoch 547/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 900085.3750 - mae: 699.7495 - val_loss: 262589.0000 - val_mae: 461.6458\n",
            "Epoch 548/1000\n",
            "37/37 [==============================] - 5s 121ms/step - loss: 891785.1875 - mae: 696.3362 - val_loss: 262047.0469 - val_mae: 461.0834\n",
            "Epoch 549/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 886142.5625 - mae: 690.9966 - val_loss: 261896.1094 - val_mae: 462.5534\n",
            "Epoch 550/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 899597.7500 - mae: 698.0046 - val_loss: 256108.4844 - val_mae: 455.8450\n",
            "Epoch 551/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 875811.0625 - mae: 686.6318 - val_loss: 269481.9062 - val_mae: 469.7847\n",
            "Epoch 552/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 890751.3125 - mae: 695.6591 - val_loss: 261001.9375 - val_mae: 461.0833\n",
            "Epoch 553/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 904124.5000 - mae: 702.3671 - val_loss: 255368.3281 - val_mae: 455.8838\n",
            "Epoch 554/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 885018.0000 - mae: 691.1993 - val_loss: 260549.3906 - val_mae: 462.4884\n",
            "Epoch 555/1000\n",
            "37/37 [==============================] - 5s 97ms/step - loss: 848330.5625 - mae: 669.7586 - val_loss: 260374.8594 - val_mae: 461.3646\n",
            "Epoch 556/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 900038.3125 - mae: 699.7748 - val_loss: 252077.7031 - val_mae: 454.1723\n",
            "Epoch 557/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 884637.0625 - mae: 690.1642 - val_loss: 259854.9219 - val_mae: 461.3646\n",
            "Epoch 558/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 878867.0000 - mae: 689.4861 - val_loss: 267939.7188 - val_mae: 470.4468\n",
            "Epoch 559/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 887900.9375 - mae: 698.3108 - val_loss: 253630.0469 - val_mae: 455.8781\n",
            "Epoch 560/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 871591.1250 - mae: 691.7499 - val_loss: 264516.5000 - val_mae: 466.7548\n",
            "Epoch 561/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 883171.8750 - mae: 690.1711 - val_loss: 261700.7969 - val_mae: 464.9919\n",
            "Epoch 562/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 858325.1250 - mae: 678.6379 - val_loss: 253194.5000 - val_mae: 456.0002\n",
            "Epoch 563/1000\n",
            "37/37 [==============================] - 6s 133ms/step - loss: 885844.0625 - mae: 690.4906 - val_loss: 255646.2500 - val_mae: 459.3333\n",
            "Epoch 564/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 887392.1875 - mae: 697.6920 - val_loss: 252713.2969 - val_mae: 457.3021\n",
            "Epoch 565/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 899223.9375 - mae: 697.4894 - val_loss: 265567.5000 - val_mae: 469.4212\n",
            "Epoch 566/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 881858.5625 - mae: 691.6659 - val_loss: 254750.5000 - val_mae: 459.0521\n",
            "Epoch 567/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 838892.2500 - mae: 671.5026 - val_loss: 244142.3750 - val_mae: 449.0158\n",
            "Epoch 568/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 878562.3750 - mae: 691.8217 - val_loss: 257085.4844 - val_mae: 461.3646\n",
            "Epoch 569/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 895306.5625 - mae: 699.7598 - val_loss: 262134.2656 - val_mae: 466.6373\n",
            "Epoch 570/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 834332.8750 - mae: 670.7819 - val_loss: 259417.2969 - val_mae: 463.6771\n",
            "Epoch 571/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 879545.4375 - mae: 687.4457 - val_loss: 253891.9531 - val_mae: 458.4296\n",
            "Epoch 572/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 849905.1875 - mae: 674.4599 - val_loss: 261226.5781 - val_mae: 466.3180\n",
            "Epoch 573/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 852757.0000 - mae: 678.6089 - val_loss: 245509.2969 - val_mae: 451.2024\n",
            "Epoch 574/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 851762.6250 - mae: 672.5228 - val_loss: 255637.8906 - val_mae: 461.3646\n",
            "Epoch 575/1000\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 873877.6250 - mae: 689.4769 - val_loss: 255143.7500 - val_mae: 462.2170\n",
            "Epoch 576/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 876692.7500 - mae: 689.9505 - val_loss: 250071.6719 - val_mae: 456.4633\n",
            "Epoch 577/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 855750.5625 - mae: 678.8776 - val_loss: 244647.9219 - val_mae: 451.3062\n",
            "Epoch 578/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 881177.8750 - mae: 690.7332 - val_loss: 254651.4375 - val_mae: 461.3646\n",
            "Epoch 579/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 856311.2500 - mae: 673.4224 - val_loss: 244212.1250 - val_mae: 451.3592\n",
            "Epoch 580/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 876988.4375 - mae: 688.6995 - val_loss: 251107.3594 - val_mae: 458.9688\n",
            "Epoch 581/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 848868.8125 - mae: 674.4969 - val_loss: 253785.0469 - val_mae: 461.0833\n",
            "Epoch 582/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 845018.4375 - mae: 676.7085 - val_loss: 253703.4531 - val_mae: 461.3646\n",
            "Epoch 583/1000\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 883969.1875 - mae: 694.3163 - val_loss: 252892.0000 - val_mae: 460.7188\n",
            "Epoch 584/1000\n",
            "37/37 [==============================] - 5s 121ms/step - loss: 875510.8125 - mae: 692.4631 - val_loss: 255852.8750 - val_mae: 463.3958\n",
            "Epoch 585/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 851742.0625 - mae: 675.1965 - val_loss: 250501.5781 - val_mae: 459.6146\n",
            "Epoch 586/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 878681.1250 - mae: 689.5098 - val_loss: 255349.2969 - val_mae: 464.6665\n",
            "Epoch 587/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 889325.8750 - mae: 697.5812 - val_loss: 252319.3750 - val_mae: 462.3409\n",
            "Epoch 588/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 857799.6875 - mae: 680.8052 - val_loss: 251923.1406 - val_mae: 462.0466\n",
            "Epoch 589/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 876029.7500 - mae: 692.0167 - val_loss: 254612.2500 - val_mae: 464.6270\n",
            "Epoch 590/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 900211.1250 - mae: 704.4247 - val_loss: 249545.1250 - val_mae: 458.6783\n",
            "Epoch 591/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 825122.4375 - mae: 662.8221 - val_loss: 244317.2500 - val_mae: 453.7046\n",
            "Epoch 592/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 839211.1250 - mae: 670.5786 - val_loss: 248571.3125 - val_mae: 459.0521\n",
            "Epoch 593/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 870086.8750 - mae: 689.0345 - val_loss: 251119.1719 - val_mae: 461.3646\n",
            "Epoch 594/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 872976.4375 - mae: 689.7686 - val_loss: 248280.4219 - val_mae: 459.3333\n",
            "Epoch 595/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 869254.3750 - mae: 687.6595 - val_loss: 255468.5000 - val_mae: 466.0177\n",
            "Epoch 596/1000\n",
            "37/37 [==============================] - 6s 147ms/step - loss: 842921.6875 - mae: 669.5720 - val_loss: 252788.5781 - val_mae: 464.2551\n",
            "Epoch 597/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 873346.4375 - mae: 689.2664 - val_loss: 249797.3750 - val_mae: 461.9301\n",
            "Epoch 598/1000\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 863836.4375 - mae: 682.4185 - val_loss: 254515.5469 - val_mae: 465.8961\n",
            "Epoch 599/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 844181.0625 - mae: 674.8151 - val_loss: 242260.4219 - val_mae: 454.4503\n",
            "Epoch 600/1000\n",
            "37/37 [==============================] - 5s 100ms/step - loss: 859546.8125 - mae: 681.0994 - val_loss: 239908.0781 - val_mae: 451.9061\n",
            "Epoch 601/1000\n",
            "37/37 [==============================] - 10s 246ms/step - loss: 855285.8750 - mae: 680.9299 - val_loss: 249161.7031 - val_mae: 461.0833\n",
            "Epoch 602/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 835140.8750 - mae: 669.7676 - val_loss: 246517.0000 - val_mae: 459.3334\n",
            "Epoch 603/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 856307.1250 - mae: 677.7921 - val_loss: 248248.0000 - val_mae: 461.5707\n",
            "Epoch 604/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 830868.0625 - mae: 667.2231 - val_loss: 243498.3281 - val_mae: 457.3021\n",
            "Epoch 605/1000\n",
            "37/37 [==============================] - 6s 138ms/step - loss: 860067.4375 - mae: 681.2779 - val_loss: 248114.0000 - val_mae: 462.1073\n",
            "Epoch 606/1000\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 824198.6875 - mae: 667.4031 - val_loss: 250249.3906 - val_mae: 462.7500\n",
            "Epoch 607/1000\n",
            "37/37 [==============================] - 10s 237ms/step - loss: 833346.6250 - mae: 670.8842 - val_loss: 247666.8281 - val_mae: 462.0816\n",
            "Epoch 608/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 853805.3125 - mae: 677.4191 - val_loss: 249854.5781 - val_mae: 463.8185\n",
            "Epoch 609/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 843059.6875 - mae: 674.4774 - val_loss: 252360.2031 - val_mae: 466.1178\n",
            "Epoch 610/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 834867.6250 - mae: 668.6318 - val_loss: 249398.9219 - val_mae: 463.7924\n",
            "Epoch 611/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 817114.6250 - mae: 664.8011 - val_loss: 249178.5000 - val_mae: 463.7797\n",
            "Epoch 612/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 848657.0000 - mae: 674.2106 - val_loss: 251131.2500 - val_mae: 466.4502\n",
            "Epoch 613/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 866253.6875 - mae: 688.6001 - val_loss: 253456.0156 - val_mae: 468.4549\n",
            "Epoch 614/1000\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 836989.0000 - mae: 671.5180 - val_loss: 246922.7969 - val_mae: 460.7384\n",
            "Epoch 615/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 842227.1875 - mae: 673.0212 - val_loss: 250424.2969 - val_mae: 466.3716\n",
            "Epoch 616/1000\n",
            "37/37 [==============================] - 6s 143ms/step - loss: 826227.9375 - mae: 667.4973 - val_loss: 245512.6094 - val_mae: 461.6839\n",
            "Epoch 617/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 842555.3750 - mae: 672.3109 - val_loss: 238258.8906 - val_mae: 454.3186\n",
            "Epoch 618/1000\n",
            "37/37 [==============================] - 5s 121ms/step - loss: 825463.0625 - mae: 661.8683 - val_loss: 245684.8125 - val_mae: 461.3646\n",
            "Epoch 619/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 843508.7500 - mae: 673.4523 - val_loss: 240384.5781 - val_mae: 457.3021\n",
            "Epoch 620/1000\n",
            "37/37 [==============================] - 10s 256ms/step - loss: 843439.4375 - mae: 672.6237 - val_loss: 244993.2500 - val_mae: 462.1955\n",
            "Epoch 621/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 847050.9375 - mae: 676.6124 - val_loss: 245244.2031 - val_mae: 461.6458\n",
            "Epoch 622/1000\n",
            "37/37 [==============================] - 10s 258ms/step - loss: 840993.2500 - mae: 670.0259 - val_loss: 235409.9219 - val_mae: 452.1089\n",
            "Epoch 623/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 806449.0000 - mae: 659.7835 - val_loss: 247205.0000 - val_mae: 463.3958\n",
            "Epoch 624/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 840806.4375 - mae: 670.0787 - val_loss: 246352.2969 - val_mae: 463.6129\n",
            "Epoch 625/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 810657.3750 - mae: 653.7606 - val_loss: 241909.3594 - val_mae: 459.6146\n",
            "Epoch 626/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 851672.3125 - mae: 679.8376 - val_loss: 243572.0469 - val_mae: 461.8377\n",
            "Epoch 627/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 817512.5625 - mae: 659.7274 - val_loss: 245892.6406 - val_mae: 463.8560\n",
            "Epoch 628/1000\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 837246.8125 - mae: 672.3209 - val_loss: 245503.0000 - val_mae: 463.5612\n",
            "Epoch 629/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 827968.1250 - mae: 669.2557 - val_loss: 247989.0156 - val_mae: 465.8611\n",
            "Epoch 630/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 847218.6250 - mae: 674.4741 - val_loss: 238223.0469 - val_mae: 457.3021\n",
            "Epoch 631/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 814008.0625 - mae: 657.3925 - val_loss: 245594.7969 - val_mae: 463.3958\n",
            "Epoch 632/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 833297.9375 - mae: 669.1352 - val_loss: 239120.3281 - val_mae: 456.7908\n",
            "Epoch 633/1000\n",
            "37/37 [==============================] - 6s 141ms/step - loss: 849385.6250 - mae: 680.1133 - val_loss: 244979.5469 - val_mae: 463.3125\n",
            "Epoch 634/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 825023.6875 - mae: 663.4759 - val_loss: 239808.6094 - val_mae: 459.0521\n",
            "Epoch 635/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 797573.1250 - mae: 657.4704 - val_loss: 241385.8906 - val_mae: 461.1596\n",
            "Epoch 636/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 829504.1875 - mae: 668.3007 - val_loss: 246372.2500 - val_mae: 465.4907\n",
            "Epoch 637/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 830328.4375 - mae: 668.7336 - val_loss: 241498.4375 - val_mae: 461.9777\n",
            "Epoch 638/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 835615.1250 - mae: 670.1490 - val_loss: 245708.2500 - val_mae: 466.3465\n",
            "Epoch 639/1000\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 836504.8750 - mae: 672.8942 - val_loss: 243252.1875 - val_mae: 463.4206\n",
            "Epoch 640/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 820047.8125 - mae: 669.0607 - val_loss: 239453.2344 - val_mae: 459.0403\n",
            "Epoch 641/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 814563.3750 - mae: 666.0726 - val_loss: 240526.9375 - val_mae: 461.6447\n",
            "Epoch 642/1000\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 808203.1250 - mae: 660.6902 - val_loss: 234920.2031 - val_mae: 455.0163\n",
            "Epoch 643/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 805405.2500 - mae: 659.5536 - val_loss: 242628.8594 - val_mae: 463.6505\n",
            "Epoch 644/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 808923.3125 - mae: 660.0021 - val_loss: 240769.0625 - val_mae: 461.6458\n",
            "Epoch 645/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 828260.8750 - mae: 673.3747 - val_loss: 243732.3125 - val_mae: 465.3245\n",
            "Epoch 646/1000\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 843689.6875 - mae: 675.7217 - val_loss: 239389.7969 - val_mae: 461.3000\n",
            "Epoch 647/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 788066.5000 - mae: 648.8304 - val_loss: 239481.4844 - val_mae: 460.7188\n",
            "Epoch 648/1000\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 846322.5625 - mae: 678.7454 - val_loss: 237554.7500 - val_mae: 459.6146\n",
            "Epoch 649/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 802670.8750 - mae: 661.9225 - val_loss: 242144.4219 - val_mae: 463.3958\n",
            "Epoch 650/1000\n",
            "37/37 [==============================] - 10s 240ms/step - loss: 810529.1250 - mae: 662.4777 - val_loss: 237189.3281 - val_mae: 459.6146\n",
            "Epoch 651/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 801699.7500 - mae: 654.5370 - val_loss: 236653.2031 - val_mae: 459.0521\n",
            "Epoch 652/1000\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 829441.1250 - mae: 669.0968 - val_loss: 236830.6875 - val_mae: 459.6146\n",
            "Epoch 653/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 815954.3125 - mae: 660.1827 - val_loss: 242798.9531 - val_mae: 464.9910\n",
            "Epoch 654/1000\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 831339.9375 - mae: 672.4908 - val_loss: 237194.7969 - val_mae: 459.5013\n",
            "Epoch 655/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 818914.1250 - mae: 663.4692 - val_loss: 242034.4531 - val_mae: 465.6280\n",
            "Epoch 656/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 805064.3750 - mae: 662.2454 - val_loss: 238394.6250 - val_mae: 461.3646\n",
            "Epoch 657/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 820198.6875 - mae: 669.3914 - val_loss: 231598.1406 - val_mae: 455.1967\n",
            "Epoch 658/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 825834.1250 - mae: 669.3829 - val_loss: 233894.2656 - val_mae: 457.2403\n",
            "Epoch 659/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 788442.3750 - mae: 650.8986 - val_loss: 240326.1250 - val_mae: 463.3958\n",
            "Epoch 660/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 795624.0625 - mae: 652.6635 - val_loss: 239373.4375 - val_mae: 463.4322\n",
            "Epoch 661/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 783858.8125 - mae: 646.7338 - val_loss: 236577.3906 - val_mae: 461.1078\n",
            "Epoch 662/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 810882.8125 - mae: 658.6569 - val_loss: 235706.6094 - val_mae: 459.3216\n",
            "Epoch 663/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 800215.0000 - mae: 653.9301 - val_loss: 236221.5625 - val_mae: 461.0826\n",
            "Epoch 664/1000\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 805847.8125 - mae: 657.4372 - val_loss: 232145.9219 - val_mae: 457.3021\n",
            "Epoch 665/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 796120.7500 - mae: 654.2247 - val_loss: 231798.8906 - val_mae: 457.0208\n",
            "Epoch 666/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 803477.1875 - mae: 661.1782 - val_loss: 230384.2656 - val_mae: 455.5913\n",
            "Epoch 667/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 803078.7500 - mae: 660.6682 - val_loss: 237934.8125 - val_mae: 463.0625\n",
            "Epoch 668/1000\n",
            "37/37 [==============================] - 6s 138ms/step - loss: 788214.9375 - mae: 648.4420 - val_loss: 233922.7344 - val_mae: 459.3333\n",
            "Epoch 669/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 797373.9375 - mae: 653.1757 - val_loss: 235622.6250 - val_mae: 460.7188\n",
            "Epoch 670/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 809972.0000 - mae: 659.2717 - val_loss: 239826.6406 - val_mae: 465.0558\n",
            "Epoch 671/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 811449.1875 - mae: 661.2819 - val_loss: 233431.4531 - val_mae: 459.3333\n",
            "Epoch 672/1000\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 804974.4375 - mae: 657.7314 - val_loss: 235007.2656 - val_mae: 461.5304\n",
            "Epoch 673/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 795299.5000 - mae: 652.0439 - val_loss: 239296.8750 - val_mae: 465.0177\n",
            "Epoch 674/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 786680.2500 - mae: 652.9748 - val_loss: 234478.8594 - val_mae: 461.2236\n",
            "Epoch 675/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 790397.2500 - mae: 653.4724 - val_loss: 235194.3125 - val_mae: 461.3646\n",
            "Epoch 676/1000\n",
            "37/37 [==============================] - 9s 245ms/step - loss: 777255.9375 - mae: 644.8105 - val_loss: 232442.1406 - val_mae: 459.0521\n",
            "Epoch 677/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 802323.1875 - mae: 654.1684 - val_loss: 237282.2656 - val_mae: 463.3958\n",
            "Epoch 678/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 815132.1875 - mae: 663.3107 - val_loss: 236019.2500 - val_mae: 462.9227\n",
            "Epoch 679/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 793498.1250 - mae: 652.1138 - val_loss: 238616.4531 - val_mae: 465.5033\n",
            "Epoch 680/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 815673.9375 - mae: 664.6071 - val_loss: 239358.7031 - val_mae: 466.4608\n",
            "Epoch 681/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 798797.5625 - mae: 656.3271 - val_loss: 230380.6406 - val_mae: 457.5323\n",
            "Epoch 682/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 795148.4375 - mae: 655.0341 - val_loss: 236790.6875 - val_mae: 464.6598\n",
            "Epoch 683/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 786567.4375 - mae: 646.4316 - val_loss: 236613.2656 - val_mae: 464.6343\n",
            "Epoch 684/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 797699.8125 - mae: 654.9531 - val_loss: 239198.3906 - val_mae: 467.2023\n",
            "Epoch 685/1000\n",
            "37/37 [==============================] - 5s 102ms/step - loss: 788302.3125 - mae: 651.1054 - val_loss: 232649.7500 - val_mae: 461.0836\n",
            "Epoch 686/1000\n",
            "37/37 [==============================] - 10s 241ms/step - loss: 786237.4375 - mae: 652.8299 - val_loss: 237243.9531 - val_mae: 465.1328\n",
            "Epoch 687/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 804553.1250 - mae: 659.2029 - val_loss: 229520.8281 - val_mae: 457.6092\n",
            "Epoch 688/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 781046.9375 - mae: 647.8862 - val_loss: 221053.9531 - val_mae: 450.0974\n",
            "Epoch 689/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 798315.5000 - mae: 656.3391 - val_loss: 234380.9375 - val_mae: 463.0640\n",
            "Epoch 690/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 811553.0625 - mae: 665.3103 - val_loss: 230476.5469 - val_mae: 459.3333\n",
            "Epoch 691/1000\n",
            "37/37 [==============================] - 5s 117ms/step - loss: 781312.1875 - mae: 648.7105 - val_loss: 234054.0469 - val_mae: 463.0381\n",
            "Epoch 692/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 798091.9375 - mae: 659.0912 - val_loss: 227672.0000 - val_mae: 456.2940\n",
            "Epoch 693/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 788760.1875 - mae: 652.8228 - val_loss: 230026.9375 - val_mae: 459.3333\n",
            "Epoch 694/1000\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 795287.8125 - mae: 652.1678 - val_loss: 231203.3594 - val_mae: 460.9678\n",
            "Epoch 695/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 783813.3125 - mae: 647.7626 - val_loss: 236892.1875 - val_mae: 466.3582\n",
            "Epoch 696/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 786394.9375 - mae: 651.2236 - val_loss: 230510.0781 - val_mae: 460.3799\n",
            "Epoch 697/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 786318.7500 - mae: 651.2820 - val_loss: 232900.2031 - val_mae: 462.6794\n",
            "Epoch 698/1000\n",
            "37/37 [==============================] - 4s 114ms/step - loss: 792253.4375 - mae: 654.1031 - val_loss: 236766.7344 - val_mae: 466.8438\n",
            "Epoch 699/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 785807.4375 - mae: 657.2268 - val_loss: 233667.7500 - val_mae: 463.9430\n",
            "Epoch 700/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 788422.6250 - mae: 653.2908 - val_loss: 230999.6719 - val_mae: 461.0000\n",
            "Epoch 701/1000\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 766366.8125 - mae: 642.1157 - val_loss: 232279.2500 - val_mae: 462.6283\n",
            "Epoch 702/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 796537.7500 - mae: 662.3397 - val_loss: 233366.4531 - val_mae: 464.1473\n",
            "Epoch 703/1000\n",
            "37/37 [==============================] - 6s 133ms/step - loss: 769552.2500 - mae: 645.9528 - val_loss: 234504.6875 - val_mae: 464.9149\n",
            "Epoch 704/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 794339.9375 - mae: 656.8878 - val_loss: 224727.8125 - val_mae: 455.7126\n",
            "Epoch 705/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 788721.1875 - mae: 654.8716 - val_loss: 223754.5625 - val_mae: 454.5963\n",
            "Epoch 706/1000\n",
            "37/37 [==============================] - 6s 133ms/step - loss: 798332.7500 - mae: 659.4864 - val_loss: 225844.6719 - val_mae: 457.3021\n",
            "Epoch 707/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 792869.8750 - mae: 655.8515 - val_loss: 229180.2031 - val_mae: 459.8969\n",
            "Epoch 708/1000\n",
            "37/37 [==============================] - 6s 147ms/step - loss: 759980.9375 - mae: 642.1529 - val_loss: 223414.5000 - val_mae: 454.6734\n",
            "Epoch 709/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 754221.8750 - mae: 636.5586 - val_loss: 226803.0625 - val_mae: 458.1718\n",
            "Epoch 710/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 775536.5000 - mae: 645.5472 - val_loss: 234409.3750 - val_mae: 465.9745\n",
            "Epoch 711/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 733335.7500 - mae: 625.2142 - val_loss: 230985.5781 - val_mae: 462.7824\n",
            "Epoch 712/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 774989.5625 - mae: 646.3161 - val_loss: 234099.0000 - val_mae: 465.9247\n",
            "Epoch 713/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 776302.3750 - mae: 649.3990 - val_loss: 228579.1250 - val_mae: 461.0069\n",
            "Epoch 714/1000\n",
            "37/37 [==============================] - 10s 243ms/step - loss: 774110.4375 - mae: 646.2523 - val_loss: 226202.7969 - val_mae: 458.2347\n",
            "Epoch 715/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 759307.9375 - mae: 640.2307 - val_loss: 230418.4531 - val_mae: 462.7322\n",
            "Epoch 716/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 784720.5000 - mae: 653.9354 - val_loss: 226661.6719 - val_mae: 459.0521\n",
            "Epoch 717/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 781964.9375 - mae: 648.4933 - val_loss: 226534.7500 - val_mae: 459.0521\n",
            "Epoch 718/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 781864.8125 - mae: 649.3474 - val_loss: 226602.5000 - val_mae: 459.3333\n",
            "Epoch 719/1000\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 756454.0625 - mae: 637.4374 - val_loss: 224546.3594 - val_mae: 456.9824\n",
            "Epoch 720/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 788347.8125 - mae: 656.4559 - val_loss: 229502.0469 - val_mae: 462.3861\n",
            "Epoch 721/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 766826.0625 - mae: 642.4429 - val_loss: 229360.4531 - val_mae: 462.3730\n",
            "Epoch 722/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 771097.1875 - mae: 642.2043 - val_loss: 227135.7031 - val_mae: 460.6107\n",
            "Epoch 723/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 774571.3750 - mae: 645.7936 - val_loss: 227442.6719 - val_mae: 460.3811\n",
            "Epoch 724/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 773080.0000 - mae: 646.3934 - val_loss: 222751.8281 - val_mae: 456.3314\n",
            "Epoch 725/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 784407.3750 - mae: 656.0048 - val_loss: 228012.2656 - val_mae: 461.3646\n",
            "Epoch 726/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 768180.1875 - mae: 644.4240 - val_loss: 219256.0469 - val_mae: 453.0994\n",
            "Epoch 727/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 756606.8750 - mae: 641.6711 - val_loss: 226783.7656 - val_mae: 460.1508\n",
            "Epoch 728/1000\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 749538.3750 - mae: 633.4347 - val_loss: 227649.6875 - val_mae: 461.3646\n",
            "Epoch 729/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 744711.2500 - mae: 628.4599 - val_loss: 228299.8281 - val_mae: 462.2732\n",
            "Epoch 730/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 776523.6875 - mae: 649.8039 - val_loss: 231393.2344 - val_mae: 465.4684\n",
            "Epoch 731/1000\n",
            "37/37 [==============================] - 10s 253ms/step - loss: 762937.2500 - mae: 642.6914 - val_loss: 227501.0156 - val_mae: 461.6458\n",
            "Epoch 732/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 772086.1875 - mae: 648.3970 - val_loss: 221704.8750 - val_mae: 456.1507\n",
            "Epoch 733/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 766488.9375 - mae: 647.4391 - val_loss: 227977.4219 - val_mae: 462.5031\n",
            "Epoch 734/1000\n",
            "37/37 [==============================] - 5s 102ms/step - loss: 781814.8125 - mae: 657.0013 - val_loss: 227638.3906 - val_mae: 462.2086\n",
            "Epoch 735/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 761013.2500 - mae: 639.6077 - val_loss: 224556.2031 - val_mae: 459.3333\n",
            "Epoch 736/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 779360.4375 - mae: 651.3499 - val_loss: 228274.8750 - val_mae: 463.2829\n",
            "Epoch 737/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 765112.1875 - mae: 641.7709 - val_loss: 225025.0156 - val_mae: 459.4716\n",
            "Epoch 738/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 767539.0625 - mae: 645.1579 - val_loss: 227549.3281 - val_mae: 462.7207\n",
            "Epoch 739/1000\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 748900.5625 - mae: 639.2087 - val_loss: 226561.1250 - val_mae: 461.6458\n",
            "Epoch 740/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 749678.0000 - mae: 637.8829 - val_loss: 226448.7500 - val_mae: 461.6458\n",
            "Epoch 741/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 773386.3125 - mae: 647.3829 - val_loss: 230894.1094 - val_mae: 466.5049\n",
            "Epoch 742/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 782389.0625 - mae: 658.1107 - val_loss: 226841.2656 - val_mae: 462.3878\n",
            "Epoch 743/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 771641.1875 - mae: 649.5627 - val_loss: 223449.7500 - val_mae: 459.0521\n",
            "Epoch 744/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 747917.4375 - mae: 634.6348 - val_loss: 230460.4531 - val_mae: 466.3906\n",
            "Epoch 745/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 757175.1875 - mae: 644.5241 - val_loss: 226273.4844 - val_mae: 462.0688\n",
            "Epoch 746/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 739416.3125 - mae: 632.4419 - val_loss: 223134.6875 - val_mae: 459.0521\n",
            "Epoch 747/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 757069.1250 - mae: 641.3875 - val_loss: 225693.7969 - val_mae: 461.6790\n",
            "Epoch 748/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 757876.9375 - mae: 644.0022 - val_loss: 220904.7500 - val_mae: 457.3021\n",
            "Epoch 749/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 756697.2500 - mae: 644.6347 - val_loss: 227268.4531 - val_mae: 463.1146\n",
            "Epoch 750/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 762464.2500 - mae: 646.7640 - val_loss: 225356.3750 - val_mae: 461.6458\n",
            "Epoch 751/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 763804.9375 - mae: 645.3697 - val_loss: 223435.5156 - val_mae: 459.4630\n",
            "Epoch 752/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 753964.7500 - mae: 647.2991 - val_loss: 225661.6719 - val_mae: 462.2610\n",
            "Epoch 753/1000\n",
            "37/37 [==============================] - 5s 100ms/step - loss: 766634.3125 - mae: 646.5219 - val_loss: 228471.8281 - val_mae: 465.1616\n",
            "Epoch 754/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 722177.0000 - mae: 626.0150 - val_loss: 224513.0781 - val_mae: 461.0833\n",
            "Epoch 755/1000\n",
            "37/37 [==============================] - 5s 121ms/step - loss: 726683.8125 - mae: 629.0833 - val_loss: 228014.3750 - val_mae: 464.8308\n",
            "Epoch 756/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 745037.0625 - mae: 636.0253 - val_loss: 228103.7500 - val_mae: 465.0869\n",
            "Epoch 757/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 720031.3125 - mae: 624.9487 - val_loss: 222434.7344 - val_mae: 459.6146\n",
            "Epoch 758/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 752738.8125 - mae: 639.9197 - val_loss: 222128.0000 - val_mae: 459.3333\n",
            "Epoch 759/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 752222.9375 - mae: 641.9401 - val_loss: 227316.2344 - val_mae: 464.4502\n",
            "Epoch 760/1000\n",
            "37/37 [==============================] - 10s 235ms/step - loss: 753049.8750 - mae: 642.2052 - val_loss: 227840.5781 - val_mae: 465.2694\n",
            "Epoch 761/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 744126.2500 - mae: 636.7430 - val_loss: 221841.9219 - val_mae: 459.3333\n",
            "Epoch 762/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 732193.5000 - mae: 630.5110 - val_loss: 226638.9375 - val_mae: 464.0832\n",
            "Epoch 763/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 742516.5000 - mae: 639.4597 - val_loss: 224249.4375 - val_mae: 461.8420\n",
            "Epoch 764/1000\n",
            "37/37 [==============================] - 6s 138ms/step - loss: 747223.3125 - mae: 646.1861 - val_loss: 224356.0625 - val_mae: 462.1103\n",
            "Epoch 765/1000\n",
            "37/37 [==============================] - 10s 252ms/step - loss: 751745.8125 - mae: 643.9283 - val_loss: 223818.5469 - val_mae: 461.5350\n",
            "Epoch 766/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 738110.1875 - mae: 637.3063 - val_loss: 221378.4844 - val_mae: 459.3333\n",
            "Epoch 767/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 727281.3750 - mae: 628.3388 - val_loss: 223258.9531 - val_mae: 461.0833\n",
            "Epoch 768/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 722285.2500 - mae: 630.6053 - val_loss: 218114.2344 - val_mae: 456.1913\n",
            "Epoch 769/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 754121.3125 - mae: 646.4290 - val_loss: 221451.2500 - val_mae: 459.7352\n",
            "Epoch 770/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 777270.4375 - mae: 659.1929 - val_loss: 221024.0156 - val_mae: 459.3333\n",
            "Epoch 771/1000\n",
            "37/37 [==============================] - 6s 135ms/step - loss: 748041.7500 - mae: 641.4250 - val_loss: 222789.2656 - val_mae: 460.9889\n",
            "Epoch 772/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 729868.7500 - mae: 629.6310 - val_loss: 223016.0625 - val_mae: 461.3645\n",
            "Epoch 773/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 739449.8750 - mae: 635.5506 - val_loss: 220468.3906 - val_mae: 458.9825\n",
            "Epoch 774/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 739195.5625 - mae: 639.1224 - val_loss: 220171.6406 - val_mae: 458.7144\n",
            "Epoch 775/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 729421.3750 - mae: 631.5803 - val_loss: 225459.8750 - val_mae: 464.0466\n",
            "Epoch 776/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 745054.5625 - mae: 640.5024 - val_loss: 222706.1719 - val_mae: 461.3958\n",
            "Epoch 777/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 724885.1250 - mae: 635.9324 - val_loss: 221858.9531 - val_mae: 460.4827\n",
            "Epoch 778/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 757398.3750 - mae: 651.3608 - val_loss: 217964.0469 - val_mae: 457.0208\n",
            "Epoch 779/1000\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 718543.5625 - mae: 632.8743 - val_loss: 224791.6094 - val_mae: 463.6704\n",
            "Epoch 780/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 715160.6875 - mae: 628.0154 - val_loss: 221562.2500 - val_mae: 460.4562\n",
            "Epoch 781/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 738341.4375 - mae: 638.4254 - val_loss: 220101.7500 - val_mae: 459.3333\n",
            "Epoch 782/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 712333.4375 - mae: 630.6360 - val_loss: 219800.5000 - val_mae: 459.0521\n",
            "Epoch 783/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 714593.8750 - mae: 626.5068 - val_loss: 222909.3281 - val_mae: 462.3773\n",
            "Epoch 784/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 725004.4375 - mae: 633.9220 - val_loss: 224504.3281 - val_mae: 463.8211\n",
            "Epoch 785/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 730815.8125 - mae: 638.8582 - val_loss: 217266.2344 - val_mae: 456.8203\n",
            "Epoch 786/1000\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 728904.0625 - mae: 636.1262 - val_loss: 219874.4375 - val_mae: 459.5211\n",
            "Epoch 787/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 734837.4375 - mae: 638.8330 - val_loss: 222291.0156 - val_mae: 461.9962\n",
            "Epoch 788/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 730633.4375 - mae: 634.3737 - val_loss: 223739.5469 - val_mae: 463.2772\n",
            "Epoch 789/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 722447.3750 - mae: 634.8477 - val_loss: 221846.2656 - val_mae: 461.6458\n",
            "Epoch 790/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 730028.3750 - mae: 636.0174 - val_loss: 217519.9375 - val_mae: 457.5833\n",
            "Epoch 791/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 731581.6875 - mae: 637.1211 - val_loss: 221571.9531 - val_mae: 461.4892\n",
            "Epoch 792/1000\n",
            "37/37 [==============================] - 10s 240ms/step - loss: 716962.8750 - mae: 630.4670 - val_loss: 217062.7656 - val_mae: 457.1893\n",
            "Epoch 793/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 738141.6250 - mae: 640.9354 - val_loss: 221227.7031 - val_mae: 461.2812\n",
            "Epoch 794/1000\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 731107.5000 - mae: 637.2145 - val_loss: 219134.8750 - val_mae: 459.3333\n",
            "Epoch 795/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 732629.5625 - mae: 637.0926 - val_loss: 223579.6250 - val_mae: 463.7519\n",
            "Epoch 796/1000\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 722945.6875 - mae: 630.4066 - val_loss: 220930.0469 - val_mae: 461.1452\n",
            "Epoch 797/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 713960.3750 - mae: 629.6969 - val_loss: 221530.2344 - val_mae: 461.9767\n",
            "Epoch 798/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 736726.1875 - mae: 643.0702 - val_loss: 216754.4375 - val_mae: 457.3021\n",
            "Epoch 799/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 720266.4375 - mae: 635.4950 - val_loss: 221146.2500 - val_mae: 461.6706\n",
            "Epoch 800/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 725057.1875 - mae: 637.4984 - val_loss: 220590.7344 - val_mae: 461.0713\n",
            "Epoch 801/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 710596.7500 - mae: 625.0692 - val_loss: 218118.0156 - val_mae: 458.6877\n",
            "Epoch 802/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 740457.0625 - mae: 647.0031 - val_loss: 218598.0781 - val_mae: 459.3333\n",
            "Epoch 803/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 730536.6875 - mae: 640.7957 - val_loss: 218326.5000 - val_mae: 459.0779\n",
            "Epoch 804/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 720207.9375 - mae: 631.6185 - val_loss: 218702.0469 - val_mae: 459.6146\n",
            "Epoch 805/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 726964.8125 - mae: 636.8696 - val_loss: 222541.2344 - val_mae: 463.3447\n",
            "Epoch 806/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 712597.1250 - mae: 629.9005 - val_loss: 222411.4531 - val_mae: 463.2687\n",
            "Epoch 807/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 695316.5625 - mae: 621.6551 - val_loss: 216267.1406 - val_mae: 457.3778\n",
            "Epoch 808/1000\n",
            "37/37 [==============================] - 6s 145ms/step - loss: 694514.4375 - mae: 624.5175 - val_loss: 218228.0000 - val_mae: 459.3333\n",
            "Epoch 809/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 711812.3750 - mae: 629.9363 - val_loss: 218399.4219 - val_mae: 459.6146\n",
            "Epoch 810/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 718165.1250 - mae: 635.2123 - val_loss: 220326.6250 - val_mae: 461.5334\n",
            "Epoch 811/1000\n",
            "37/37 [==============================] - 10s 250ms/step - loss: 712565.1250 - mae: 629.3388 - val_loss: 215959.4219 - val_mae: 457.2710\n",
            "Epoch 812/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 737953.6250 - mae: 644.1974 - val_loss: 220298.2344 - val_mae: 461.6458\n",
            "Epoch 813/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 713963.3125 - mae: 633.6199 - val_loss: 219880.4219 - val_mae: 461.2137\n",
            "Epoch 814/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 704192.7500 - mae: 623.2139 - val_loss: 221976.4219 - val_mae: 463.3510\n",
            "Epoch 815/1000\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 714163.1875 - mae: 630.5663 - val_loss: 220212.8281 - val_mae: 461.7517\n",
            "Epoch 816/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 711271.6250 - mae: 633.9968 - val_loss: 217853.3750 - val_mae: 459.4271\n",
            "Epoch 817/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 730777.1875 - mae: 641.6544 - val_loss: 217555.5469 - val_mae: 459.1332\n",
            "Epoch 818/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 729857.1250 - mae: 641.2096 - val_loss: 219414.2344 - val_mae: 461.0000\n",
            "Epoch 819/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 726801.9375 - mae: 638.8317 - val_loss: 217425.3594 - val_mae: 459.1073\n",
            "Epoch 820/1000\n",
            "37/37 [==============================] - 6s 135ms/step - loss: 713732.1875 - mae: 629.7662 - val_loss: 221468.4219 - val_mae: 463.1575\n",
            "Epoch 821/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 703791.8750 - mae: 628.1360 - val_loss: 219352.7344 - val_mae: 461.1136\n",
            "Epoch 822/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 710779.6250 - mae: 635.9350 - val_loss: 215092.9375 - val_mae: 456.9199\n",
            "Epoch 823/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 697594.5625 - mae: 626.7630 - val_loss: 217645.8906 - val_mae: 459.6094\n",
            "Epoch 824/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 707354.9375 - mae: 629.6373 - val_loss: 221202.1250 - val_mae: 463.0999\n",
            "Epoch 825/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 707814.1250 - mae: 630.2253 - val_loss: 217555.1875 - val_mae: 459.6146\n",
            "Epoch 826/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 719196.1875 - mae: 639.4686 - val_loss: 217008.5000 - val_mae: 459.0197\n",
            "Epoch 827/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 708873.6250 - mae: 632.9690 - val_loss: 217262.0625 - val_mae: 459.3780\n",
            "Epoch 828/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 706762.8750 - mae: 634.3657 - val_loss: 217417.6875 - val_mae: 459.6146\n",
            "Epoch 829/1000\n",
            "37/37 [==============================] - 10s 248ms/step - loss: 706403.9375 - mae: 633.8411 - val_loss: 220958.2344 - val_mae: 463.1146\n",
            "Epoch 830/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 704913.8750 - mae: 632.7393 - val_loss: 215370.7344 - val_mae: 457.6647\n",
            "Epoch 831/1000\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 715156.4375 - mae: 637.6536 - val_loss: 218921.8594 - val_mae: 461.1760\n",
            "Epoch 832/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 706028.1250 - mae: 629.7354 - val_loss: 219030.8281 - val_mae: 461.3646\n",
            "Epoch 833/1000\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 692697.0000 - mae: 626.8839 - val_loss: 220671.0469 - val_mae: 462.9956\n",
            "Epoch 834/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 694595.1875 - mae: 623.7416 - val_loss: 218598.1250 - val_mae: 460.9521\n",
            "Epoch 835/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 718606.3750 - mae: 639.2397 - val_loss: 218543.5625 - val_mae: 460.9394\n",
            "Epoch 836/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 711161.4375 - mae: 634.9106 - val_loss: 218123.5625 - val_mae: 460.4889\n",
            "Epoch 837/1000\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 704472.2500 - mae: 631.0740 - val_loss: 220690.7031 - val_mae: 463.2271\n",
            "Epoch 838/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 701040.1250 - mae: 631.0474 - val_loss: 219165.5625 - val_mae: 461.8263\n",
            "Epoch 839/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 697369.4375 - mae: 624.9893 - val_loss: 216732.9844 - val_mae: 459.3333\n",
            "Epoch 840/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 705755.8125 - mae: 631.5019 - val_loss: 220538.7031 - val_mae: 463.1910\n",
            "Epoch 841/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 707423.6875 - mae: 635.8491 - val_loss: 220661.7969 - val_mae: 463.3958\n",
            "Epoch 842/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 689213.0625 - mae: 623.3321 - val_loss: 218436.7344 - val_mae: 461.1343\n",
            "Epoch 843/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 699139.2500 - mae: 627.5167 - val_loss: 220581.1719 - val_mae: 463.3958\n",
            "Epoch 844/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 693034.1875 - mae: 627.8030 - val_loss: 215864.5625 - val_mae: 458.5158\n",
            "Epoch 845/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 700748.0000 - mae: 631.1049 - val_loss: 218298.1719 - val_mae: 461.0972\n",
            "Epoch 846/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 696138.0000 - mae: 629.2812 - val_loss: 216509.0469 - val_mae: 459.3352\n",
            "Epoch 847/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 709424.6875 - mae: 635.5858 - val_loss: 219707.3906 - val_mae: 462.5409\n",
            "Epoch 848/1000\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 706192.8125 - mae: 634.2034 - val_loss: 219902.4531 - val_mae: 462.7865\n",
            "Epoch 849/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 698745.0625 - mae: 629.0177 - val_loss: 218378.7969 - val_mae: 461.3646\n",
            "Epoch 850/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 702774.5625 - mae: 635.8237 - val_loss: 216123.3594 - val_mae: 459.0521\n",
            "Epoch 851/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 699823.4375 - mae: 632.2465 - val_loss: 216861.1094 - val_mae: 459.9571\n",
            "Epoch 852/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 692309.1250 - mae: 626.2881 - val_loss: 217506.0781 - val_mae: 460.4471\n",
            "Epoch 853/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 689371.3125 - mae: 626.7232 - val_loss: 217957.2031 - val_mae: 460.9974\n",
            "Epoch 854/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 671307.3125 - mae: 619.5754 - val_loss: 219888.2031 - val_mae: 463.0164\n",
            "Epoch 855/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 675322.8125 - mae: 624.4564 - val_loss: 219038.1250 - val_mae: 462.0503\n",
            "Epoch 856/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 682094.2500 - mae: 620.6619 - val_loss: 219235.9219 - val_mae: 462.3078\n",
            "Epoch 857/1000\n",
            "37/37 [==============================] - 6s 139ms/step - loss: 678590.4375 - mae: 623.2543 - val_loss: 218144.6094 - val_mae: 461.3646\n",
            "Epoch 858/1000\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 686424.2500 - mae: 627.3126 - val_loss: 215809.6719 - val_mae: 458.9044\n",
            "Epoch 859/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 697380.3750 - mae: 630.4065 - val_loss: 217239.6250 - val_mae: 460.3612\n",
            "Epoch 860/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 681318.7500 - mae: 623.0933 - val_loss: 218068.1094 - val_mae: 461.3646\n",
            "Epoch 861/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 691331.0625 - mae: 630.7741 - val_loss: 218669.3281 - val_mae: 462.1115\n",
            "Epoch 862/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 700244.3750 - mae: 633.4528 - val_loss: 214119.4531 - val_mae: 457.3021\n",
            "Epoch 863/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 694656.8125 - mae: 635.5145 - val_loss: 217705.5781 - val_mae: 461.0000\n",
            "Epoch 864/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 693813.8125 - mae: 631.2622 - val_loss: 217316.9219 - val_mae: 460.5800\n",
            "Epoch 865/1000\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 688391.6250 - mae: 632.5959 - val_loss: 216428.0469 - val_mae: 459.8496\n",
            "Epoch 866/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 703635.4375 - mae: 636.8455 - val_loss: 212789.1719 - val_mae: 456.0809\n",
            "Epoch 867/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 696299.1250 - mae: 632.6872 - val_loss: 216412.5156 - val_mae: 459.8745\n",
            "Epoch 868/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 689103.6250 - mae: 632.1237 - val_loss: 217247.3594 - val_mae: 460.5402\n",
            "Epoch 869/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 689276.5000 - mae: 630.5318 - val_loss: 213757.7344 - val_mae: 457.0208\n",
            "Epoch 870/1000\n",
            "37/37 [==============================] - 9s 215ms/step - loss: 677210.1875 - mae: 621.7548 - val_loss: 215675.5781 - val_mae: 459.0521\n",
            "Epoch 871/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 682420.2500 - mae: 625.0483 - val_loss: 219517.2969 - val_mae: 463.1146\n",
            "Epoch 872/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 689148.6250 - mae: 627.4979 - val_loss: 218973.1719 - val_mae: 462.4285\n",
            "Epoch 873/1000\n",
            "37/37 [==============================] - 5s 101ms/step - loss: 675729.0625 - mae: 623.2931 - val_loss: 216137.3281 - val_mae: 459.6146\n",
            "Epoch 874/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 675669.7500 - mae: 625.7666 - val_loss: 215872.3906 - val_mae: 459.3333\n",
            "Epoch 875/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 672181.4375 - mae: 623.9363 - val_loss: 215088.9375 - val_mae: 458.4127\n",
            "Epoch 876/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 689509.2500 - mae: 630.2407 - val_loss: 218901.0000 - val_mae: 462.4632\n",
            "Epoch 877/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 647203.9375 - mae: 608.5760 - val_loss: 217753.2500 - val_mae: 461.3646\n",
            "Epoch 878/1000\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 656621.5000 - mae: 616.4164 - val_loss: 220170.2969 - val_mae: 463.9884\n",
            "Epoch 879/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 672948.5000 - mae: 622.6209 - val_loss: 217983.9844 - val_mae: 461.6458\n",
            "Epoch 880/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 672593.6875 - mae: 622.3259 - val_loss: 216583.6719 - val_mae: 459.9640\n",
            "Epoch 881/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 657813.9375 - mae: 615.1981 - val_loss: 220447.0156 - val_mae: 464.3883\n",
            "Epoch 882/1000\n",
            "37/37 [==============================] - 6s 130ms/step - loss: 687171.0000 - mae: 631.1467 - val_loss: 219858.5781 - val_mae: 463.6771\n",
            "Epoch 883/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 661792.0625 - mae: 615.2744 - val_loss: 218143.2500 - val_mae: 461.6428\n",
            "Epoch 884/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 657288.7500 - mae: 617.4594 - val_loss: 215531.2656 - val_mae: 459.0521\n",
            "Epoch 885/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 680439.3750 - mae: 625.2993 - val_loss: 216373.0000 - val_mae: 460.0099\n",
            "Epoch 886/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 670124.8750 - mae: 621.7010 - val_loss: 216660.8125 - val_mae: 460.3866\n",
            "Epoch 887/1000\n",
            "37/37 [==============================] - 6s 132ms/step - loss: 678758.8125 - mae: 623.8599 - val_loss: 213884.2031 - val_mae: 457.3021\n",
            "Epoch 888/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 671437.0000 - mae: 621.6498 - val_loss: 216369.6250 - val_mae: 459.7731\n",
            "Epoch 889/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 692654.1250 - mae: 634.3637 - val_loss: 218030.9219 - val_mae: 461.8081\n",
            "Epoch 890/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 675907.4375 - mae: 626.3292 - val_loss: 217687.8906 - val_mae: 461.1916\n",
            "Epoch 891/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 665985.0625 - mae: 618.5508 - val_loss: 216041.1406 - val_mae: 459.4193\n",
            "Epoch 892/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 657814.0625 - mae: 618.8417 - val_loss: 216017.8281 - val_mae: 459.3941\n",
            "Epoch 893/1000\n",
            "37/37 [==============================] - 5s 120ms/step - loss: 675173.6250 - mae: 625.3947 - val_loss: 219084.6875 - val_mae: 462.8202\n",
            "Epoch 894/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 666663.7500 - mae: 623.6396 - val_loss: 217848.4219 - val_mae: 461.3763\n",
            "Epoch 895/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 664638.3750 - mae: 624.3999 - val_loss: 216486.3750 - val_mae: 460.2149\n",
            "Epoch 896/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 673209.1875 - mae: 628.0320 - val_loss: 216033.9375 - val_mae: 459.6145\n",
            "Epoch 897/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 674612.7500 - mae: 626.3616 - val_loss: 217046.3281 - val_mae: 460.3964\n",
            "Epoch 898/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 678389.0625 - mae: 626.6849 - val_loss: 218243.8906 - val_mae: 461.9143\n",
            "Epoch 899/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 673502.4375 - mae: 625.0179 - val_loss: 216541.3281 - val_mae: 460.2642\n",
            "Epoch 900/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 656039.2500 - mae: 617.9423 - val_loss: 217912.4219 - val_mae: 461.6458\n",
            "Epoch 901/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 667823.1250 - mae: 628.8708 - val_loss: 215583.1719 - val_mae: 458.8922\n",
            "Epoch 902/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 677144.1875 - mae: 630.6165 - val_loss: 215020.2500 - val_mae: 458.3664\n",
            "Epoch 903/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 657397.7500 - mae: 618.8964 - val_loss: 215811.4531 - val_mae: 459.3333\n",
            "Epoch 904/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 661263.8750 - mae: 618.6293 - val_loss: 213969.5469 - val_mae: 457.3021\n",
            "Epoch 905/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 661455.3750 - mae: 618.4733 - val_loss: 216820.6875 - val_mae: 460.1050\n",
            "Epoch 906/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 658057.1875 - mae: 619.7219 - val_loss: 216036.3281 - val_mae: 459.3333\n",
            "Epoch 907/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 674743.7500 - mae: 632.0016 - val_loss: 216946.1719 - val_mae: 460.6428\n",
            "Epoch 908/1000\n",
            "37/37 [==============================] - 6s 133ms/step - loss: 677949.8125 - mae: 629.4726 - val_loss: 217852.6406 - val_mae: 461.3137\n",
            "Epoch 909/1000\n",
            "37/37 [==============================] - 6s 139ms/step - loss: 673611.3750 - mae: 627.9307 - val_loss: 218694.0000 - val_mae: 462.3429\n",
            "Epoch 910/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 650397.8750 - mae: 621.4915 - val_loss: 214755.7031 - val_mae: 457.9869\n",
            "Epoch 911/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 630438.3125 - mae: 607.4861 - val_loss: 216856.5469 - val_mae: 460.2878\n",
            "Epoch 912/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 678069.8750 - mae: 631.9331 - val_loss: 215910.0469 - val_mae: 459.3333\n",
            "Epoch 913/1000\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 661721.3125 - mae: 626.6652 - val_loss: 214361.2344 - val_mae: 457.5833\n",
            "Epoch 914/1000\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 666797.2500 - mae: 624.8540 - val_loss: 217499.4219 - val_mae: 461.0833\n",
            "Epoch 915/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 675534.4375 - mae: 630.8784 - val_loss: 215955.6406 - val_mae: 459.3333\n",
            "Epoch 916/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 668614.1250 - mae: 627.9745 - val_loss: 217768.7656 - val_mae: 461.1194\n",
            "Epoch 917/1000\n",
            "37/37 [==============================] - 5s 118ms/step - loss: 658366.0625 - mae: 627.1364 - val_loss: 214789.7969 - val_mae: 457.9009\n",
            "Epoch 918/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 668649.6875 - mae: 627.3430 - val_loss: 217487.2344 - val_mae: 460.7885\n",
            "Epoch 919/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 651566.3750 - mae: 620.0225 - val_loss: 216946.2344 - val_mae: 460.2024\n",
            "Epoch 920/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 655984.6875 - mae: 622.8924 - val_loss: 218705.7500 - val_mae: 462.2088\n",
            "Epoch 921/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 663651.3750 - mae: 622.9214 - val_loss: 217470.2031 - val_mae: 460.7157\n",
            "Epoch 922/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 642018.9375 - mae: 612.1928 - val_loss: 215658.7344 - val_mae: 458.6602\n",
            "Epoch 923/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 670891.3750 - mae: 630.9998 - val_loss: 217098.4531 - val_mae: 460.5570\n",
            "Epoch 924/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 655814.6875 - mae: 623.7509 - val_loss: 221004.0625 - val_mae: 464.9128\n",
            "Epoch 925/1000\n",
            "37/37 [==============================] - 6s 137ms/step - loss: 641727.3125 - mae: 614.4061 - val_loss: 216643.3594 - val_mae: 460.0789\n",
            "Epoch 926/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 671044.6250 - mae: 631.1470 - val_loss: 215666.6250 - val_mae: 458.8431\n",
            "Epoch 927/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 649705.1875 - mae: 617.0502 - val_loss: 216960.9219 - val_mae: 460.0924\n",
            "Epoch 928/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 642688.5625 - mae: 614.1163 - val_loss: 218284.6250 - val_mae: 461.6458\n",
            "Epoch 929/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 661119.9375 - mae: 625.2871 - val_loss: 216715.7969 - val_mae: 459.7866\n",
            "Epoch 930/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 653924.5000 - mae: 619.0967 - val_loss: 220911.0000 - val_mae: 464.7046\n",
            "Epoch 931/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 649796.5000 - mae: 616.8298 - val_loss: 218528.2031 - val_mae: 461.7943\n",
            "Epoch 932/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 664684.3750 - mae: 628.9351 - val_loss: 217459.8906 - val_mae: 460.4497\n",
            "Epoch 933/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 644135.4375 - mae: 620.8491 - val_loss: 221279.4219 - val_mae: 465.0221\n",
            "Epoch 934/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 653584.2500 - mae: 621.1033 - val_loss: 217061.0625 - val_mae: 460.0075\n",
            "Epoch 935/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 653154.1875 - mae: 619.2029 - val_loss: 216804.6250 - val_mae: 459.7147\n",
            "Epoch 936/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 653535.1250 - mae: 623.7176 - val_loss: 216822.2344 - val_mae: 459.7024\n",
            "Epoch 937/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 659571.6250 - mae: 624.9727 - val_loss: 219647.3281 - val_mae: 463.0392\n",
            "Epoch 938/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 661063.5000 - mae: 625.6509 - val_loss: 220042.4375 - val_mae: 463.3958\n",
            "Epoch 939/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 648197.2500 - mae: 623.2855 - val_loss: 214768.3750 - val_mae: 457.3020\n",
            "Epoch 940/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 653481.0000 - mae: 624.4826 - val_loss: 220372.2969 - val_mae: 463.6771\n",
            "Epoch 941/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 652548.9375 - mae: 621.3091 - val_loss: 219808.6719 - val_mae: 463.0880\n",
            "Epoch 942/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 659904.6250 - mae: 627.0186 - val_loss: 216342.5625 - val_mae: 458.7498\n",
            "Epoch 943/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 644462.3750 - mae: 621.2375 - val_loss: 217238.9844 - val_mae: 459.8982\n",
            "Epoch 944/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 672770.8125 - mae: 636.9279 - val_loss: 222889.9375 - val_mae: 466.6341\n",
            "Epoch 945/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 648339.8750 - mae: 625.6271 - val_loss: 216199.5156 - val_mae: 458.7931\n",
            "Epoch 946/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 644686.1250 - mae: 619.1110 - val_loss: 217035.6406 - val_mae: 459.5798\n",
            "Epoch 947/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 649739.4375 - mae: 626.0425 - val_loss: 217095.4844 - val_mae: 459.6146\n",
            "Epoch 948/1000\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 651550.6875 - mae: 621.4334 - val_loss: 218325.0781 - val_mae: 461.0833\n",
            "Epoch 949/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 645734.2500 - mae: 624.0645 - val_loss: 217113.3750 - val_mae: 459.5429\n",
            "Epoch 950/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 636528.2500 - mae: 618.1182 - val_loss: 215880.2031 - val_mae: 457.9796\n",
            "Epoch 951/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 631542.6875 - mae: 615.6439 - val_loss: 217443.4531 - val_mae: 459.8009\n",
            "Epoch 952/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 653412.9375 - mae: 624.5252 - val_loss: 217471.1406 - val_mae: 459.7892\n",
            "Epoch 953/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 651640.2500 - mae: 625.0245 - val_loss: 219067.5781 - val_mae: 461.6458\n",
            "Epoch 954/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 670525.5000 - mae: 637.2073 - val_loss: 215677.4219 - val_mae: 457.6000\n",
            "Epoch 955/1000\n",
            "37/37 [==============================] - 10s 244ms/step - loss: 652485.4375 - mae: 623.3546 - val_loss: 219293.8750 - val_mae: 461.7824\n",
            "Epoch 956/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 645256.3125 - mae: 623.3787 - val_loss: 220361.3750 - val_mae: 463.1146\n",
            "Epoch 957/1000\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 643438.0625 - mae: 619.3472 - val_loss: 217512.4844 - val_mae: 459.6146\n",
            "Epoch 958/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 626454.0625 - mae: 615.1743 - val_loss: 218998.4844 - val_mae: 461.3645\n",
            "Epoch 959/1000\n",
            "37/37 [==============================] - 5s 121ms/step - loss: 627413.9375 - mae: 612.4529 - val_loss: 220757.4844 - val_mae: 463.3958\n",
            "Epoch 960/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 642124.8125 - mae: 618.8936 - val_loss: 215644.5000 - val_mae: 457.3021\n",
            "Epoch 961/1000\n",
            "37/37 [==============================] - 5s 102ms/step - loss: 651363.5000 - mae: 628.3600 - val_loss: 215549.7500 - val_mae: 457.1508\n",
            "Epoch 962/1000\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 629546.4375 - mae: 613.7526 - val_loss: 215800.3750 - val_mae: 457.3552\n",
            "Epoch 963/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 639410.1250 - mae: 617.0845 - val_loss: 217548.5000 - val_mae: 459.3744\n",
            "Epoch 964/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 644997.0625 - mae: 622.5652 - val_loss: 217837.2500 - val_mae: 459.6146\n",
            "Epoch 965/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 652674.1250 - mae: 628.5338 - val_loss: 218186.7031 - val_mae: 459.9127\n",
            "Epoch 966/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 644712.1250 - mae: 624.0508 - val_loss: 217377.4844 - val_mae: 459.0565\n",
            "Epoch 967/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 646524.5000 - mae: 624.9290 - val_loss: 217421.2969 - val_mae: 459.0521\n",
            "Epoch 968/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 616753.5000 - mae: 613.1212 - val_loss: 217734.7969 - val_mae: 459.3145\n",
            "Epoch 969/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 647911.1250 - mae: 628.7303 - val_loss: 218090.3750 - val_mae: 459.6146\n",
            "Epoch 970/1000\n",
            "37/37 [==============================] - 10s 251ms/step - loss: 645824.0000 - mae: 623.1542 - val_loss: 219313.1094 - val_mae: 461.1264\n",
            "Epoch 971/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 635249.4375 - mae: 618.3384 - val_loss: 219318.9375 - val_mae: 461.0833\n",
            "Epoch 972/1000\n",
            "37/37 [==============================] - 10s 255ms/step - loss: 641455.6250 - mae: 621.4533 - val_loss: 218397.3125 - val_mae: 459.7612\n",
            "Epoch 973/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 630625.8750 - mae: 614.2732 - val_loss: 217732.4375 - val_mae: 459.0521\n",
            "Epoch 974/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 624492.3125 - mae: 613.3883 - val_loss: 216581.2656 - val_mae: 457.4925\n",
            "Epoch 975/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 618107.5000 - mae: 607.2339 - val_loss: 216805.0781 - val_mae: 457.6599\n",
            "Epoch 976/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 645030.6250 - mae: 625.2192 - val_loss: 218013.2500 - val_mae: 459.1660\n",
            "Epoch 977/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 625563.8750 - mae: 615.2347 - val_loss: 216845.5000 - val_mae: 457.5833\n",
            "Epoch 978/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 631025.1875 - mae: 618.3422 - val_loss: 219826.4844 - val_mae: 461.2211\n",
            "Epoch 979/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 633525.1875 - mae: 618.6448 - val_loss: 220025.0469 - val_mae: 461.3646\n",
            "Epoch 980/1000\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 647476.8750 - mae: 627.9996 - val_loss: 221633.0000 - val_mae: 463.2762\n",
            "Epoch 981/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 645556.9375 - mae: 628.3174 - val_loss: 219960.8281 - val_mae: 461.1909\n",
            "Epoch 982/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 621242.9375 - mae: 612.7806 - val_loss: 218722.8281 - val_mae: 459.5255\n",
            "Epoch 983/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 646943.9375 - mae: 628.1749 - val_loss: 218793.2031 - val_mae: 459.5304\n",
            "Epoch 984/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 633424.7500 - mae: 619.1809 - val_loss: 220382.4219 - val_mae: 461.4375\n",
            "Epoch 985/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 638730.1875 - mae: 622.8522 - val_loss: 217053.0156 - val_mae: 457.3021\n",
            "Epoch 986/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 626965.6875 - mae: 617.3289 - val_loss: 216636.6406 - val_mae: 456.8369\n",
            "Epoch 987/1000\n",
            "37/37 [==============================] - 10s 254ms/step - loss: 640864.6875 - mae: 625.0217 - val_loss: 220492.2031 - val_mae: 461.3646\n",
            "Epoch 988/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 621425.9375 - mae: 613.2956 - val_loss: 217270.2344 - val_mae: 457.3276\n",
            "Epoch 989/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 629848.3125 - mae: 618.1869 - val_loss: 220045.0781 - val_mae: 460.8156\n",
            "Epoch 990/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 642196.0625 - mae: 626.2966 - val_loss: 220964.2344 - val_mae: 461.6458\n",
            "Epoch 991/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 639007.6875 - mae: 623.8905 - val_loss: 219083.8594 - val_mae: 459.3226\n",
            "Epoch 992/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 624204.0625 - mae: 611.9319 - val_loss: 219182.1875 - val_mae: 459.3557\n",
            "Epoch 993/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 615902.3125 - mae: 614.7624 - val_loss: 219225.1250 - val_mae: 459.3334\n",
            "Epoch 994/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 612992.5000 - mae: 616.8846 - val_loss: 218952.6250 - val_mae: 459.0066\n",
            "Epoch 995/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 624529.8750 - mae: 618.6755 - val_loss: 219419.1875 - val_mae: 459.3912\n",
            "Epoch 996/1000\n",
            "37/37 [==============================] - 6s 148ms/step - loss: 644266.6875 - mae: 627.5591 - val_loss: 217141.1250 - val_mae: 456.6698\n",
            "Epoch 997/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 627107.3125 - mae: 618.4545 - val_loss: 217490.5469 - val_mae: 456.9396\n",
            "Epoch 998/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 645806.9375 - mae: 632.1890 - val_loss: 219569.7031 - val_mae: 459.3333\n",
            "Epoch 999/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 630295.9375 - mae: 619.2153 - val_loss: 219744.8750 - val_mae: 459.4386\n",
            "Epoch 1000/1000\n",
            "37/37 [==============================] - 5s 103ms/step - loss: 622372.3125 - mae: 612.3853 - val_loss: 219178.9219 - val_mae: 458.8186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history "
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76ccb596-4df8-4a26-fc69-331a2c1b1447"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [1430541.625,\n",
              "  1434987.875,\n",
              "  1453779.25,\n",
              "  1468546.625,\n",
              "  1410761.0,\n",
              "  1432802.875,\n",
              "  1444587.5,\n",
              "  1450186.125,\n",
              "  1451866.5,\n",
              "  1468367.375,\n",
              "  1430483.375,\n",
              "  1467144.0,\n",
              "  1421729.875,\n",
              "  1372957.5,\n",
              "  1422590.0,\n",
              "  1417359.625,\n",
              "  1425695.0,\n",
              "  1445597.875,\n",
              "  1396278.5,\n",
              "  1438399.625,\n",
              "  1419936.625,\n",
              "  1392040.75,\n",
              "  1410775.5,\n",
              "  1390133.625,\n",
              "  1425041.0,\n",
              "  1402114.625,\n",
              "  1411709.875,\n",
              "  1386668.875,\n",
              "  1420029.125,\n",
              "  1400001.875,\n",
              "  1407646.25,\n",
              "  1392922.875,\n",
              "  1381783.625,\n",
              "  1399789.875,\n",
              "  1375635.625,\n",
              "  1357534.5,\n",
              "  1363275.125,\n",
              "  1417830.125,\n",
              "  1382671.375,\n",
              "  1395382.625,\n",
              "  1401869.625,\n",
              "  1396488.875,\n",
              "  1381589.625,\n",
              "  1397916.75,\n",
              "  1396445.75,\n",
              "  1390635.125,\n",
              "  1369770.625,\n",
              "  1396774.75,\n",
              "  1399480.625,\n",
              "  1396862.5,\n",
              "  1376167.5,\n",
              "  1358865.375,\n",
              "  1348201.25,\n",
              "  1364869.125,\n",
              "  1376675.375,\n",
              "  1383211.25,\n",
              "  1382499.125,\n",
              "  1379250.375,\n",
              "  1371922.25,\n",
              "  1339750.0,\n",
              "  1375669.875,\n",
              "  1380594.125,\n",
              "  1360714.5,\n",
              "  1358291.0,\n",
              "  1384696.25,\n",
              "  1386165.0,\n",
              "  1359436.625,\n",
              "  1366340.75,\n",
              "  1333319.0,\n",
              "  1357433.0,\n",
              "  1367394.375,\n",
              "  1365513.5,\n",
              "  1345141.875,\n",
              "  1347994.0,\n",
              "  1357127.0,\n",
              "  1364089.5,\n",
              "  1367619.25,\n",
              "  1347809.25,\n",
              "  1346992.75,\n",
              "  1359465.875,\n",
              "  1322985.625,\n",
              "  1353787.875,\n",
              "  1378905.875,\n",
              "  1342949.375,\n",
              "  1306970.125,\n",
              "  1323033.25,\n",
              "  1334257.0,\n",
              "  1333463.125,\n",
              "  1368585.25,\n",
              "  1334918.75,\n",
              "  1335059.0,\n",
              "  1320892.375,\n",
              "  1331056.375,\n",
              "  1309529.875,\n",
              "  1325299.625,\n",
              "  1348705.75,\n",
              "  1344130.125,\n",
              "  1339883.375,\n",
              "  1321393.125,\n",
              "  1317101.125,\n",
              "  1326357.125,\n",
              "  1315228.5,\n",
              "  1292981.75,\n",
              "  1332302.75,\n",
              "  1322803.375,\n",
              "  1303788.625,\n",
              "  1311872.375,\n",
              "  1309208.125,\n",
              "  1336493.375,\n",
              "  1331477.375,\n",
              "  1318956.75,\n",
              "  1288069.5,\n",
              "  1303944.875,\n",
              "  1307034.25,\n",
              "  1323501.125,\n",
              "  1336595.625,\n",
              "  1296023.5,\n",
              "  1286734.0,\n",
              "  1293360.875,\n",
              "  1299866.375,\n",
              "  1321124.5,\n",
              "  1295341.0,\n",
              "  1294101.625,\n",
              "  1314214.125,\n",
              "  1305942.625,\n",
              "  1300026.125,\n",
              "  1253213.625,\n",
              "  1275858.375,\n",
              "  1313292.125,\n",
              "  1289713.0,\n",
              "  1295161.625,\n",
              "  1274933.875,\n",
              "  1277324.875,\n",
              "  1261296.375,\n",
              "  1287438.625,\n",
              "  1276482.625,\n",
              "  1267284.5,\n",
              "  1278813.125,\n",
              "  1255854.375,\n",
              "  1271412.25,\n",
              "  1294439.375,\n",
              "  1311391.625,\n",
              "  1232615.0,\n",
              "  1263767.625,\n",
              "  1258264.125,\n",
              "  1275030.125,\n",
              "  1255973.625,\n",
              "  1273271.0,\n",
              "  1246500.0,\n",
              "  1269495.5,\n",
              "  1283446.75,\n",
              "  1234401.375,\n",
              "  1238434.5,\n",
              "  1295606.625,\n",
              "  1259728.75,\n",
              "  1259357.375,\n",
              "  1268454.5,\n",
              "  1234781.0,\n",
              "  1272315.75,\n",
              "  1249291.375,\n",
              "  1198445.75,\n",
              "  1237252.25,\n",
              "  1249201.5,\n",
              "  1250111.125,\n",
              "  1273642.375,\n",
              "  1249854.5,\n",
              "  1242702.5,\n",
              "  1240946.875,\n",
              "  1269010.375,\n",
              "  1274135.875,\n",
              "  1253975.5,\n",
              "  1253028.625,\n",
              "  1233967.5,\n",
              "  1240884.125,\n",
              "  1247599.125,\n",
              "  1248271.5,\n",
              "  1227383.5,\n",
              "  1204309.375,\n",
              "  1220599.375,\n",
              "  1259124.25,\n",
              "  1226401.75,\n",
              "  1208410.0,\n",
              "  1223358.125,\n",
              "  1212070.875,\n",
              "  1232288.75,\n",
              "  1208028.75,\n",
              "  1246223.625,\n",
              "  1196553.625,\n",
              "  1200960.0,\n",
              "  1242440.125,\n",
              "  1234047.0,\n",
              "  1217288.875,\n",
              "  1205350.375,\n",
              "  1213273.5,\n",
              "  1210275.5,\n",
              "  1190750.0,\n",
              "  1191816.0,\n",
              "  1205918.875,\n",
              "  1191946.375,\n",
              "  1222617.75,\n",
              "  1182916.75,\n",
              "  1187719.25,\n",
              "  1164925.875,\n",
              "  1194760.125,\n",
              "  1235347.25,\n",
              "  1188921.0,\n",
              "  1198196.375,\n",
              "  1187129.125,\n",
              "  1173831.75,\n",
              "  1218138.75,\n",
              "  1202784.125,\n",
              "  1188934.0,\n",
              "  1189222.375,\n",
              "  1191336.0,\n",
              "  1221186.625,\n",
              "  1203057.875,\n",
              "  1215239.25,\n",
              "  1196256.625,\n",
              "  1200934.25,\n",
              "  1215828.75,\n",
              "  1175045.125,\n",
              "  1207114.875,\n",
              "  1161021.0,\n",
              "  1194055.625,\n",
              "  1199353.875,\n",
              "  1187903.625,\n",
              "  1191865.75,\n",
              "  1181638.375,\n",
              "  1162064.625,\n",
              "  1178206.25,\n",
              "  1216576.375,\n",
              "  1179176.25,\n",
              "  1162749.25,\n",
              "  1187433.0,\n",
              "  1183586.375,\n",
              "  1168099.0,\n",
              "  1159577.375,\n",
              "  1173714.25,\n",
              "  1180400.875,\n",
              "  1160304.125,\n",
              "  1192840.0,\n",
              "  1177851.5,\n",
              "  1188721.25,\n",
              "  1145189.125,\n",
              "  1136731.5,\n",
              "  1187479.625,\n",
              "  1160847.375,\n",
              "  1189270.75,\n",
              "  1156099.75,\n",
              "  1179986.75,\n",
              "  1150173.5,\n",
              "  1154771.625,\n",
              "  1140745.25,\n",
              "  1139520.875,\n",
              "  1146396.375,\n",
              "  1159116.125,\n",
              "  1109129.0,\n",
              "  1148087.125,\n",
              "  1135914.0,\n",
              "  1157589.875,\n",
              "  1165711.75,\n",
              "  1173095.0,\n",
              "  1129448.625,\n",
              "  1151786.75,\n",
              "  1125245.5,\n",
              "  1137827.625,\n",
              "  1154452.5,\n",
              "  1152226.5,\n",
              "  1121303.875,\n",
              "  1124710.625,\n",
              "  1146235.25,\n",
              "  1133946.75,\n",
              "  1143504.375,\n",
              "  1119297.875,\n",
              "  1148540.5,\n",
              "  1105978.25,\n",
              "  1128629.125,\n",
              "  1141107.25,\n",
              "  1148866.875,\n",
              "  1138608.25,\n",
              "  1125796.75,\n",
              "  1093708.75,\n",
              "  1115642.75,\n",
              "  1102545.25,\n",
              "  1122561.25,\n",
              "  1127199.125,\n",
              "  1133712.375,\n",
              "  1102380.5,\n",
              "  1151459.625,\n",
              "  1115943.625,\n",
              "  1104216.5,\n",
              "  1095903.25,\n",
              "  1119595.375,\n",
              "  1134074.375,\n",
              "  1114345.75,\n",
              "  1097714.25,\n",
              "  1081423.875,\n",
              "  1136254.0,\n",
              "  1100998.625,\n",
              "  1114542.0,\n",
              "  1116737.25,\n",
              "  1071778.25,\n",
              "  1084381.625,\n",
              "  1119964.875,\n",
              "  1091697.0,\n",
              "  1109586.75,\n",
              "  1095012.625,\n",
              "  1105681.125,\n",
              "  1111681.75,\n",
              "  1099800.125,\n",
              "  1108328.125,\n",
              "  1081784.0,\n",
              "  1122387.375,\n",
              "  1108446.0,\n",
              "  1112260.0,\n",
              "  1101412.625,\n",
              "  1025083.5,\n",
              "  1108037.125,\n",
              "  1102127.75,\n",
              "  1106191.5,\n",
              "  1102355.25,\n",
              "  1055754.75,\n",
              "  1048403.3125,\n",
              "  1067653.625,\n",
              "  1066999.5,\n",
              "  1097477.125,\n",
              "  1074793.875,\n",
              "  1075635.625,\n",
              "  1075106.125,\n",
              "  1101554.375,\n",
              "  1078063.25,\n",
              "  1098563.375,\n",
              "  1063154.125,\n",
              "  1060303.5,\n",
              "  1065859.25,\n",
              "  1087818.625,\n",
              "  1060402.5,\n",
              "  1056658.375,\n",
              "  1077704.0,\n",
              "  1086537.25,\n",
              "  1050196.625,\n",
              "  1068712.75,\n",
              "  1063113.0,\n",
              "  1074372.25,\n",
              "  1074762.0,\n",
              "  1074269.875,\n",
              "  1033042.0,\n",
              "  1054733.375,\n",
              "  1047695.5625,\n",
              "  1053127.125,\n",
              "  1052428.375,\n",
              "  1039691.125,\n",
              "  1071170.875,\n",
              "  1047799.0625,\n",
              "  1052545.375,\n",
              "  1063047.75,\n",
              "  1048600.875,\n",
              "  1075857.375,\n",
              "  1058267.75,\n",
              "  1056776.0,\n",
              "  1065499.875,\n",
              "  1075415.0,\n",
              "  1057371.625,\n",
              "  1028456.5,\n",
              "  1017984.75,\n",
              "  1055059.625,\n",
              "  1037363.625,\n",
              "  1023686.625,\n",
              "  1016357.9375,\n",
              "  1027325.0,\n",
              "  1050814.0,\n",
              "  1042695.5,\n",
              "  1046725.0625,\n",
              "  1027231.5625,\n",
              "  1044624.75,\n",
              "  1045962.375,\n",
              "  1032925.5625,\n",
              "  1023660.6875,\n",
              "  1042492.8125,\n",
              "  1008974.0,\n",
              "  1048649.625,\n",
              "  1010896.0,\n",
              "  1040226.3125,\n",
              "  1022895.8125,\n",
              "  1027257.5,\n",
              "  1045735.0625,\n",
              "  1031786.6875,\n",
              "  1028776.5625,\n",
              "  1036647.75,\n",
              "  1036953.25,\n",
              "  1035259.5625,\n",
              "  1051282.25,\n",
              "  988735.4375,\n",
              "  1026873.5,\n",
              "  1008287.6875,\n",
              "  1025309.875,\n",
              "  1021403.0625,\n",
              "  1013916.125,\n",
              "  984642.4375,\n",
              "  1015878.375,\n",
              "  1013464.375,\n",
              "  1004333.4375,\n",
              "  982038.0625,\n",
              "  1021670.0625,\n",
              "  988911.0,\n",
              "  1037289.9375,\n",
              "  1008493.4375,\n",
              "  1011180.5625,\n",
              "  1012316.125,\n",
              "  983630.375,\n",
              "  994931.625,\n",
              "  1022793.25,\n",
              "  996589.6875,\n",
              "  994714.375,\n",
              "  1023591.75,\n",
              "  1000868.875,\n",
              "  991718.75,\n",
              "  1014004.1875,\n",
              "  968283.8125,\n",
              "  1003300.4375,\n",
              "  1003155.5,\n",
              "  974602.25,\n",
              "  988775.3125,\n",
              "  987648.25,\n",
              "  994035.4375,\n",
              "  1010280.8125,\n",
              "  989260.375,\n",
              "  968616.625,\n",
              "  978811.9375,\n",
              "  1014461.8125,\n",
              "  968336.75,\n",
              "  964670.4375,\n",
              "  998037.5,\n",
              "  991367.625,\n",
              "  982432.0,\n",
              "  975530.0625,\n",
              "  946404.1875,\n",
              "  965542.0625,\n",
              "  971532.5625,\n",
              "  978990.6875,\n",
              "  969860.75,\n",
              "  985733.1875,\n",
              "  978184.6875,\n",
              "  975603.625,\n",
              "  997872.5625,\n",
              "  967285.9375,\n",
              "  993052.6875,\n",
              "  975312.0,\n",
              "  986887.8125,\n",
              "  958818.125,\n",
              "  981316.875,\n",
              "  980888.8125,\n",
              "  965130.5,\n",
              "  973397.0625,\n",
              "  953305.8125,\n",
              "  951090.0,\n",
              "  959596.125,\n",
              "  953859.625,\n",
              "  939367.0625,\n",
              "  937838.125,\n",
              "  940450.75,\n",
              "  960410.8125,\n",
              "  990492.5,\n",
              "  949373.875,\n",
              "  956035.3125,\n",
              "  951280.75,\n",
              "  944721.1875,\n",
              "  974593.75,\n",
              "  937634.875,\n",
              "  929984.875,\n",
              "  972291.625,\n",
              "  944781.4375,\n",
              "  960600.1875,\n",
              "  957704.625,\n",
              "  944232.6875,\n",
              "  918486.8125,\n",
              "  944044.6875,\n",
              "  937785.6875,\n",
              "  943803.0625,\n",
              "  921918.375,\n",
              "  949388.8125,\n",
              "  945103.75,\n",
              "  942433.125,\n",
              "  944924.8125,\n",
              "  935814.8125,\n",
              "  926763.5,\n",
              "  946401.0,\n",
              "  939710.25,\n",
              "  938514.5625,\n",
              "  942998.625,\n",
              "  901068.5,\n",
              "  928530.625,\n",
              "  943145.1875,\n",
              "  908181.4375,\n",
              "  948630.3125,\n",
              "  941676.9375,\n",
              "  944784.0,\n",
              "  909457.1875,\n",
              "  934859.8125,\n",
              "  922891.5,\n",
              "  936620.8125,\n",
              "  913985.625,\n",
              "  919976.8125,\n",
              "  936004.3125,\n",
              "  915587.6875,\n",
              "  919638.1875,\n",
              "  911306.0,\n",
              "  923857.1875,\n",
              "  950266.9375,\n",
              "  932060.5625,\n",
              "  905801.375,\n",
              "  923978.8125,\n",
              "  901360.3125,\n",
              "  933501.375,\n",
              "  931170.125,\n",
              "  925510.5,\n",
              "  910082.5,\n",
              "  898958.0625,\n",
              "  928812.5,\n",
              "  900970.125,\n",
              "  904136.0,\n",
              "  905924.9375,\n",
              "  918300.0,\n",
              "  935997.5625,\n",
              "  901832.875,\n",
              "  866003.5625,\n",
              "  903722.75,\n",
              "  911018.25,\n",
              "  886373.5,\n",
              "  873011.625,\n",
              "  911638.8125,\n",
              "  890605.25,\n",
              "  904536.6875,\n",
              "  894033.9375,\n",
              "  878109.0,\n",
              "  909211.75,\n",
              "  908657.875,\n",
              "  898794.6875,\n",
              "  890226.3125,\n",
              "  891368.6875,\n",
              "  896983.125,\n",
              "  909732.4375,\n",
              "  873144.9375,\n",
              "  880074.5625,\n",
              "  909791.125,\n",
              "  894758.625,\n",
              "  900085.375,\n",
              "  891785.1875,\n",
              "  886142.5625,\n",
              "  899597.75,\n",
              "  875811.0625,\n",
              "  890751.3125,\n",
              "  904124.5,\n",
              "  885018.0,\n",
              "  848330.5625,\n",
              "  900038.3125,\n",
              "  884637.0625,\n",
              "  878867.0,\n",
              "  887900.9375,\n",
              "  871591.125,\n",
              "  883171.875,\n",
              "  858325.125,\n",
              "  885844.0625,\n",
              "  887392.1875,\n",
              "  899223.9375,\n",
              "  881858.5625,\n",
              "  838892.25,\n",
              "  878562.375,\n",
              "  895306.5625,\n",
              "  834332.875,\n",
              "  879545.4375,\n",
              "  849905.1875,\n",
              "  852757.0,\n",
              "  851762.625,\n",
              "  873877.625,\n",
              "  876692.75,\n",
              "  855750.5625,\n",
              "  881177.875,\n",
              "  856311.25,\n",
              "  876988.4375,\n",
              "  848868.8125,\n",
              "  845018.4375,\n",
              "  883969.1875,\n",
              "  875510.8125,\n",
              "  851742.0625,\n",
              "  878681.125,\n",
              "  889325.875,\n",
              "  857799.6875,\n",
              "  876029.75,\n",
              "  900211.125,\n",
              "  825122.4375,\n",
              "  839211.125,\n",
              "  870086.875,\n",
              "  872976.4375,\n",
              "  869254.375,\n",
              "  842921.6875,\n",
              "  873346.4375,\n",
              "  863836.4375,\n",
              "  844181.0625,\n",
              "  859546.8125,\n",
              "  855285.875,\n",
              "  835140.875,\n",
              "  856307.125,\n",
              "  830868.0625,\n",
              "  860067.4375,\n",
              "  824198.6875,\n",
              "  833346.625,\n",
              "  853805.3125,\n",
              "  843059.6875,\n",
              "  834867.625,\n",
              "  817114.625,\n",
              "  848657.0,\n",
              "  866253.6875,\n",
              "  836989.0,\n",
              "  842227.1875,\n",
              "  826227.9375,\n",
              "  842555.375,\n",
              "  825463.0625,\n",
              "  843508.75,\n",
              "  843439.4375,\n",
              "  847050.9375,\n",
              "  840993.25,\n",
              "  806449.0,\n",
              "  840806.4375,\n",
              "  810657.375,\n",
              "  851672.3125,\n",
              "  817512.5625,\n",
              "  837246.8125,\n",
              "  827968.125,\n",
              "  847218.625,\n",
              "  814008.0625,\n",
              "  833297.9375,\n",
              "  849385.625,\n",
              "  825023.6875,\n",
              "  797573.125,\n",
              "  829504.1875,\n",
              "  830328.4375,\n",
              "  835615.125,\n",
              "  836504.875,\n",
              "  820047.8125,\n",
              "  814563.375,\n",
              "  808203.125,\n",
              "  805405.25,\n",
              "  808923.3125,\n",
              "  828260.875,\n",
              "  843689.6875,\n",
              "  788066.5,\n",
              "  846322.5625,\n",
              "  802670.875,\n",
              "  810529.125,\n",
              "  801699.75,\n",
              "  829441.125,\n",
              "  815954.3125,\n",
              "  831339.9375,\n",
              "  818914.125,\n",
              "  805064.375,\n",
              "  820198.6875,\n",
              "  825834.125,\n",
              "  788442.375,\n",
              "  795624.0625,\n",
              "  783858.8125,\n",
              "  810882.8125,\n",
              "  800215.0,\n",
              "  805847.8125,\n",
              "  796120.75,\n",
              "  803477.1875,\n",
              "  803078.75,\n",
              "  788214.9375,\n",
              "  797373.9375,\n",
              "  809972.0,\n",
              "  811449.1875,\n",
              "  804974.4375,\n",
              "  795299.5,\n",
              "  786680.25,\n",
              "  790397.25,\n",
              "  777255.9375,\n",
              "  802323.1875,\n",
              "  815132.1875,\n",
              "  793498.125,\n",
              "  815673.9375,\n",
              "  798797.5625,\n",
              "  795148.4375,\n",
              "  786567.4375,\n",
              "  797699.8125,\n",
              "  788302.3125,\n",
              "  786237.4375,\n",
              "  804553.125,\n",
              "  781046.9375,\n",
              "  798315.5,\n",
              "  811553.0625,\n",
              "  781312.1875,\n",
              "  798091.9375,\n",
              "  788760.1875,\n",
              "  795287.8125,\n",
              "  783813.3125,\n",
              "  786394.9375,\n",
              "  786318.75,\n",
              "  792253.4375,\n",
              "  785807.4375,\n",
              "  788422.625,\n",
              "  766366.8125,\n",
              "  796537.75,\n",
              "  769552.25,\n",
              "  794339.9375,\n",
              "  788721.1875,\n",
              "  798332.75,\n",
              "  792869.875,\n",
              "  759980.9375,\n",
              "  754221.875,\n",
              "  775536.5,\n",
              "  733335.75,\n",
              "  774989.5625,\n",
              "  776302.375,\n",
              "  774110.4375,\n",
              "  759307.9375,\n",
              "  784720.5,\n",
              "  781964.9375,\n",
              "  781864.8125,\n",
              "  756454.0625,\n",
              "  788347.8125,\n",
              "  766826.0625,\n",
              "  771097.1875,\n",
              "  774571.375,\n",
              "  773080.0,\n",
              "  784407.375,\n",
              "  768180.1875,\n",
              "  756606.875,\n",
              "  749538.375,\n",
              "  744711.25,\n",
              "  776523.6875,\n",
              "  762937.25,\n",
              "  772086.1875,\n",
              "  766488.9375,\n",
              "  781814.8125,\n",
              "  761013.25,\n",
              "  779360.4375,\n",
              "  765112.1875,\n",
              "  767539.0625,\n",
              "  748900.5625,\n",
              "  749678.0,\n",
              "  773386.3125,\n",
              "  782389.0625,\n",
              "  771641.1875,\n",
              "  747917.4375,\n",
              "  757175.1875,\n",
              "  739416.3125,\n",
              "  757069.125,\n",
              "  757876.9375,\n",
              "  756697.25,\n",
              "  762464.25,\n",
              "  763804.9375,\n",
              "  753964.75,\n",
              "  766634.3125,\n",
              "  722177.0,\n",
              "  726683.8125,\n",
              "  745037.0625,\n",
              "  720031.3125,\n",
              "  752738.8125,\n",
              "  752222.9375,\n",
              "  753049.875,\n",
              "  744126.25,\n",
              "  732193.5,\n",
              "  742516.5,\n",
              "  747223.3125,\n",
              "  751745.8125,\n",
              "  738110.1875,\n",
              "  727281.375,\n",
              "  722285.25,\n",
              "  754121.3125,\n",
              "  777270.4375,\n",
              "  748041.75,\n",
              "  729868.75,\n",
              "  739449.875,\n",
              "  739195.5625,\n",
              "  729421.375,\n",
              "  745054.5625,\n",
              "  724885.125,\n",
              "  757398.375,\n",
              "  718543.5625,\n",
              "  715160.6875,\n",
              "  738341.4375,\n",
              "  712333.4375,\n",
              "  714593.875,\n",
              "  725004.4375,\n",
              "  730815.8125,\n",
              "  728904.0625,\n",
              "  734837.4375,\n",
              "  730633.4375,\n",
              "  722447.375,\n",
              "  730028.375,\n",
              "  731581.6875,\n",
              "  716962.875,\n",
              "  738141.625,\n",
              "  731107.5,\n",
              "  732629.5625,\n",
              "  722945.6875,\n",
              "  713960.375,\n",
              "  736726.1875,\n",
              "  720266.4375,\n",
              "  725057.1875,\n",
              "  710596.75,\n",
              "  740457.0625,\n",
              "  730536.6875,\n",
              "  720207.9375,\n",
              "  726964.8125,\n",
              "  712597.125,\n",
              "  695316.5625,\n",
              "  694514.4375,\n",
              "  711812.375,\n",
              "  718165.125,\n",
              "  712565.125,\n",
              "  737953.625,\n",
              "  713963.3125,\n",
              "  704192.75,\n",
              "  714163.1875,\n",
              "  711271.625,\n",
              "  730777.1875,\n",
              "  729857.125,\n",
              "  726801.9375,\n",
              "  713732.1875,\n",
              "  703791.875,\n",
              "  710779.625,\n",
              "  697594.5625,\n",
              "  707354.9375,\n",
              "  707814.125,\n",
              "  719196.1875,\n",
              "  708873.625,\n",
              "  706762.875,\n",
              "  706403.9375,\n",
              "  704913.875,\n",
              "  715156.4375,\n",
              "  706028.125,\n",
              "  692697.0,\n",
              "  694595.1875,\n",
              "  718606.375,\n",
              "  711161.4375,\n",
              "  704472.25,\n",
              "  701040.125,\n",
              "  697369.4375,\n",
              "  705755.8125,\n",
              "  707423.6875,\n",
              "  689213.0625,\n",
              "  699139.25,\n",
              "  693034.1875,\n",
              "  700748.0,\n",
              "  696138.0,\n",
              "  709424.6875,\n",
              "  706192.8125,\n",
              "  698745.0625,\n",
              "  702774.5625,\n",
              "  699823.4375,\n",
              "  692309.125,\n",
              "  689371.3125,\n",
              "  671307.3125,\n",
              "  675322.8125,\n",
              "  682094.25,\n",
              "  678590.4375,\n",
              "  686424.25,\n",
              "  697380.375,\n",
              "  681318.75,\n",
              "  691331.0625,\n",
              "  700244.375,\n",
              "  694656.8125,\n",
              "  693813.8125,\n",
              "  688391.625,\n",
              "  703635.4375,\n",
              "  696299.125,\n",
              "  689103.625,\n",
              "  689276.5,\n",
              "  677210.1875,\n",
              "  682420.25,\n",
              "  689148.625,\n",
              "  675729.0625,\n",
              "  675669.75,\n",
              "  672181.4375,\n",
              "  689509.25,\n",
              "  647203.9375,\n",
              "  656621.5,\n",
              "  672948.5,\n",
              "  672593.6875,\n",
              "  657813.9375,\n",
              "  687171.0,\n",
              "  661792.0625,\n",
              "  657288.75,\n",
              "  680439.375,\n",
              "  670124.875,\n",
              "  678758.8125,\n",
              "  671437.0,\n",
              "  692654.125,\n",
              "  675907.4375,\n",
              "  665985.0625,\n",
              "  657814.0625,\n",
              "  675173.625,\n",
              "  666663.75,\n",
              "  664638.375,\n",
              "  673209.1875,\n",
              "  674612.75,\n",
              "  678389.0625,\n",
              "  673502.4375,\n",
              "  656039.25,\n",
              "  667823.125,\n",
              "  677144.1875,\n",
              "  657397.75,\n",
              "  661263.875,\n",
              "  661455.375,\n",
              "  658057.1875,\n",
              "  674743.75,\n",
              "  677949.8125,\n",
              "  673611.375,\n",
              "  650397.875,\n",
              "  630438.3125,\n",
              "  678069.875,\n",
              "  661721.3125,\n",
              "  666797.25,\n",
              "  675534.4375,\n",
              "  668614.125,\n",
              "  658366.0625,\n",
              "  668649.6875,\n",
              "  651566.375,\n",
              "  655984.6875,\n",
              "  663651.375,\n",
              "  642018.9375,\n",
              "  670891.375,\n",
              "  655814.6875,\n",
              "  641727.3125,\n",
              "  671044.625,\n",
              "  649705.1875,\n",
              "  642688.5625,\n",
              "  661119.9375,\n",
              "  653924.5,\n",
              "  649796.5,\n",
              "  664684.375,\n",
              "  644135.4375,\n",
              "  653584.25,\n",
              "  653154.1875,\n",
              "  653535.125,\n",
              "  659571.625,\n",
              "  661063.5,\n",
              "  648197.25,\n",
              "  653481.0,\n",
              "  652548.9375,\n",
              "  659904.625,\n",
              "  644462.375,\n",
              "  672770.8125,\n",
              "  648339.875,\n",
              "  644686.125,\n",
              "  649739.4375,\n",
              "  651550.6875,\n",
              "  645734.25,\n",
              "  636528.25,\n",
              "  631542.6875,\n",
              "  653412.9375,\n",
              "  651640.25,\n",
              "  670525.5,\n",
              "  652485.4375,\n",
              "  645256.3125,\n",
              "  643438.0625,\n",
              "  626454.0625,\n",
              "  627413.9375,\n",
              "  642124.8125,\n",
              "  651363.5,\n",
              "  629546.4375,\n",
              "  639410.125,\n",
              "  644997.0625,\n",
              "  652674.125,\n",
              "  644712.125,\n",
              "  646524.5,\n",
              "  616753.5,\n",
              "  647911.125,\n",
              "  645824.0,\n",
              "  635249.4375,\n",
              "  641455.625,\n",
              "  630625.875,\n",
              "  624492.3125,\n",
              "  618107.5,\n",
              "  645030.625,\n",
              "  625563.875,\n",
              "  631025.1875,\n",
              "  633525.1875,\n",
              "  647476.875,\n",
              "  645556.9375,\n",
              "  621242.9375,\n",
              "  646943.9375,\n",
              "  633424.75,\n",
              "  638730.1875,\n",
              "  626965.6875,\n",
              "  640864.6875,\n",
              "  621425.9375,\n",
              "  629848.3125,\n",
              "  642196.0625,\n",
              "  639007.6875,\n",
              "  624204.0625,\n",
              "  615902.3125,\n",
              "  612992.5,\n",
              "  624529.875,\n",
              "  644266.6875,\n",
              "  627107.3125,\n",
              "  645806.9375,\n",
              "  630295.9375,\n",
              "  622372.3125],\n",
              " 'mae': [960.0521850585938,\n",
              "  965.22265625,\n",
              "  971.9818725585938,\n",
              "  978.6756591796875,\n",
              "  955.7294311523438,\n",
              "  961.7481689453125,\n",
              "  970.24462890625,\n",
              "  968.39697265625,\n",
              "  969.7451782226562,\n",
              "  981.1522827148438,\n",
              "  962.2437744140625,\n",
              "  977.8728637695312,\n",
              "  960.49072265625,\n",
              "  940.1468505859375,\n",
              "  956.8775634765625,\n",
              "  952.921875,\n",
              "  958.6941528320312,\n",
              "  964.709716796875,\n",
              "  945.1375122070312,\n",
              "  964.4103393554688,\n",
              "  957.91796875,\n",
              "  942.552490234375,\n",
              "  951.496337890625,\n",
              "  942.8070068359375,\n",
              "  959.1427612304688,\n",
              "  951.4675903320312,\n",
              "  954.0720825195312,\n",
              "  937.9131469726562,\n",
              "  956.6258544921875,\n",
              "  949.57470703125,\n",
              "  949.3348999023438,\n",
              "  943.8743286132812,\n",
              "  938.3927612304688,\n",
              "  945.7854614257812,\n",
              "  938.2890014648438,\n",
              "  933.9874877929688,\n",
              "  939.2632446289062,\n",
              "  953.1804809570312,\n",
              "  940.1937255859375,\n",
              "  939.5057983398438,\n",
              "  945.478271484375,\n",
              "  943.1465454101562,\n",
              "  936.5689697265625,\n",
              "  945.2923583984375,\n",
              "  941.7169189453125,\n",
              "  936.5765991210938,\n",
              "  937.3582153320312,\n",
              "  944.2139892578125,\n",
              "  948.7947387695312,\n",
              "  941.65966796875,\n",
              "  933.1212158203125,\n",
              "  928.6483764648438,\n",
              "  921.80517578125,\n",
              "  930.3313598632812,\n",
              "  937.3568725585938,\n",
              "  936.2946166992188,\n",
              "  939.4119873046875,\n",
              "  938.6381225585938,\n",
              "  930.7819213867188,\n",
              "  916.7261962890625,\n",
              "  933.5789794921875,\n",
              "  932.049072265625,\n",
              "  925.9142456054688,\n",
              "  928.4432983398438,\n",
              "  937.8247680664062,\n",
              "  937.9087524414062,\n",
              "  931.2711181640625,\n",
              "  928.952392578125,\n",
              "  912.2015380859375,\n",
              "  921.8033447265625,\n",
              "  929.4114379882812,\n",
              "  930.4071044921875,\n",
              "  916.4891357421875,\n",
              "  920.9345703125,\n",
              "  924.58349609375,\n",
              "  929.4507446289062,\n",
              "  929.6376953125,\n",
              "  920.3192749023438,\n",
              "  920.6248779296875,\n",
              "  924.2072143554688,\n",
              "  906.8561401367188,\n",
              "  923.382568359375,\n",
              "  935.033935546875,\n",
              "  916.7529907226562,\n",
              "  903.0455322265625,\n",
              "  908.80029296875,\n",
              "  918.204833984375,\n",
              "  911.6407470703125,\n",
              "  927.5843505859375,\n",
              "  910.9173583984375,\n",
              "  916.9862060546875,\n",
              "  908.1511840820312,\n",
              "  911.8130493164062,\n",
              "  903.7931518554688,\n",
              "  909.9349365234375,\n",
              "  920.1952514648438,\n",
              "  920.6762084960938,\n",
              "  914.3438720703125,\n",
              "  907.0847778320312,\n",
              "  906.7614135742188,\n",
              "  904.8739013671875,\n",
              "  906.4933471679688,\n",
              "  898.2529296875,\n",
              "  912.3681030273438,\n",
              "  908.61474609375,\n",
              "  902.4444580078125,\n",
              "  903.7590942382812,\n",
              "  900.9849243164062,\n",
              "  917.4946899414062,\n",
              "  912.9063720703125,\n",
              "  906.3627319335938,\n",
              "  894.5176391601562,\n",
              "  900.5724487304688,\n",
              "  902.0767211914062,\n",
              "  910.0797119140625,\n",
              "  910.3895263671875,\n",
              "  895.6007690429688,\n",
              "  892.9622802734375,\n",
              "  895.8268432617188,\n",
              "  895.1008911132812,\n",
              "  905.4342651367188,\n",
              "  894.441162109375,\n",
              "  892.8961181640625,\n",
              "  904.9908447265625,\n",
              "  901.302490234375,\n",
              "  894.3568115234375,\n",
              "  875.317626953125,\n",
              "  883.2547607421875,\n",
              "  901.7979125976562,\n",
              "  889.874267578125,\n",
              "  893.0674438476562,\n",
              "  885.0396728515625,\n",
              "  886.44140625,\n",
              "  878.6445922851562,\n",
              "  889.5650024414062,\n",
              "  890.804931640625,\n",
              "  882.0663452148438,\n",
              "  886.7571411132812,\n",
              "  875.720947265625,\n",
              "  881.2512817382812,\n",
              "  891.8887939453125,\n",
              "  903.8504028320312,\n",
              "  869.8701171875,\n",
              "  880.8161010742188,\n",
              "  878.6400146484375,\n",
              "  885.5907592773438,\n",
              "  873.09814453125,\n",
              "  881.9683837890625,\n",
              "  874.5635986328125,\n",
              "  883.830322265625,\n",
              "  885.986083984375,\n",
              "  865.1424560546875,\n",
              "  867.4483642578125,\n",
              "  891.4724731445312,\n",
              "  876.8036499023438,\n",
              "  874.9539184570312,\n",
              "  878.495849609375,\n",
              "  867.5216674804688,\n",
              "  880.5611572265625,\n",
              "  869.152587890625,\n",
              "  849.9800415039062,\n",
              "  863.3675537109375,\n",
              "  867.1078491210938,\n",
              "  869.82421875,\n",
              "  884.5016479492188,\n",
              "  872.4285278320312,\n",
              "  870.8795166015625,\n",
              "  865.6449584960938,\n",
              "  877.5279541015625,\n",
              "  881.9497680664062,\n",
              "  871.8712158203125,\n",
              "  872.89453125,\n",
              "  867.9365844726562,\n",
              "  867.1395263671875,\n",
              "  866.3281860351562,\n",
              "  869.9627685546875,\n",
              "  859.9552612304688,\n",
              "  847.453125,\n",
              "  856.9046630859375,\n",
              "  877.956298828125,\n",
              "  857.1974487304688,\n",
              "  852.7162475585938,\n",
              "  858.4248657226562,\n",
              "  853.5870361328125,\n",
              "  863.56591796875,\n",
              "  851.0153198242188,\n",
              "  866.2380981445312,\n",
              "  849.8541259765625,\n",
              "  846.985107421875,\n",
              "  865.8428955078125,\n",
              "  858.5848999023438,\n",
              "  857.501708984375,\n",
              "  847.3736572265625,\n",
              "  855.7095336914062,\n",
              "  847.7888793945312,\n",
              "  842.1463012695312,\n",
              "  837.7993774414062,\n",
              "  846.1251220703125,\n",
              "  846.5928344726562,\n",
              "  856.6519165039062,\n",
              "  839.857177734375,\n",
              "  842.6633911132812,\n",
              "  834.142333984375,\n",
              "  839.0493774414062,\n",
              "  865.58251953125,\n",
              "  840.9381713867188,\n",
              "  844.5239868164062,\n",
              "  843.5197143554688,\n",
              "  835.1657104492188,\n",
              "  853.9547119140625,\n",
              "  846.0967407226562,\n",
              "  842.7098388671875,\n",
              "  836.6109619140625,\n",
              "  843.8438110351562,\n",
              "  851.6910400390625,\n",
              "  846.4449462890625,\n",
              "  851.6283569335938,\n",
              "  840.9207153320312,\n",
              "  847.5205078125,\n",
              "  850.8006591796875,\n",
              "  836.01416015625,\n",
              "  846.8905639648438,\n",
              "  827.7090454101562,\n",
              "  840.1477661132812,\n",
              "  842.9756469726562,\n",
              "  836.6941528320312,\n",
              "  845.1879272460938,\n",
              "  839.8353271484375,\n",
              "  825.7062377929688,\n",
              "  832.1551513671875,\n",
              "  851.9404296875,\n",
              "  838.5008544921875,\n",
              "  830.533447265625,\n",
              "  836.9752807617188,\n",
              "  836.6411743164062,\n",
              "  827.7958984375,\n",
              "  825.0157470703125,\n",
              "  837.0777587890625,\n",
              "  831.6734619140625,\n",
              "  827.1077270507812,\n",
              "  839.66455078125,\n",
              "  834.1442260742188,\n",
              "  834.6774291992188,\n",
              "  818.4979248046875,\n",
              "  814.627197265625,\n",
              "  836.2926635742188,\n",
              "  827.3978271484375,\n",
              "  837.2789306640625,\n",
              "  823.2272338867188,\n",
              "  835.0518798828125,\n",
              "  820.5162353515625,\n",
              "  821.674560546875,\n",
              "  813.359130859375,\n",
              "  817.1142578125,\n",
              "  815.241943359375,\n",
              "  823.7969360351562,\n",
              "  802.2184448242188,\n",
              "  820.2185668945312,\n",
              "  812.29736328125,\n",
              "  823.0789794921875,\n",
              "  829.7593994140625,\n",
              "  831.5569458007812,\n",
              "  812.3658447265625,\n",
              "  822.9486083984375,\n",
              "  808.6251831054688,\n",
              "  813.60595703125,\n",
              "  820.6015014648438,\n",
              "  825.2700805664062,\n",
              "  806.5601806640625,\n",
              "  809.5464477539062,\n",
              "  817.6651611328125,\n",
              "  813.1171264648438,\n",
              "  821.0465087890625,\n",
              "  804.404296875,\n",
              "  819.0756225585938,\n",
              "  795.7952880859375,\n",
              "  810.5919189453125,\n",
              "  813.3174438476562,\n",
              "  819.5944213867188,\n",
              "  813.7828979492188,\n",
              "  805.7713012695312,\n",
              "  796.0442504882812,\n",
              "  807.10693359375,\n",
              "  797.5304565429688,\n",
              "  806.2132568359375,\n",
              "  808.7171630859375,\n",
              "  810.2843627929688,\n",
              "  798.8622436523438,\n",
              "  817.4340209960938,\n",
              "  807.7741088867188,\n",
              "  798.8712768554688,\n",
              "  796.995849609375,\n",
              "  803.802001953125,\n",
              "  815.244384765625,\n",
              "  801.6327514648438,\n",
              "  795.4766235351562,\n",
              "  787.3472900390625,\n",
              "  811.6493530273438,\n",
              "  792.8269653320312,\n",
              "  801.4503173828125,\n",
              "  803.98193359375,\n",
              "  786.31689453125,\n",
              "  787.2598876953125,\n",
              "  802.9339599609375,\n",
              "  790.1196899414062,\n",
              "  797.4708251953125,\n",
              "  794.6417846679688,\n",
              "  795.3604125976562,\n",
              "  802.367919921875,\n",
              "  792.7993774414062,\n",
              "  797.5885620117188,\n",
              "  789.939208984375,\n",
              "  806.7603759765625,\n",
              "  801.5714111328125,\n",
              "  800.8809814453125,\n",
              "  790.8653564453125,\n",
              "  759.3935546875,\n",
              "  795.6969604492188,\n",
              "  795.2373657226562,\n",
              "  798.1098022460938,\n",
              "  795.1992797851562,\n",
              "  774.7904052734375,\n",
              "  772.3314208984375,\n",
              "  781.1900634765625,\n",
              "  782.8111572265625,\n",
              "  794.7030029296875,\n",
              "  786.4103393554688,\n",
              "  784.0777587890625,\n",
              "  781.8603515625,\n",
              "  801.3028564453125,\n",
              "  782.2060546875,\n",
              "  793.3417358398438,\n",
              "  775.1546630859375,\n",
              "  775.6941528320312,\n",
              "  779.7689208984375,\n",
              "  788.9734497070312,\n",
              "  773.5631103515625,\n",
              "  775.25,\n",
              "  784.429931640625,\n",
              "  786.601806640625,\n",
              "  773.9220581054688,\n",
              "  783.4151611328125,\n",
              "  778.9558715820312,\n",
              "  782.4140014648438,\n",
              "  781.5088500976562,\n",
              "  783.1262817382812,\n",
              "  769.3803100585938,\n",
              "  771.4757080078125,\n",
              "  769.3728637695312,\n",
              "  776.2980346679688,\n",
              "  771.023681640625,\n",
              "  769.5318603515625,\n",
              "  783.0362548828125,\n",
              "  771.6170043945312,\n",
              "  769.6611328125,\n",
              "  776.4279174804688,\n",
              "  771.7476196289062,\n",
              "  785.4373168945312,\n",
              "  772.0027465820312,\n",
              "  774.7645874023438,\n",
              "  779.8565673828125,\n",
              "  786.6761474609375,\n",
              "  776.4242553710938,\n",
              "  759.974853515625,\n",
              "  756.9354858398438,\n",
              "  775.739013671875,\n",
              "  768.864013671875,\n",
              "  758.820556640625,\n",
              "  757.6178588867188,\n",
              "  763.7192993164062,\n",
              "  773.5341186523438,\n",
              "  767.5382080078125,\n",
              "  772.0620727539062,\n",
              "  763.340576171875,\n",
              "  769.29638671875,\n",
              "  768.3024291992188,\n",
              "  759.962890625,\n",
              "  759.36279296875,\n",
              "  767.046142578125,\n",
              "  755.734619140625,\n",
              "  775.5140991210938,\n",
              "  755.62255859375,\n",
              "  767.9005126953125,\n",
              "  756.8304443359375,\n",
              "  758.4180908203125,\n",
              "  770.4664916992188,\n",
              "  764.6541137695312,\n",
              "  761.5983276367188,\n",
              "  768.8557739257812,\n",
              "  770.257568359375,\n",
              "  763.9798583984375,\n",
              "  775.53955078125,\n",
              "  749.4000244140625,\n",
              "  762.0213623046875,\n",
              "  751.3090209960938,\n",
              "  760.535888671875,\n",
              "  756.4755859375,\n",
              "  752.1165161132812,\n",
              "  738.1693725585938,\n",
              "  754.6629028320312,\n",
              "  754.3379516601562,\n",
              "  749.3364868164062,\n",
              "  743.4659423828125,\n",
              "  759.9461669921875,\n",
              "  741.8764038085938,\n",
              "  764.6934814453125,\n",
              "  751.2670288085938,\n",
              "  751.9547119140625,\n",
              "  754.0639038085938,\n",
              "  742.9703369140625,\n",
              "  747.3394775390625,\n",
              "  757.0137939453125,\n",
              "  746.5835571289062,\n",
              "  744.7122802734375,\n",
              "  762.1848754882812,\n",
              "  748.5413208007812,\n",
              "  742.2172241210938,\n",
              "  754.0654907226562,\n",
              "  735.674560546875,\n",
              "  749.1781005859375,\n",
              "  749.09716796875,\n",
              "  737.9405517578125,\n",
              "  743.5396728515625,\n",
              "  742.2437133789062,\n",
              "  744.9447631835938,\n",
              "  750.6863403320312,\n",
              "  740.4605712890625,\n",
              "  739.6748046875,\n",
              "  738.4901123046875,\n",
              "  757.19873046875,\n",
              "  734.9234619140625,\n",
              "  731.6907958984375,\n",
              "  744.3351440429688,\n",
              "  744.3677368164062,\n",
              "  739.1104736328125,\n",
              "  734.5235595703125,\n",
              "  724.0614013671875,\n",
              "  728.0645141601562,\n",
              "  734.7968139648438,\n",
              "  735.3133544921875,\n",
              "  730.6422119140625,\n",
              "  739.96435546875,\n",
              "  736.2576293945312,\n",
              "  739.1039428710938,\n",
              "  748.0000610351562,\n",
              "  728.340576171875,\n",
              "  744.1468505859375,\n",
              "  737.4631958007812,\n",
              "  742.9868774414062,\n",
              "  730.9114990234375,\n",
              "  739.4453125,\n",
              "  741.5306396484375,\n",
              "  732.5557861328125,\n",
              "  734.6661987304688,\n",
              "  727.6484375,\n",
              "  724.5567016601562,\n",
              "  727.84765625,\n",
              "  728.361083984375,\n",
              "  720.8618774414062,\n",
              "  723.8667602539062,\n",
              "  718.1826782226562,\n",
              "  727.1210327148438,\n",
              "  744.720947265625,\n",
              "  718.3366088867188,\n",
              "  726.9781494140625,\n",
              "  725.8302612304688,\n",
              "  723.9386596679688,\n",
              "  735.0870971679688,\n",
              "  715.1256713867188,\n",
              "  718.4528198242188,\n",
              "  737.7066040039062,\n",
              "  717.8714599609375,\n",
              "  728.2940063476562,\n",
              "  725.5130615234375,\n",
              "  718.6275024414062,\n",
              "  706.9763793945312,\n",
              "  718.3751831054688,\n",
              "  718.9324340820312,\n",
              "  720.9302978515625,\n",
              "  709.2301635742188,\n",
              "  723.5220336914062,\n",
              "  720.3651733398438,\n",
              "  721.2745971679688,\n",
              "  718.5562133789062,\n",
              "  712.7645263671875,\n",
              "  709.7803955078125,\n",
              "  720.373046875,\n",
              "  716.0371704101562,\n",
              "  716.3667602539062,\n",
              "  719.204833984375,\n",
              "  698.792236328125,\n",
              "  711.0595703125,\n",
              "  720.404052734375,\n",
              "  702.6470336914062,\n",
              "  727.8469848632812,\n",
              "  715.7689819335938,\n",
              "  723.9535522460938,\n",
              "  703.4393920898438,\n",
              "  716.0029907226562,\n",
              "  708.6776123046875,\n",
              "  719.3963623046875,\n",
              "  709.61962890625,\n",
              "  708.966552734375,\n",
              "  715.7827758789062,\n",
              "  703.9239501953125,\n",
              "  706.180419921875,\n",
              "  706.2686767578125,\n",
              "  712.984130859375,\n",
              "  723.4166870117188,\n",
              "  715.1581420898438,\n",
              "  704.3973388671875,\n",
              "  713.544677734375,\n",
              "  701.6864624023438,\n",
              "  718.4346923828125,\n",
              "  711.61962890625,\n",
              "  711.9183349609375,\n",
              "  702.9988403320312,\n",
              "  701.5894775390625,\n",
              "  713.2509765625,\n",
              "  699.5726318359375,\n",
              "  702.2981567382812,\n",
              "  699.3847045898438,\n",
              "  706.9801635742188,\n",
              "  719.7289428710938,\n",
              "  702.7861938476562,\n",
              "  684.2439575195312,\n",
              "  700.5400390625,\n",
              "  707.7860107421875,\n",
              "  689.92041015625,\n",
              "  687.43408203125,\n",
              "  705.0918579101562,\n",
              "  692.812255859375,\n",
              "  699.6189575195312,\n",
              "  691.6614990234375,\n",
              "  690.678466796875,\n",
              "  704.1506958007812,\n",
              "  704.8934326171875,\n",
              "  698.1752319335938,\n",
              "  693.590576171875,\n",
              "  696.064453125,\n",
              "  696.4208984375,\n",
              "  704.1448364257812,\n",
              "  694.1521606445312,\n",
              "  690.8958740234375,\n",
              "  703.5965576171875,\n",
              "  697.6680908203125,\n",
              "  699.74951171875,\n",
              "  696.3362426757812,\n",
              "  690.9966430664062,\n",
              "  698.004638671875,\n",
              "  686.6318359375,\n",
              "  695.6590576171875,\n",
              "  702.3671264648438,\n",
              "  691.1993408203125,\n",
              "  669.7586059570312,\n",
              "  699.7747802734375,\n",
              "  690.1642456054688,\n",
              "  689.4861450195312,\n",
              "  698.310791015625,\n",
              "  691.7499389648438,\n",
              "  690.171142578125,\n",
              "  678.6378784179688,\n",
              "  690.4906005859375,\n",
              "  697.6920166015625,\n",
              "  697.4893798828125,\n",
              "  691.6658935546875,\n",
              "  671.5025634765625,\n",
              "  691.8216552734375,\n",
              "  699.7598266601562,\n",
              "  670.7819213867188,\n",
              "  687.4456787109375,\n",
              "  674.4598999023438,\n",
              "  678.6089477539062,\n",
              "  672.5227661132812,\n",
              "  689.4769287109375,\n",
              "  689.9505004882812,\n",
              "  678.8776245117188,\n",
              "  690.7332153320312,\n",
              "  673.42236328125,\n",
              "  688.6995239257812,\n",
              "  674.4968872070312,\n",
              "  676.70849609375,\n",
              "  694.3162841796875,\n",
              "  692.4630737304688,\n",
              "  675.196533203125,\n",
              "  689.5098266601562,\n",
              "  697.5812377929688,\n",
              "  680.8052368164062,\n",
              "  692.0167236328125,\n",
              "  704.4247436523438,\n",
              "  662.8220825195312,\n",
              "  670.57861328125,\n",
              "  689.0345458984375,\n",
              "  689.7686157226562,\n",
              "  687.6594848632812,\n",
              "  669.5719604492188,\n",
              "  689.266357421875,\n",
              "  682.4185180664062,\n",
              "  674.8151245117188,\n",
              "  681.099365234375,\n",
              "  680.929931640625,\n",
              "  669.767578125,\n",
              "  677.7920532226562,\n",
              "  667.22314453125,\n",
              "  681.2778930664062,\n",
              "  667.403076171875,\n",
              "  670.8842163085938,\n",
              "  677.4190673828125,\n",
              "  674.4773559570312,\n",
              "  668.6317749023438,\n",
              "  664.8011474609375,\n",
              "  674.2105712890625,\n",
              "  688.60009765625,\n",
              "  671.5180053710938,\n",
              "  673.0211791992188,\n",
              "  667.4972534179688,\n",
              "  672.3108520507812,\n",
              "  661.8683471679688,\n",
              "  673.4523315429688,\n",
              "  672.6236572265625,\n",
              "  676.6123657226562,\n",
              "  670.02587890625,\n",
              "  659.7835083007812,\n",
              "  670.0787353515625,\n",
              "  653.7606201171875,\n",
              "  679.837646484375,\n",
              "  659.7274169921875,\n",
              "  672.3209228515625,\n",
              "  669.2557373046875,\n",
              "  674.47412109375,\n",
              "  657.3925170898438,\n",
              "  669.1351928710938,\n",
              "  680.1133422851562,\n",
              "  663.4758911132812,\n",
              "  657.4703979492188,\n",
              "  668.3006591796875,\n",
              "  668.733642578125,\n",
              "  670.1489868164062,\n",
              "  672.8942260742188,\n",
              "  669.0607299804688,\n",
              "  666.0726318359375,\n",
              "  660.6902465820312,\n",
              "  659.5536499023438,\n",
              "  660.0021362304688,\n",
              "  673.3746948242188,\n",
              "  675.7216796875,\n",
              "  648.8303833007812,\n",
              "  678.7454223632812,\n",
              "  661.9225463867188,\n",
              "  662.4777221679688,\n",
              "  654.5370483398438,\n",
              "  669.0968017578125,\n",
              "  660.1826782226562,\n",
              "  672.4907836914062,\n",
              "  663.4691772460938,\n",
              "  662.245361328125,\n",
              "  669.391357421875,\n",
              "  669.3828735351562,\n",
              "  650.8986206054688,\n",
              "  652.6634521484375,\n",
              "  646.7337646484375,\n",
              "  658.6569213867188,\n",
              "  653.9300537109375,\n",
              "  657.4371948242188,\n",
              "  654.2246704101562,\n",
              "  661.17822265625,\n",
              "  660.6681518554688,\n",
              "  648.4420166015625,\n",
              "  653.1756591796875,\n",
              "  659.271728515625,\n",
              "  661.2818603515625,\n",
              "  657.7313842773438,\n",
              "  652.0439453125,\n",
              "  652.9747924804688,\n",
              "  653.4723510742188,\n",
              "  644.8104858398438,\n",
              "  654.1683959960938,\n",
              "  663.3107299804688,\n",
              "  652.11376953125,\n",
              "  664.6071166992188,\n",
              "  656.3271484375,\n",
              "  655.0341186523438,\n",
              "  646.431640625,\n",
              "  654.9530639648438,\n",
              "  651.1054077148438,\n",
              "  652.8298950195312,\n",
              "  659.202880859375,\n",
              "  647.8861694335938,\n",
              "  656.339111328125,\n",
              "  665.310302734375,\n",
              "  648.7105102539062,\n",
              "  659.0911865234375,\n",
              "  652.82275390625,\n",
              "  652.1678466796875,\n",
              "  647.7626342773438,\n",
              "  651.2236328125,\n",
              "  651.281982421875,\n",
              "  654.1031494140625,\n",
              "  657.226806640625,\n",
              "  653.2908325195312,\n",
              "  642.11572265625,\n",
              "  662.3396606445312,\n",
              "  645.9528198242188,\n",
              "  656.8878173828125,\n",
              "  654.8716430664062,\n",
              "  659.4863891601562,\n",
              "  655.8515014648438,\n",
              "  642.1528930664062,\n",
              "  636.55859375,\n",
              "  645.5471801757812,\n",
              "  625.2142333984375,\n",
              "  646.3161010742188,\n",
              "  649.3990478515625,\n",
              "  646.2522583007812,\n",
              "  640.230712890625,\n",
              "  653.9353637695312,\n",
              "  648.4933471679688,\n",
              "  649.347412109375,\n",
              "  637.4374389648438,\n",
              "  656.4558715820312,\n",
              "  642.4429321289062,\n",
              "  642.2042846679688,\n",
              "  645.7935791015625,\n",
              "  646.3934326171875,\n",
              "  656.0048217773438,\n",
              "  644.4239501953125,\n",
              "  641.6710815429688,\n",
              "  633.4346923828125,\n",
              "  628.4598999023438,\n",
              "  649.8038940429688,\n",
              "  642.69140625,\n",
              "  648.3970336914062,\n",
              "  647.4390869140625,\n",
              "  657.0012817382812,\n",
              "  639.607666015625,\n",
              "  651.349853515625,\n",
              "  641.7709350585938,\n",
              "  645.1578979492188,\n",
              "  639.208740234375,\n",
              "  637.8829345703125,\n",
              "  647.3828735351562,\n",
              "  658.1106567382812,\n",
              "  649.562744140625,\n",
              "  634.6348266601562,\n",
              "  644.5241088867188,\n",
              "  632.44189453125,\n",
              "  641.387451171875,\n",
              "  644.002197265625,\n",
              "  644.6347045898438,\n",
              "  646.7640380859375,\n",
              "  645.3696899414062,\n",
              "  647.2991333007812,\n",
              "  646.5219116210938,\n",
              "  626.0149536132812,\n",
              "  629.0833129882812,\n",
              "  636.0253295898438,\n",
              "  624.94873046875,\n",
              "  639.919677734375,\n",
              "  641.9400634765625,\n",
              "  642.2052001953125,\n",
              "  636.7429809570312,\n",
              "  630.510986328125,\n",
              "  639.4596557617188,\n",
              "  646.1860961914062,\n",
              "  643.9283447265625,\n",
              "  637.3062744140625,\n",
              "  628.3388061523438,\n",
              "  630.6053466796875,\n",
              "  646.4290161132812,\n",
              "  659.1929321289062,\n",
              "  641.425048828125,\n",
              "  629.6309814453125,\n",
              "  635.5505981445312,\n",
              "  639.1224365234375,\n",
              "  631.580322265625,\n",
              "  640.50244140625,\n",
              "  635.9324340820312,\n",
              "  651.36083984375,\n",
              "  632.8743286132812,\n",
              "  628.015380859375,\n",
              "  638.4253540039062,\n",
              "  630.635986328125,\n",
              "  626.5068359375,\n",
              "  633.9219970703125,\n",
              "  638.8582153320312,\n",
              "  636.126220703125,\n",
              "  638.8330078125,\n",
              "  634.3736572265625,\n",
              "  634.84765625,\n",
              "  636.0173950195312,\n",
              "  637.12109375,\n",
              "  630.467041015625,\n",
              "  640.9353637695312,\n",
              "  637.2144775390625,\n",
              "  637.0925903320312,\n",
              "  630.4065551757812,\n",
              "  629.6968994140625,\n",
              "  643.0701904296875,\n",
              "  635.4949951171875,\n",
              "  637.4983520507812,\n",
              "  625.0692138671875,\n",
              "  647.0030517578125,\n",
              "  640.7957153320312,\n",
              "  631.6185302734375,\n",
              "  636.86962890625,\n",
              "  629.9005126953125,\n",
              "  621.6550903320312,\n",
              "  624.5174560546875,\n",
              "  629.9363403320312,\n",
              "  635.2123413085938,\n",
              "  629.3388061523438,\n",
              "  644.1973876953125,\n",
              "  633.619873046875,\n",
              "  623.2138671875,\n",
              "  630.5662841796875,\n",
              "  633.9967651367188,\n",
              "  641.6543579101562,\n",
              "  641.2095947265625,\n",
              "  638.8317260742188,\n",
              "  629.7661743164062,\n",
              "  628.135986328125,\n",
              "  635.9349975585938,\n",
              "  626.7630004882812,\n",
              "  629.6372680664062,\n",
              "  630.2252807617188,\n",
              "  639.4685668945312,\n",
              "  632.968994140625,\n",
              "  634.36572265625,\n",
              "  633.8411254882812,\n",
              "  632.7393188476562,\n",
              "  637.6536254882812,\n",
              "  629.7353515625,\n",
              "  626.8838500976562,\n",
              "  623.7416381835938,\n",
              "  639.23974609375,\n",
              "  634.91064453125,\n",
              "  631.0740356445312,\n",
              "  631.04736328125,\n",
              "  624.9893188476562,\n",
              "  631.5018920898438,\n",
              "  635.8490600585938,\n",
              "  623.3320922851562,\n",
              "  627.5167236328125,\n",
              "  627.8030395507812,\n",
              "  631.1049194335938,\n",
              "  629.2811889648438,\n",
              "  635.5857543945312,\n",
              "  634.203369140625,\n",
              "  629.0177001953125,\n",
              "  635.8236694335938,\n",
              "  632.2465209960938,\n",
              "  626.2881469726562,\n",
              "  626.7232055664062,\n",
              "  619.5753784179688,\n",
              "  624.4563598632812,\n",
              "  620.661865234375,\n",
              "  623.2543334960938,\n",
              "  627.3125610351562,\n",
              "  630.406494140625,\n",
              "  623.09326171875,\n",
              "  630.7741088867188,\n",
              "  633.4528198242188,\n",
              "  635.5144653320312,\n",
              "  631.26220703125,\n",
              "  632.5958862304688,\n",
              "  636.845458984375,\n",
              "  632.6871948242188,\n",
              "  632.1236572265625,\n",
              "  630.5317993164062,\n",
              "  621.7547607421875,\n",
              "  625.0482788085938,\n",
              "  627.4979248046875,\n",
              "  623.2930908203125,\n",
              "  625.7666015625,\n",
              "  623.936279296875,\n",
              "  630.24072265625,\n",
              "  608.5759887695312,\n",
              "  616.4163818359375,\n",
              "  622.6209106445312,\n",
              "  622.3258666992188,\n",
              "  615.1981201171875,\n",
              "  631.146728515625,\n",
              "  615.2744140625,\n",
              "  617.4593505859375,\n",
              "  625.29931640625,\n",
              "  621.7009887695312,\n",
              "  623.85986328125,\n",
              "  621.6498413085938,\n",
              "  634.3637084960938,\n",
              "  626.3291625976562,\n",
              "  618.55078125,\n",
              "  618.8416748046875,\n",
              "  625.3947143554688,\n",
              "  623.6395874023438,\n",
              "  624.39990234375,\n",
              "  628.031982421875,\n",
              "  626.361572265625,\n",
              "  626.6849365234375,\n",
              "  625.0179443359375,\n",
              "  617.9423217773438,\n",
              "  628.8707885742188,\n",
              "  630.616455078125,\n",
              "  618.8963623046875,\n",
              "  618.6292724609375,\n",
              "  618.4732666015625,\n",
              "  619.7218627929688,\n",
              "  632.0015869140625,\n",
              "  629.4725952148438,\n",
              "  627.9306640625,\n",
              "  621.4915161132812,\n",
              "  607.4861450195312,\n",
              "  631.93310546875,\n",
              "  626.6651611328125,\n",
              "  624.85400390625,\n",
              "  630.87841796875,\n",
              "  627.9744873046875,\n",
              "  627.1363525390625,\n",
              "  627.343017578125,\n",
              "  620.0225219726562,\n",
              "  622.8923950195312,\n",
              "  622.9214477539062,\n",
              "  612.1928100585938,\n",
              "  630.9998168945312,\n",
              "  623.7508544921875,\n",
              "  614.4061279296875,\n",
              "  631.14697265625,\n",
              "  617.0502319335938,\n",
              "  614.1162719726562,\n",
              "  625.287109375,\n",
              "  619.0967407226562,\n",
              "  616.8297729492188,\n",
              "  628.93505859375,\n",
              "  620.8490600585938,\n",
              "  621.103271484375,\n",
              "  619.202880859375,\n",
              "  623.7175903320312,\n",
              "  624.97265625,\n",
              "  625.65087890625,\n",
              "  623.2854614257812,\n",
              "  624.4826049804688,\n",
              "  621.30908203125,\n",
              "  627.0186157226562,\n",
              "  621.237548828125,\n",
              "  636.9278564453125,\n",
              "  625.6271362304688,\n",
              "  619.1110229492188,\n",
              "  626.04248046875,\n",
              "  621.4334106445312,\n",
              "  624.064453125,\n",
              "  618.1181640625,\n",
              "  615.6439208984375,\n",
              "  624.5252075195312,\n",
              "  625.0244750976562,\n",
              "  637.207275390625,\n",
              "  623.3546142578125,\n",
              "  623.3787231445312,\n",
              "  619.34716796875,\n",
              "  615.1742553710938,\n",
              "  612.452880859375,\n",
              "  618.8936157226562,\n",
              "  628.3600463867188,\n",
              "  613.7526245117188,\n",
              "  617.0845336914062,\n",
              "  622.5652465820312,\n",
              "  628.5337524414062,\n",
              "  624.05078125,\n",
              "  624.928955078125,\n",
              "  613.1212158203125,\n",
              "  628.7303466796875,\n",
              "  623.1541748046875,\n",
              "  618.3384399414062,\n",
              "  621.4533081054688,\n",
              "  614.273193359375,\n",
              "  613.3883056640625,\n",
              "  607.23388671875,\n",
              "  625.21923828125,\n",
              "  615.2347412109375,\n",
              "  618.3421630859375,\n",
              "  618.6448364257812,\n",
              "  627.9995727539062,\n",
              "  628.3174438476562,\n",
              "  612.7805786132812,\n",
              "  628.1749267578125,\n",
              "  619.180908203125,\n",
              "  622.8522338867188,\n",
              "  617.3289184570312,\n",
              "  625.021728515625,\n",
              "  613.2955932617188,\n",
              "  618.1868896484375,\n",
              "  626.2965698242188,\n",
              "  623.8905029296875,\n",
              "  611.931884765625,\n",
              "  614.7623901367188,\n",
              "  616.8845825195312,\n",
              "  618.6754760742188,\n",
              "  627.5591430664062,\n",
              "  618.4545288085938,\n",
              "  632.18896484375,\n",
              "  619.21533203125,\n",
              "  612.3853149414062],\n",
              " 'val_loss': [543064.9375,\n",
              "  538173.25,\n",
              "  532856.3125,\n",
              "  515093.0,\n",
              "  540118.9375,\n",
              "  517112.21875,\n",
              "  533773.5625,\n",
              "  512237.34375,\n",
              "  515651.5,\n",
              "  540976.8125,\n",
              "  527454.75,\n",
              "  505332.09375,\n",
              "  534145.6875,\n",
              "  516582.25,\n",
              "  520032.25,\n",
              "  527970.8125,\n",
              "  535753.4375,\n",
              "  539052.8125,\n",
              "  525756.25,\n",
              "  520931.03125,\n",
              "  532706.4375,\n",
              "  489154.5,\n",
              "  526919.625,\n",
              "  501082.84375,\n",
              "  508857.90625,\n",
              "  516311.21875,\n",
              "  499002.34375,\n",
              "  489843.0,\n",
              "  510074.28125,\n",
              "  517778.5,\n",
              "  513137.75,\n",
              "  479009.34375,\n",
              "  507320.84375,\n",
              "  527475.9375,\n",
              "  505897.40625,\n",
              "  504617.03125,\n",
              "  504557.125,\n",
              "  499813.125,\n",
              "  523830.34375,\n",
              "  502481.40625,\n",
              "  530782.3125,\n",
              "  497013.75,\n",
              "  508677.15625,\n",
              "  503957.78125,\n",
              "  506664.15625,\n",
              "  494371.125,\n",
              "  501872.34375,\n",
              "  484719.25,\n",
              "  504551.5,\n",
              "  495616.59375,\n",
              "  478723.75,\n",
              "  486231.65625,\n",
              "  509904.84375,\n",
              "  484898.375,\n",
              "  488056.21875,\n",
              "  495538.84375,\n",
              "  478750.125,\n",
              "  482042.0,\n",
              "  505758.5,\n",
              "  504962.71875,\n",
              "  508349.625,\n",
              "  491568.28125,\n",
              "  482843.28125,\n",
              "  494201.875,\n",
              "  464772.84375,\n",
              "  472782.03125,\n",
              "  484135.5,\n",
              "  471471.5,\n",
              "  466891.875,\n",
              "  486110.375,\n",
              "  485425.65625,\n",
              "  500634.15625,\n",
              "  492081.875,\n",
              "  478851.40625,\n",
              "  474867.90625,\n",
              "  474243.0,\n",
              "  493311.625,\n",
              "  492665.65625,\n",
              "  464403.875,\n",
              "  483466.40625,\n",
              "  494578.0,\n",
              "  482153.0,\n",
              "  481422.34375,\n",
              "  484703.125,\n",
              "  480133.40625,\n",
              "  471631.59375,\n",
              "  486608.09375,\n",
              "  470365.40625,\n",
              "  477476.125,\n",
              "  484601.34375,\n",
              "  483913.90625,\n",
              "  479450.34375,\n",
              "  463209.59375,\n",
              "  466496.0,\n",
              "  465821.375,\n",
              "  476789.90625,\n",
              "  449071.71875,\n",
              "  467712.5,\n",
              "  478597.09375,\n",
              "  489472.75,\n",
              "  465795.375,\n",
              "  476683.625,\n",
              "  460746.90625,\n",
              "  467704.90625,\n",
              "  459487.875,\n",
              "  466432.78125,\n",
              "  450474.40625,\n",
              "  453707.53125,\n",
              "  460776.84375,\n",
              "  456297.125,\n",
              "  467061.625,\n",
              "  443648.84375,\n",
              "  480831.125,\n",
              "  437964.5,\n",
              "  464517.75,\n",
              "  448709.875,\n",
              "  459401.375,\n",
              "  462622.0,\n",
              "  435587.5,\n",
              "  461336.25,\n",
              "  468139.40625,\n",
              "  448846.625,\n",
              "  455629.65625,\n",
              "  447621.65625,\n",
              "  447010.625,\n",
              "  439014.40625,\n",
              "  430802.09375,\n",
              "  445190.25,\n",
              "  451898.125,\n",
              "  443966.5,\n",
              "  450682.125,\n",
              "  438954.25,\n",
              "  431062.53125,\n",
              "  437754.59375,\n",
              "  437138.34375,\n",
              "  458640.25,\n",
              "  458008.78125,\n",
              "  453617.90625,\n",
              "  449550.09375,\n",
              "  434165.125,\n",
              "  448328.09375,\n",
              "  432950.75,\n",
              "  447089.90625,\n",
              "  431764.96875,\n",
              "  445857.625,\n",
              "  452412.21875,\n",
              "  426626.0,\n",
              "  458317.34375,\n",
              "  439697.03125,\n",
              "  439086.625,\n",
              "  442232.28125,\n",
              "  423698.71875,\n",
              "  448096.40625,\n",
              "  440415.40625,\n",
              "  425275.59375,\n",
              "  428422.90625,\n",
              "  434865.09375,\n",
              "  445012.84375,\n",
              "  401468.25,\n",
              "  426081.46875,\n",
              "  407344.84375,\n",
              "  435623.125,\n",
              "  420636.5,\n",
              "  423768.625,\n",
              "  405090.625,\n",
              "  418880.65625,\n",
              "  415091.875,\n",
              "  428352.40625,\n",
              "  431467.34375,\n",
              "  434052.03125,\n",
              "  444002.71875,\n",
              "  432845.15625,\n",
              "  393754.25,\n",
              "  414288.84375,\n",
              "  417391.875,\n",
              "  423656.40625,\n",
              "  416238.46875,\n",
              "  422496.21875,\n",
              "  421929.78125,\n",
              "  428117.96875,\n",
              "  403548.40625,\n",
              "  429999.46875,\n",
              "  426327.65625,\n",
              "  419033.34375,\n",
              "  425174.09375,\n",
              "  411180.40625,\n",
              "  434360.84375,\n",
              "  399707.53125,\n",
              "  408929.5,\n",
              "  408934.375,\n",
              "  404753.25,\n",
              "  393893.15625,\n",
              "  393363.34375,\n",
              "  406710.875,\n",
              "  422996.71875,\n",
              "  418786.5,\n",
              "  401415.78125,\n",
              "  384099.53125,\n",
              "  414161.03125,\n",
              "  420163.71875,\n",
              "  419591.90625,\n",
              "  415393.0,\n",
              "  381488.0,\n",
              "  414265.125,\n",
              "  393440.78125,\n",
              "  376324.625,\n",
              "  402456.03125,\n",
              "  401901.90625,\n",
              "  397921.96875,\n",
              "  404404.875,\n",
              "  387369.5,\n",
              "  393303.25,\n",
              "  382753.5,\n",
              "  389383.75,\n",
              "  388276.53125,\n",
              "  381137.5,\n",
              "  386981.96875,\n",
              "  393592.15625,\n",
              "  393067.90625,\n",
              "  398843.375,\n",
              "  395581.03125,\n",
              "  391449.84375,\n",
              "  403504.09375,\n",
              "  400244.09375,\n",
              "  396164.71875,\n",
              "  395589.625,\n",
              "  395054.375,\n",
              "  398052.78125,\n",
              "  387737.90625,\n",
              "  393450.34375,\n",
              "  399138.65625,\n",
              "  398563.59375,\n",
              "  382092.15625,\n",
              "  378033.78125,\n",
              "  384585.84375,\n",
              "  367336.53125,\n",
              "  386141.28125,\n",
              "  395318.03125,\n",
              "  398329.875,\n",
              "  375865.59375,\n",
              "  381488.5,\n",
              "  371344.96875,\n",
              "  376944.96875,\n",
              "  383426.46875,\n",
              "  373363.375,\n",
              "  372870.59375,\n",
              "  371762.71875,\n",
              "  387392.78125,\n",
              "  383414.90625,\n",
              "  382886.21875,\n",
              "  382323.25,\n",
              "  385349.34375,\n",
              "  362424.25,\n",
              "  378293.25,\n",
              "  383796.75,\n",
              "  366927.78125,\n",
              "  376820.84375,\n",
              "  369418.90625,\n",
              "  372351.78125,\n",
              "  381155.25,\n",
              "  368944.40625,\n",
              "  370319.96875,\n",
              "  376192.65625,\n",
              "  357067.96875,\n",
              "  384499.03125,\n",
              "  365374.375,\n",
              "  364923.40625,\n",
              "  358643.09375,\n",
              "  369773.59375,\n",
              "  362855.5,\n",
              "  360597.5,\n",
              "  375140.65625,\n",
              "  365412.375,\n",
              "  368354.15625,\n",
              "  366774.90625,\n",
              "  375494.125,\n",
              "  363473.875,\n",
              "  377813.09375,\n",
              "  359127.90625,\n",
              "  373328.53125,\n",
              "  363814.03125,\n",
              "  372460.46875,\n",
              "  362791.65625,\n",
              "  356735.90625,\n",
              "  365219.40625,\n",
              "  355793.71875,\n",
              "  364187.5,\n",
              "  367139.59375,\n",
              "  363200.90625,\n",
              "  344840.0625,\n",
              "  351205.34375,\n",
              "  352837.34375,\n",
              "  352413.84375,\n",
              "  346441.34375,\n",
              "  348761.65625,\n",
              "  356535.625,\n",
              "  364864.75,\n",
              "  362329.34375,\n",
              "  358542.25,\n",
              "  357995.15625,\n",
              "  362892.46875,\n",
              "  360344.40625,\n",
              "  361926.125,\n",
              "  356095.90625,\n",
              "  350193.21875,\n",
              "  349739.34375,\n",
              "  342581.90625,\n",
              "  345469.0625,\n",
              "  339605.5,\n",
              "  332491.65625,\n",
              "  340757.0625,\n",
              "  340253.28125,\n",
              "  334495.15625,\n",
              "  359459.78125,\n",
              "  364839.46875,\n",
              "  339880.75,\n",
              "  342711.375,\n",
              "  344172.125,\n",
              "  354240.96875,\n",
              "  348678.34375,\n",
              "  353298.0,\n",
              "  356269.65625,\n",
              "  335356.0,\n",
              "  338274.28125,\n",
              "  346349.21875,\n",
              "  325654.0625,\n",
              "  336960.03125,\n",
              "  344935.09375,\n",
              "  330966.96875,\n",
              "  344089.28125,\n",
              "  343564.78125,\n",
              "  343181.21875,\n",
              "  334252.375,\n",
              "  333939.46875,\n",
              "  336779.84375,\n",
              "  341310.34375,\n",
              "  332498.34375,\n",
              "  345565.375,\n",
              "  340097.21875,\n",
              "  337781.84375,\n",
              "  325779.9375,\n",
              "  318849.40625,\n",
              "  343349.53125,\n",
              "  337744.0,\n",
              "  329119.59375,\n",
              "  336924.3125,\n",
              "  328267.21875,\n",
              "  336131.53125,\n",
              "  330734.40625,\n",
              "  328525.90625,\n",
              "  342949.84375,\n",
              "  323018.75,\n",
              "  332209.40625,\n",
              "  333397.4375,\n",
              "  345915.90625,\n",
              "  327638.46875,\n",
              "  328886.46875,\n",
              "  326880.0,\n",
              "  323329.625,\n",
              "  329258.03125,\n",
              "  325701.9375,\n",
              "  325129.9375,\n",
              "  329452.71875,\n",
              "  313170.65625,\n",
              "  320759.46875,\n",
              "  317239.75,\n",
              "  315252.75,\n",
              "  311665.34375,\n",
              "  322241.6875,\n",
              "  331229.15625,\n",
              "  338793.84375,\n",
              "  317835.09375,\n",
              "  325389.90625,\n",
              "  321725.78125,\n",
              "  327811.875,\n",
              "  324062.875,\n",
              "  331466.375,\n",
              "  326398.53125,\n",
              "  315095.21875,\n",
              "  317849.59375,\n",
              "  321987.34375,\n",
              "  321673.8125,\n",
              "  316662.4375,\n",
              "  325365.84375,\n",
              "  325028.46875,\n",
              "  319942.21875,\n",
              "  311849.6875,\n",
              "  322264.40625,\n",
              "  311057.5,\n",
              "  303180.53125,\n",
              "  322427.1875,\n",
              "  313159.875,\n",
              "  301944.75,\n",
              "  316601.25,\n",
              "  313085.875,\n",
              "  307067.65625,\n",
              "  303662.46875,\n",
              "  318223.84375,\n",
              "  313410.96875,\n",
              "  306896.3125,\n",
              "  318356.46875,\n",
              "  306051.875,\n",
              "  313344.75,\n",
              "  301012.25,\n",
              "  312378.53125,\n",
              "  316333.40625,\n",
              "  311771.46875,\n",
              "  299637.46875,\n",
              "  310819.03125,\n",
              "  310429.46875,\n",
              "  310138.25,\n",
              "  305362.03125,\n",
              "  299021.40625,\n",
              "  308879.71875,\n",
              "  316004.9375,\n",
              "  315231.28125,\n",
              "  315030.0,\n",
              "  296998.71875,\n",
              "  307068.53125,\n",
              "  306784.6875,\n",
              "  298965.40625,\n",
              "  304809.0,\n",
              "  305666.34375,\n",
              "  309357.3125,\n",
              "  304722.96875,\n",
              "  297262.875,\n",
              "  307120.09375,\n",
              "  292438.625,\n",
              "  299128.53125,\n",
              "  295740.34375,\n",
              "  298506.625,\n",
              "  302331.15625,\n",
              "  312916.28125,\n",
              "  297436.34375,\n",
              "  293127.6875,\n",
              "  293605.5625,\n",
              "  297298.0625,\n",
              "  292130.5,\n",
              "  288305.71875,\n",
              "  302241.03125,\n",
              "  302861.25,\n",
              "  294533.71875,\n",
              "  288393.15625,\n",
              "  300769.90625,\n",
              "  289752.28125,\n",
              "  289411.40625,\n",
              "  292883.59375,\n",
              "  292431.21875,\n",
              "  299065.125,\n",
              "  288877.40625,\n",
              "  298352.15625,\n",
              "  292050.15625,\n",
              "  284097.09375,\n",
              "  294304.09375,\n",
              "  284082.34375,\n",
              "  289774.125,\n",
              "  286481.84375,\n",
              "  286257.40625,\n",
              "  299302.25,\n",
              "  284764.65625,\n",
              "  291164.625,\n",
              "  294374.1875,\n",
              "  287314.53125,\n",
              "  287099.59375,\n",
              "  294102.59375,\n",
              "  290036.03125,\n",
              "  296223.59375,\n",
              "  282957.0625,\n",
              "  282408.21875,\n",
              "  278641.75,\n",
              "  281988.3125,\n",
              "  288124.84375,\n",
              "  284259.59375,\n",
              "  287338.78125,\n",
              "  290515.59375,\n",
              "  283305.65625,\n",
              "  289259.46875,\n",
              "  286139.28125,\n",
              "  282108.03125,\n",
              "  291820.53125,\n",
              "  288353.40625,\n",
              "  274954.96875,\n",
              "  274636.90625,\n",
              "  277753.09375,\n",
              "  274047.15625,\n",
              "  280755.375,\n",
              "  286454.125,\n",
              "  285763.03125,\n",
              "  279085.21875,\n",
              "  275305.78125,\n",
              "  291692.65625,\n",
              "  291349.59375,\n",
              "  283903.125,\n",
              "  280717.8125,\n",
              "  277235.71875,\n",
              "  271210.875,\n",
              "  286385.875,\n",
              "  276328.40625,\n",
              "  279260.75,\n",
              "  279076.15625,\n",
              "  269221.15625,\n",
              "  268941.8125,\n",
              "  280861.75,\n",
              "  283413.6875,\n",
              "  283421.53125,\n",
              "  273934.3125,\n",
              "  279754.21875,\n",
              "  282585.71875,\n",
              "  261298.234375,\n",
              "  273174.03125,\n",
              "  269632.03125,\n",
              "  272572.71875,\n",
              "  271869.34375,\n",
              "  265550.8125,\n",
              "  271275.59375,\n",
              "  274175.96875,\n",
              "  264482.78125,\n",
              "  270655.3125,\n",
              "  270613.71875,\n",
              "  272644.125,\n",
              "  266120.46875,\n",
              "  275065.90625,\n",
              "  271986.15625,\n",
              "  271686.375,\n",
              "  268740.46875,\n",
              "  265237.28125,\n",
              "  279606.40625,\n",
              "  270660.25,\n",
              "  270105.0,\n",
              "  267051.15625,\n",
              "  269672.34375,\n",
              "  260717.4375,\n",
              "  263469.46875,\n",
              "  265965.96875,\n",
              "  257317.953125,\n",
              "  276844.9375,\n",
              "  256667.515625,\n",
              "  267686.375,\n",
              "  267353.28125,\n",
              "  267224.46875,\n",
              "  266844.15625,\n",
              "  260904.125,\n",
              "  263356.625,\n",
              "  269038.4375,\n",
              "  262992.1875,\n",
              "  268320.65625,\n",
              "  262589.0,\n",
              "  262047.046875,\n",
              "  261896.109375,\n",
              "  256108.484375,\n",
              "  269481.90625,\n",
              "  261001.9375,\n",
              "  255368.328125,\n",
              "  260549.390625,\n",
              "  260374.859375,\n",
              "  252077.703125,\n",
              "  259854.921875,\n",
              "  267939.71875,\n",
              "  253630.046875,\n",
              "  264516.5,\n",
              "  261700.796875,\n",
              "  253194.5,\n",
              "  255646.25,\n",
              "  252713.296875,\n",
              "  265567.5,\n",
              "  254750.5,\n",
              "  244142.375,\n",
              "  257085.484375,\n",
              "  262134.265625,\n",
              "  259417.296875,\n",
              "  253891.953125,\n",
              "  261226.578125,\n",
              "  245509.296875,\n",
              "  255637.890625,\n",
              "  255143.75,\n",
              "  250071.671875,\n",
              "  244647.921875,\n",
              "  254651.4375,\n",
              "  244212.125,\n",
              "  251107.359375,\n",
              "  253785.046875,\n",
              "  253703.453125,\n",
              "  252892.0,\n",
              "  255852.875,\n",
              "  250501.578125,\n",
              "  255349.296875,\n",
              "  252319.375,\n",
              "  251923.140625,\n",
              "  254612.25,\n",
              "  249545.125,\n",
              "  244317.25,\n",
              "  248571.3125,\n",
              "  251119.171875,\n",
              "  248280.421875,\n",
              "  255468.5,\n",
              "  252788.578125,\n",
              "  249797.375,\n",
              "  254515.546875,\n",
              "  242260.421875,\n",
              "  239908.078125,\n",
              "  249161.703125,\n",
              "  246517.0,\n",
              "  248248.0,\n",
              "  243498.328125,\n",
              "  248114.0,\n",
              "  250249.390625,\n",
              "  247666.828125,\n",
              "  249854.578125,\n",
              "  252360.203125,\n",
              "  249398.921875,\n",
              "  249178.5,\n",
              "  251131.25,\n",
              "  253456.015625,\n",
              "  246922.796875,\n",
              "  250424.296875,\n",
              "  245512.609375,\n",
              "  238258.890625,\n",
              "  245684.8125,\n",
              "  240384.578125,\n",
              "  244993.25,\n",
              "  245244.203125,\n",
              "  235409.921875,\n",
              "  247205.0,\n",
              "  246352.296875,\n",
              "  241909.359375,\n",
              "  243572.046875,\n",
              "  245892.640625,\n",
              "  245503.0,\n",
              "  247989.015625,\n",
              "  238223.046875,\n",
              "  245594.796875,\n",
              "  239120.328125,\n",
              "  244979.546875,\n",
              "  239808.609375,\n",
              "  241385.890625,\n",
              "  246372.25,\n",
              "  241498.4375,\n",
              "  245708.25,\n",
              "  243252.1875,\n",
              "  239453.234375,\n",
              "  240526.9375,\n",
              "  234920.203125,\n",
              "  242628.859375,\n",
              "  240769.0625,\n",
              "  243732.3125,\n",
              "  239389.796875,\n",
              "  239481.484375,\n",
              "  237554.75,\n",
              "  242144.421875,\n",
              "  237189.328125,\n",
              "  236653.203125,\n",
              "  236830.6875,\n",
              "  242798.953125,\n",
              "  237194.796875,\n",
              "  242034.453125,\n",
              "  238394.625,\n",
              "  231598.140625,\n",
              "  233894.265625,\n",
              "  240326.125,\n",
              "  239373.4375,\n",
              "  236577.390625,\n",
              "  235706.609375,\n",
              "  236221.5625,\n",
              "  232145.921875,\n",
              "  231798.890625,\n",
              "  230384.265625,\n",
              "  237934.8125,\n",
              "  233922.734375,\n",
              "  235622.625,\n",
              "  239826.640625,\n",
              "  233431.453125,\n",
              "  235007.265625,\n",
              "  239296.875,\n",
              "  234478.859375,\n",
              "  235194.3125,\n",
              "  232442.140625,\n",
              "  237282.265625,\n",
              "  236019.25,\n",
              "  238616.453125,\n",
              "  239358.703125,\n",
              "  230380.640625,\n",
              "  236790.6875,\n",
              "  236613.265625,\n",
              "  239198.390625,\n",
              "  232649.75,\n",
              "  237243.953125,\n",
              "  229520.828125,\n",
              "  221053.953125,\n",
              "  234380.9375,\n",
              "  230476.546875,\n",
              "  234054.046875,\n",
              "  227672.0,\n",
              "  230026.9375,\n",
              "  231203.359375,\n",
              "  236892.1875,\n",
              "  230510.078125,\n",
              "  232900.203125,\n",
              "  236766.734375,\n",
              "  233667.75,\n",
              "  230999.671875,\n",
              "  232279.25,\n",
              "  233366.453125,\n",
              "  234504.6875,\n",
              "  224727.8125,\n",
              "  223754.5625,\n",
              "  225844.671875,\n",
              "  229180.203125,\n",
              "  223414.5,\n",
              "  226803.0625,\n",
              "  234409.375,\n",
              "  230985.578125,\n",
              "  234099.0,\n",
              "  228579.125,\n",
              "  226202.796875,\n",
              "  230418.453125,\n",
              "  226661.671875,\n",
              "  226534.75,\n",
              "  226602.5,\n",
              "  224546.359375,\n",
              "  229502.046875,\n",
              "  229360.453125,\n",
              "  227135.703125,\n",
              "  227442.671875,\n",
              "  222751.828125,\n",
              "  228012.265625,\n",
              "  219256.046875,\n",
              "  226783.765625,\n",
              "  227649.6875,\n",
              "  228299.828125,\n",
              "  231393.234375,\n",
              "  227501.015625,\n",
              "  221704.875,\n",
              "  227977.421875,\n",
              "  227638.390625,\n",
              "  224556.203125,\n",
              "  228274.875,\n",
              "  225025.015625,\n",
              "  227549.328125,\n",
              "  226561.125,\n",
              "  226448.75,\n",
              "  230894.109375,\n",
              "  226841.265625,\n",
              "  223449.75,\n",
              "  230460.453125,\n",
              "  226273.484375,\n",
              "  223134.6875,\n",
              "  225693.796875,\n",
              "  220904.75,\n",
              "  227268.453125,\n",
              "  225356.375,\n",
              "  223435.515625,\n",
              "  225661.671875,\n",
              "  228471.828125,\n",
              "  224513.078125,\n",
              "  228014.375,\n",
              "  228103.75,\n",
              "  222434.734375,\n",
              "  222128.0,\n",
              "  227316.234375,\n",
              "  227840.578125,\n",
              "  221841.921875,\n",
              "  226638.9375,\n",
              "  224249.4375,\n",
              "  224356.0625,\n",
              "  223818.546875,\n",
              "  221378.484375,\n",
              "  223258.953125,\n",
              "  218114.234375,\n",
              "  221451.25,\n",
              "  221024.015625,\n",
              "  222789.265625,\n",
              "  223016.0625,\n",
              "  220468.390625,\n",
              "  220171.640625,\n",
              "  225459.875,\n",
              "  222706.171875,\n",
              "  221858.953125,\n",
              "  217964.046875,\n",
              "  224791.609375,\n",
              "  221562.25,\n",
              "  220101.75,\n",
              "  219800.5,\n",
              "  222909.328125,\n",
              "  224504.328125,\n",
              "  217266.234375,\n",
              "  219874.4375,\n",
              "  222291.015625,\n",
              "  223739.546875,\n",
              "  221846.265625,\n",
              "  217519.9375,\n",
              "  221571.953125,\n",
              "  217062.765625,\n",
              "  221227.703125,\n",
              "  219134.875,\n",
              "  223579.625,\n",
              "  220930.046875,\n",
              "  221530.234375,\n",
              "  216754.4375,\n",
              "  221146.25,\n",
              "  220590.734375,\n",
              "  218118.015625,\n",
              "  218598.078125,\n",
              "  218326.5,\n",
              "  218702.046875,\n",
              "  222541.234375,\n",
              "  222411.453125,\n",
              "  216267.140625,\n",
              "  218228.0,\n",
              "  218399.421875,\n",
              "  220326.625,\n",
              "  215959.421875,\n",
              "  220298.234375,\n",
              "  219880.421875,\n",
              "  221976.421875,\n",
              "  220212.828125,\n",
              "  217853.375,\n",
              "  217555.546875,\n",
              "  219414.234375,\n",
              "  217425.359375,\n",
              "  221468.421875,\n",
              "  219352.734375,\n",
              "  215092.9375,\n",
              "  217645.890625,\n",
              "  221202.125,\n",
              "  217555.1875,\n",
              "  217008.5,\n",
              "  217262.0625,\n",
              "  217417.6875,\n",
              "  220958.234375,\n",
              "  215370.734375,\n",
              "  218921.859375,\n",
              "  219030.828125,\n",
              "  220671.046875,\n",
              "  218598.125,\n",
              "  218543.5625,\n",
              "  218123.5625,\n",
              "  220690.703125,\n",
              "  219165.5625,\n",
              "  216732.984375,\n",
              "  220538.703125,\n",
              "  220661.796875,\n",
              "  218436.734375,\n",
              "  220581.171875,\n",
              "  215864.5625,\n",
              "  218298.171875,\n",
              "  216509.046875,\n",
              "  219707.390625,\n",
              "  219902.453125,\n",
              "  218378.796875,\n",
              "  216123.359375,\n",
              "  216861.109375,\n",
              "  217506.078125,\n",
              "  217957.203125,\n",
              "  219888.203125,\n",
              "  219038.125,\n",
              "  219235.921875,\n",
              "  218144.609375,\n",
              "  215809.671875,\n",
              "  217239.625,\n",
              "  218068.109375,\n",
              "  218669.328125,\n",
              "  214119.453125,\n",
              "  217705.578125,\n",
              "  217316.921875,\n",
              "  216428.046875,\n",
              "  212789.171875,\n",
              "  216412.515625,\n",
              "  217247.359375,\n",
              "  213757.734375,\n",
              "  215675.578125,\n",
              "  219517.296875,\n",
              "  218973.171875,\n",
              "  216137.328125,\n",
              "  215872.390625,\n",
              "  215088.9375,\n",
              "  218901.0,\n",
              "  217753.25,\n",
              "  220170.296875,\n",
              "  217983.984375,\n",
              "  216583.671875,\n",
              "  220447.015625,\n",
              "  219858.578125,\n",
              "  218143.25,\n",
              "  215531.265625,\n",
              "  216373.0,\n",
              "  216660.8125,\n",
              "  213884.203125,\n",
              "  216369.625,\n",
              "  218030.921875,\n",
              "  217687.890625,\n",
              "  216041.140625,\n",
              "  216017.828125,\n",
              "  219084.6875,\n",
              "  217848.421875,\n",
              "  216486.375,\n",
              "  216033.9375,\n",
              "  217046.328125,\n",
              "  218243.890625,\n",
              "  216541.328125,\n",
              "  217912.421875,\n",
              "  215583.171875,\n",
              "  215020.25,\n",
              "  215811.453125,\n",
              "  213969.546875,\n",
              "  216820.6875,\n",
              "  216036.328125,\n",
              "  216946.171875,\n",
              "  217852.640625,\n",
              "  218694.0,\n",
              "  214755.703125,\n",
              "  216856.546875,\n",
              "  215910.046875,\n",
              "  214361.234375,\n",
              "  217499.421875,\n",
              "  215955.640625,\n",
              "  217768.765625,\n",
              "  214789.796875,\n",
              "  217487.234375,\n",
              "  216946.234375,\n",
              "  218705.75,\n",
              "  217470.203125,\n",
              "  215658.734375,\n",
              "  217098.453125,\n",
              "  221004.0625,\n",
              "  216643.359375,\n",
              "  215666.625,\n",
              "  216960.921875,\n",
              "  218284.625,\n",
              "  216715.796875,\n",
              "  220911.0,\n",
              "  218528.203125,\n",
              "  217459.890625,\n",
              "  221279.421875,\n",
              "  217061.0625,\n",
              "  216804.625,\n",
              "  216822.234375,\n",
              "  219647.328125,\n",
              "  220042.4375,\n",
              "  214768.375,\n",
              "  220372.296875,\n",
              "  219808.671875,\n",
              "  216342.5625,\n",
              "  217238.984375,\n",
              "  222889.9375,\n",
              "  216199.515625,\n",
              "  217035.640625,\n",
              "  217095.484375,\n",
              "  218325.078125,\n",
              "  217113.375,\n",
              "  215880.203125,\n",
              "  217443.453125,\n",
              "  217471.140625,\n",
              "  219067.578125,\n",
              "  215677.421875,\n",
              "  219293.875,\n",
              "  220361.375,\n",
              "  217512.484375,\n",
              "  218998.484375,\n",
              "  220757.484375,\n",
              "  215644.5,\n",
              "  215549.75,\n",
              "  215800.375,\n",
              "  217548.5,\n",
              "  217837.25,\n",
              "  218186.703125,\n",
              "  217377.484375,\n",
              "  217421.296875,\n",
              "  217734.796875,\n",
              "  218090.375,\n",
              "  219313.109375,\n",
              "  219318.9375,\n",
              "  218397.3125,\n",
              "  217732.4375,\n",
              "  216581.265625,\n",
              "  216805.078125,\n",
              "  218013.25,\n",
              "  216845.5,\n",
              "  219826.484375,\n",
              "  220025.046875,\n",
              "  221633.0,\n",
              "  219960.828125,\n",
              "  218722.828125,\n",
              "  218793.203125,\n",
              "  220382.421875,\n",
              "  217053.015625,\n",
              "  216636.640625,\n",
              "  220492.203125,\n",
              "  217270.234375,\n",
              "  220045.078125,\n",
              "  220964.234375,\n",
              "  219083.859375,\n",
              "  219182.1875,\n",
              "  219225.125,\n",
              "  218952.625,\n",
              "  219419.1875,\n",
              "  217141.125,\n",
              "  217490.546875,\n",
              "  219569.703125,\n",
              "  219744.875,\n",
              "  219178.921875],\n",
              " 'val_mae': [569.5404663085938,\n",
              "  566.8486938476562,\n",
              "  559.7733764648438,\n",
              "  546.316650390625,\n",
              "  567.1821899414062,\n",
              "  546.6755981445312,\n",
              "  563.195556640625,\n",
              "  543.6968383789062,\n",
              "  545.0774536132812,\n",
              "  570.6043090820312,\n",
              "  559.1987915039062,\n",
              "  539.325927734375,\n",
              "  561.6555786132812,\n",
              "  548.2302856445312,\n",
              "  549.8993530273438,\n",
              "  558.2507934570312,\n",
              "  565.770751953125,\n",
              "  566.8583374023438,\n",
              "  556.02587890625,\n",
              "  553.3349609375,\n",
              "  562.5936889648438,\n",
              "  523.7437133789062,\n",
              "  555.185546875,\n",
              "  533.5778198242188,\n",
              "  541.3782348632812,\n",
              "  544.7944946289062,\n",
              "  531.6246948242188,\n",
              "  522.2539672851562,\n",
              "  540.5292358398438,\n",
              "  548.3233032226562,\n",
              "  546.488525390625,\n",
              "  515.587890625,\n",
              "  538.2173461914062,\n",
              "  556.7959594726562,\n",
              "  536.6516723632812,\n",
              "  536.20068359375,\n",
              "  535.643798828125,\n",
              "  532.9592895507812,\n",
              "  553.268310546875,\n",
              "  533.7025756835938,\n",
              "  560.9945678710938,\n",
              "  530.0829467773438,\n",
              "  540.2050170898438,\n",
              "  537.5342407226562,\n",
              "  538.8358764648438,\n",
              "  528.073974609375,\n",
              "  535.5908813476562,\n",
              "  517.772216796875,\n",
              "  536.619140625,\n",
              "  527.2319946289062,\n",
              "  514.0663452148438,\n",
              "  521.86962890625,\n",
              "  542.1981811523438,\n",
              "  520.5906372070312,\n",
              "  517.3118286132812,\n",
              "  525.3843383789062,\n",
              "  512.2232055664062,\n",
              "  513.322509765625,\n",
              "  538.8607788085938,\n",
              "  537.3712768554688,\n",
              "  539.5978393554688,\n",
              "  525.8837280273438,\n",
              "  516.7937622070312,\n",
              "  526.6366577148438,\n",
              "  497.9667053222656,\n",
              "  506.113525390625,\n",
              "  516.229248046875,\n",
              "  504.817138671875,\n",
              "  502.4264221191406,\n",
              "  516.33349609375,\n",
              "  515.6699829101562,\n",
              "  531.6386108398438,\n",
              "  523.1240234375,\n",
              "  511.357177734375,\n",
              "  508.765380859375,\n",
              "  508.4048767089844,\n",
              "  526.6742553710938,\n",
              "  526.3163452148438,\n",
              "  498.0223693847656,\n",
              "  516.85546875,\n",
              "  526.4254760742188,\n",
              "  515.583251953125,\n",
              "  514.3603515625,\n",
              "  516.023193359375,\n",
              "  513.3633422851562,\n",
              "  503.7154235839844,\n",
              "  520.5298461914062,\n",
              "  502.715087890625,\n",
              "  510.5098571777344,\n",
              "  518.5986938476562,\n",
              "  517.675537109375,\n",
              "  510.908203125,\n",
              "  497.1758728027344,\n",
              "  498.851806640625,\n",
              "  497.9146423339844,\n",
              "  508.03662109375,\n",
              "  484.107177734375,\n",
              "  502.952392578125,\n",
              "  512.5145874023438,\n",
              "  522.6185913085938,\n",
              "  501.0428771972656,\n",
              "  511.4347229003906,\n",
              "  493.0557556152344,\n",
              "  500.848388671875,\n",
              "  492.0411682128906,\n",
              "  499.84130859375,\n",
              "  486.1414489746094,\n",
              "  487.7909240722656,\n",
              "  490.9236755371094,\n",
              "  488.5224609375,\n",
              "  498.6366271972656,\n",
              "  476.4637756347656,\n",
              "  514.8193969726562,\n",
              "  472.7997131347656,\n",
              "  496.0791931152344,\n",
              "  482.6410217285156,\n",
              "  492.7574768066406,\n",
              "  494.4335021972656,\n",
              "  469.9740295410156,\n",
              "  492.5927734375,\n",
              "  500.6847229003906,\n",
              "  480.8365173339844,\n",
              "  488.64501953125,\n",
              "  479.8316345214844,\n",
              "  479.1944274902344,\n",
              "  469.5448303222656,\n",
              "  464.85498046875,\n",
              "  477.291259765625,\n",
              "  484.8116149902344,\n",
              "  475.4620056152344,\n",
              "  483.8291931152344,\n",
              "  472.6298828125,\n",
              "  463.86328125,\n",
              "  471.9942321777344,\n",
              "  471.6670227050781,\n",
              "  492.7442932128906,\n",
              "  491.9898376464844,\n",
              "  489.8390197753906,\n",
              "  482.912109375,\n",
              "  470.2459411621094,\n",
              "  482.4265441894531,\n",
              "  469.2937316894531,\n",
              "  481.6285705566406,\n",
              "  468.6829528808594,\n",
              "  480.8848876953125,\n",
              "  489.2040100097656,\n",
              "  461.3565979003906,\n",
              "  497.0533142089844,\n",
              "  477.7327575683594,\n",
              "  477.4287109375,\n",
              "  479.0541076660156,\n",
              "  459.813720703125,\n",
              "  486.8746337890625,\n",
              "  478.1112365722656,\n",
              "  465.2769470214844,\n",
              "  466.9862365722656,\n",
              "  475.1136169433594,\n",
              "  485.0517272949219,\n",
              "  443.1268310546875,\n",
              "  465.7076110839844,\n",
              "  450.8487854003906,\n",
              "  475.552978515625,\n",
              "  462.8667297363281,\n",
              "  464.4376525878906,\n",
              "  449.732666015625,\n",
              "  461.9347229003906,\n",
              "  455.2123107910156,\n",
              "  471.396484375,\n",
              "  473.3116149902344,\n",
              "  479.4001159667969,\n",
              "  489.0565490722656,\n",
              "  478.4963684082031,\n",
              "  438.8231506347656,\n",
              "  459.6145935058594,\n",
              "  461.3645935058594,\n",
              "  470.0610656738281,\n",
              "  461.0833435058594,\n",
              "  469.4723815917969,\n",
              "  469.7403869628906,\n",
              "  477.8080749511719,\n",
              "  450.9844055175781,\n",
              "  479.4209899902344,\n",
              "  477.4461975097656,\n",
              "  469.3925476074219,\n",
              "  477.674560546875,\n",
              "  461.3645935058594,\n",
              "  487.3602294921875,\n",
              "  451.077392578125,\n",
              "  461.28125,\n",
              "  461.0833435058594,\n",
              "  459.6145324707031,\n",
              "  449.3807373046875,\n",
              "  449.393798828125,\n",
              "  461.3645935058594,\n",
              "  479.1588439941406,\n",
              "  477.10107421875,\n",
              "  459.0520935058594,\n",
              "  441.3366394042969,\n",
              "  471.5058288574219,\n",
              "  479.5892028808594,\n",
              "  479.5627136230469,\n",
              "  477.2240295410156,\n",
              "  441.469482421875,\n",
              "  477.4517822265625,\n",
              "  457.0208435058594,\n",
              "  439.5179443359375,\n",
              "  467.0559997558594,\n",
              "  467.0426330566406,\n",
              "  461.0,\n",
              "  469.0475158691406,\n",
              "  451.3828125,\n",
              "  459.6145935058594,\n",
              "  449.6592712402344,\n",
              "  453.7355041503906,\n",
              "  453.3841247558594,\n",
              "  449.4185791015625,\n",
              "  457.3020935058594,\n",
              "  461.0833435058594,\n",
              "  461.3645935058594,\n",
              "  468.9128112792969,\n",
              "  463.6770935058594,\n",
              "  461.3645935058594,\n",
              "  476.3813171386719,\n",
              "  470.8912353515625,\n",
              "  469.1278381347656,\n",
              "  468.8330993652344,\n",
              "  468.8199157714844,\n",
              "  470.8375549316406,\n",
              "  461.3645935058594,\n",
              "  469.060791015625,\n",
              "  476.7301330566406,\n",
              "  476.4222717285156,\n",
              "  459.3333435058594,\n",
              "  457.3020935058594,\n",
              "  461.3645935058594,\n",
              "  447.9356384277344,\n",
              "  466.3742980957031,\n",
              "  476.2634582519531,\n",
              "  478.5493469238281,\n",
              "  453.8011474609375,\n",
              "  461.6458435058594,\n",
              "  452.0784912109375,\n",
              "  459.6145324707031,\n",
              "  463.3958435058594,\n",
              "  454.1492004394531,\n",
              "  454.1622619628906,\n",
              "  453.52978515625,\n",
              "  470.5715637207031,\n",
              "  468.8084411621094,\n",
              "  468.794921875,\n",
              "  468.5001525878906,\n",
              "  470.7999267578125,\n",
              "  450.1924743652344,\n",
              "  463.1145935058594,\n",
              "  470.7602233886719,\n",
              "  457.5833435058594,\n",
              "  463.3958435058594,\n",
              "  459.6145935058594,\n",
              "  461.3645324707031,\n",
              "  470.1317443847656,\n",
              "  456.3918762207031,\n",
              "  461.28125,\n",
              "  468.3418273925781,\n",
              "  450.056640625,\n",
              "  477.5786437988281,\n",
              "  459.0520935058594,\n",
              "  459.3332824707031,\n",
              "  452.4220886230469,\n",
              "  466.5127258300781,\n",
              "  458.6875,\n",
              "  454.2120056152344,\n",
              "  470.5353088378906,\n",
              "  461.3645935058594,\n",
              "  463.3958435058594,\n",
              "  466.4329833984375,\n",
              "  475.8184814453125,\n",
              "  461.3645935058594,\n",
              "  477.234375,\n",
              "  459.6145935058594,\n",
              "  474.8685302734375,\n",
              "  466.3534240722656,\n",
              "  475.6593933105469,\n",
              "  466.0457763671875,\n",
              "  459.6145935058594,\n",
              "  468.0506286621094,\n",
              "  459.6145935058594,\n",
              "  467.7427062988281,\n",
              "  470.0419616699219,\n",
              "  467.7159423828125,\n",
              "  450.4014892578125,\n",
              "  454.7584228515625,\n",
              "  459.0520324707031,\n",
              "  459.3333435058594,\n",
              "  452.7669372558594,\n",
              "  454.1655578613281,\n",
              "  466.1544189453125,\n",
              "  474.4187927246094,\n",
              "  470.1907043457031,\n",
              "  468.427734375,\n",
              "  468.1327819824219,\n",
              "  474.3121032714844,\n",
              "  469.856201171875,\n",
              "  474.2596130371094,\n",
              "  468.0799255371094,\n",
              "  461.3645935058594,\n",
              "  461.3645935058594,\n",
              "  457.3020935058594,\n",
              "  459.3333435058594,\n",
              "  452.6842956542969,\n",
              "  448.6348571777344,\n",
              "  457.3020935058594,\n",
              "  457.0208435058594,\n",
              "  450.7060852050781,\n",
              "  475.9157409667969,\n",
              "  482.8228454589844,\n",
              "  455.0895080566406,\n",
              "  456.8523864746094,\n",
              "  461.0833435058594,\n",
              "  473.8357238769531,\n",
              "  468.1495666503906,\n",
              "  473.7829284667969,\n",
              "  476.3510437011719,\n",
              "  457.0208435058594,\n",
              "  459.3333435058594,\n",
              "  467.8028869628906,\n",
              "  449.126708984375,\n",
              "  459.3333435058594,\n",
              "  467.4824523925781,\n",
              "  453.2284851074219,\n",
              "  467.7372131347656,\n",
              "  467.4424743652344,\n",
              "  467.7106018066406,\n",
              "  459.0520935058594,\n",
              "  459.6145935058594,\n",
              "  461.6458435058594,\n",
              "  467.3762512207031,\n",
              "  459.0520935058594,\n",
              "  473.897705078125,\n",
              "  467.8989562988281,\n",
              "  463.3958435058594,\n",
              "  453.1067810058594,\n",
              "  449.0575256347656,\n",
              "  474.0461730957031,\n",
              "  467.2703552246094,\n",
              "  459.3333435058594,\n",
              "  467.5249328613281,\n",
              "  459.3333435058594,\n",
              "  467.7801818847656,\n",
              "  461.6458435058594,\n",
              "  457.2875671386719,\n",
              "  475.586669921875,\n",
              "  457.5833435058594,\n",
              "  463.3958435058594,\n",
              "  467.1390686035156,\n",
              "  480.9610290527344,\n",
              "  461.0833435058594,\n",
              "  465.0682373046875,\n",
              "  461.3645935058594,\n",
              "  459.6145935058594,\n",
              "  463.3958435058594,\n",
              "  461.6458435058594,\n",
              "  461.0833435058594,\n",
              "  466.7388916015625,\n",
              "  451.3785400390625,\n",
              "  459.3333435058594,\n",
              "  457.5833435058594,\n",
              "  453.73046875,\n",
              "  451.7124938964844,\n",
              "  461.0833435058594,\n",
              "  472.490966796875,\n",
              "  480.6089782714844,\n",
              "  459.0520935058594,\n",
              "  467.16943359375,\n",
              "  464.8439025878906,\n",
              "  469.4556884765625,\n",
              "  466.8486328125,\n",
              "  474.6180725097656,\n",
              "  468.8535461425781,\n",
              "  459.3333435058594,\n",
              "  461.3645935058594,\n",
              "  466.7823791503906,\n",
              "  467.0509338378906,\n",
              "  461.3645935058594,\n",
              "  472.4034118652344,\n",
              "  472.6581726074219,\n",
              "  466.7164306640625,\n",
              "  459.0520935058594,\n",
              "  468.7212219238281,\n",
              "  459.0520935058594,\n",
              "  452.0028381347656,\n",
              "  472.21728515625,\n",
              "  461.6458435058594,\n",
              "  451.761962890625,\n",
              "  466.3288879394531,\n",
              "  464.2845458984375,\n",
              "  455.8636474609375,\n",
              "  454.126953125,\n",
              "  468.58935546875,\n",
              "  463.1145935058594,\n",
              "  459.3333435058594,\n",
              "  471.9552917480469,\n",
              "  459.0520935058594,\n",
              "  467.0556335449219,\n",
              "  453.936767578125,\n",
              "  466.4666748046875,\n",
              "  471.8230895996094,\n",
              "  467.0025634765625,\n",
              "  454.270751953125,\n",
              "  466.4139709472656,\n",
              "  466.4007568359375,\n",
              "  466.6689453125,\n",
              "  461.0833435058594,\n",
              "  457.5833435058594,\n",
              "  466.3478088378906,\n",
              "  474.1794128417969,\n",
              "  473.0279846191406,\n",
              "  473.5645446777344,\n",
              "  457.0208435058594,\n",
              "  466.5633850097656,\n",
              "  466.8314514160156,\n",
              "  459.0520935058594,\n",
              "  463.3958435058594,\n",
              "  466.7923889160156,\n",
              "  471.6314697265625,\n",
              "  466.2037048339844,\n",
              "  459.3333435058594,\n",
              "  468.490234375,\n",
              "  454.251953125,\n",
              "  461.0833435058594,\n",
              "  459.0520935058594,\n",
              "  461.3645935058594,\n",
              "  466.674560546875,\n",
              "  478.1609802246094,\n",
              "  461.3645935058594,\n",
              "  456.6568908691406,\n",
              "  459.0520935058594,\n",
              "  464.0155334472656,\n",
              "  456.695068359375,\n",
              "  454.3122253417969,\n",
              "  468.0393371582031,\n",
              "  470.9067687988281,\n",
              "  461.0833435058594,\n",
              "  457.5833435058594,\n",
              "  467.9861755371094,\n",
              "  456.787109375,\n",
              "  456.8004455566406,\n",
              "  461.3645935058594,\n",
              "  461.0833435058594,\n",
              "  468.2012939453125,\n",
              "  459.3333435058594,\n",
              "  468.1750793457031,\n",
              "  464.0993957519531,\n",
              "  454.8617248535156,\n",
              "  466.1037902832031,\n",
              "  457.0208435058594,\n",
              "  461.3645935058594,\n",
              "  459.3333435058594,\n",
              "  459.6145935058594,\n",
              "  472.7427673339844,\n",
              "  456.9853515625,\n",
              "  463.6770935058594,\n",
              "  467.749267578125,\n",
              "  461.0833435058594,\n",
              "  461.3645935058594,\n",
              "  470.2725524902344,\n",
              "  465.6648254394531,\n",
              "  471.9702453613281,\n",
              "  459.6145935058594,\n",
              "  459.0520935058594,\n",
              "  455.0850830078125,\n",
              "  459.6145935058594,\n",
              "  465.8677062988281,\n",
              "  461.6458435058594,\n",
              "  465.5602111816406,\n",
              "  469.7300720214844,\n",
              "  461.6458435058594,\n",
              "  467.5526123046875,\n",
              "  465.7897033691406,\n",
              "  461.0833435058594,\n",
              "  471.91162109375,\n",
              "  469.2923583984375,\n",
              "  455.24169921875,\n",
              "  455.2554016113281,\n",
              "  459.3333435058594,\n",
              "  455.281005859375,\n",
              "  463.9351501464844,\n",
              "  469.4171142578125,\n",
              "  467.6905212402344,\n",
              "  461.3645935058594,\n",
              "  457.0964660644531,\n",
              "  476.1620178222656,\n",
              "  476.1228332519531,\n",
              "  467.0625915527344,\n",
              "  465.018310546875,\n",
              "  461.3645935058594,\n",
              "  457.3020935058594,\n",
              "  472.0315856933594,\n",
              "  461.3645935058594,\n",
              "  465.2342834472656,\n",
              "  465.502685546875,\n",
              "  455.2085266113281,\n",
              "  455.22119140625,\n",
              "  467.2135925292969,\n",
              "  470.922119140625,\n",
              "  470.9797058105469,\n",
              "  461.3645935058594,\n",
              "  467.443115234375,\n",
              "  471.1827087402344,\n",
              "  451.2499084472656,\n",
              "  463.3412780761719,\n",
              "  459.3333435058594,\n",
              "  463.3152160644531,\n",
              "  461.3645935058594,\n",
              "  455.3778076171875,\n",
              "  461.3645935058594,\n",
              "  465.2938537597656,\n",
              "  453.81298828125,\n",
              "  462.9557800292969,\n",
              "  463.5049133300781,\n",
              "  463.3958435058594,\n",
              "  457.4165954589844,\n",
              "  466.9660949707031,\n",
              "  464.9222412109375,\n",
              "  464.908935546875,\n",
              "  463.14599609375,\n",
              "  459.0520935058594,\n",
              "  474.1938171386719,\n",
              "  465.138427734375,\n",
              "  463.3958435058594,\n",
              "  461.3645935058594,\n",
              "  464.8187255859375,\n",
              "  455.6107177734375,\n",
              "  459.3333435058594,\n",
              "  461.3645935058594,\n",
              "  453.8992004394531,\n",
              "  473.84765625,\n",
              "  453.6439208984375,\n",
              "  464.7281188964844,\n",
              "  463.3958435058594,\n",
              "  463.6770935058594,\n",
              "  464.6891174316406,\n",
              "  459.0520935058594,\n",
              "  459.8162536621094,\n",
              "  468.4979553222656,\n",
              "  462.605712890625,\n",
              "  468.1643981933594,\n",
              "  461.6458435058594,\n",
              "  461.0833740234375,\n",
              "  462.5534362792969,\n",
              "  455.8450012207031,\n",
              "  469.7846984863281,\n",
              "  461.0833435058594,\n",
              "  455.8838195800781,\n",
              "  462.4883728027344,\n",
              "  461.3645935058594,\n",
              "  454.1722717285156,\n",
              "  461.3645935058594,\n",
              "  470.4468078613281,\n",
              "  455.8780517578125,\n",
              "  466.7547912597656,\n",
              "  464.9919128417969,\n",
              "  456.0001525878906,\n",
              "  459.3333435058594,\n",
              "  457.3020935058594,\n",
              "  469.4211730957031,\n",
              "  459.0520935058594,\n",
              "  449.0157775878906,\n",
              "  461.3645935058594,\n",
              "  466.6373291015625,\n",
              "  463.6770935058594,\n",
              "  458.4295959472656,\n",
              "  466.3179931640625,\n",
              "  451.202392578125,\n",
              "  461.3645935058594,\n",
              "  462.2170104980469,\n",
              "  456.4632568359375,\n",
              "  451.30615234375,\n",
              "  461.3645935058594,\n",
              "  451.3592224121094,\n",
              "  458.96875,\n",
              "  461.0833435058594,\n",
              "  461.3645935058594,\n",
              "  460.71875,\n",
              "  463.3958435058594,\n",
              "  459.6145935058594,\n",
              "  464.66650390625,\n",
              "  462.3409118652344,\n",
              "  462.046630859375,\n",
              "  464.6270446777344,\n",
              "  458.6783142089844,\n",
              "  453.7046203613281,\n",
              "  459.0520935058594,\n",
              "  461.3645935058594,\n",
              "  459.3333435058594,\n",
              "  466.0176696777344,\n",
              "  464.255126953125,\n",
              "  461.9300537109375,\n",
              "  465.8960876464844,\n",
              "  454.4503479003906,\n",
              "  451.9061279296875,\n",
              "  461.0833435058594,\n",
              "  459.3333740234375,\n",
              "  461.5707092285156,\n",
              "  457.3020935058594,\n",
              "  462.1073303222656,\n",
              "  462.75,\n",
              "  462.0816345214844,\n",
              "  463.8184814453125,\n",
              "  466.1177978515625,\n",
              "  463.7923583984375,\n",
              "  463.7796936035156,\n",
              "  466.4501953125,\n",
              "  468.4549255371094,\n",
              "  460.7383728027344,\n",
              "  466.37158203125,\n",
              "  461.6838684082031,\n",
              "  454.318603515625,\n",
              "  461.3645935058594,\n",
              "  457.3020935058594,\n",
              "  462.1954650878906,\n",
              "  461.6457824707031,\n",
              "  452.10888671875,\n",
              "  463.3958435058594,\n",
              "  463.6129150390625,\n",
              "  459.6145935058594,\n",
              "  461.8376770019531,\n",
              "  463.8560485839844,\n",
              "  463.5612487792969,\n",
              "  465.861083984375,\n",
              "  457.3020935058594,\n",
              "  463.3958435058594,\n",
              "  456.790771484375,\n",
              "  463.3125,\n",
              "  459.0520935058594,\n",
              "  461.1596374511719,\n",
              "  465.49072265625,\n",
              "  461.9776916503906,\n",
              "  466.3464660644531,\n",
              "  463.4205627441406,\n",
              "  459.0402526855469,\n",
              "  461.6446838378906,\n",
              "  455.0162658691406,\n",
              "  463.6505432128906,\n",
              "  461.6458435058594,\n",
              "  465.324462890625,\n",
              "  461.300048828125,\n",
              "  460.71875,\n",
              "  459.6145935058594,\n",
              "  463.3958435058594,\n",
              "  459.6145935058594,\n",
              "  459.0521240234375,\n",
              "  459.6145935058594,\n",
              "  464.9909973144531,\n",
              "  459.5012512207031,\n",
              "  465.6279602050781,\n",
              "  461.3645935058594,\n",
              "  455.1966552734375,\n",
              "  457.2403259277344,\n",
              "  463.3958435058594,\n",
              "  463.4322204589844,\n",
              "  461.1077880859375,\n",
              "  459.3215637207031,\n",
              "  461.0825500488281,\n",
              "  457.3020935058594,\n",
              "  457.0208435058594,\n",
              "  455.59130859375,\n",
              "  463.0625,\n",
              "  459.3333435058594,\n",
              "  460.7187805175781,\n",
              "  465.0557556152344,\n",
              "  459.3333435058594,\n",
              "  461.5304260253906,\n",
              "  465.0177307128906,\n",
              "  461.2236328125,\n",
              "  461.3645935058594,\n",
              "  459.0520935058594,\n",
              "  463.3958435058594,\n",
              "  462.9227294921875,\n",
              "  465.5033264160156,\n",
              "  466.4607849121094,\n",
              "  457.5323181152344,\n",
              "  464.6597595214844,\n",
              "  464.63427734375,\n",
              "  467.2023010253906,\n",
              "  461.0836486816406,\n",
              "  465.1328125,\n",
              "  457.6092224121094,\n",
              "  450.097412109375,\n",
              "  463.06396484375,\n",
              "  459.3333435058594,\n",
              "  463.0381164550781,\n",
              "  456.2939758300781,\n",
              "  459.3333435058594,\n",
              "  460.9678039550781,\n",
              "  466.3581848144531,\n",
              "  460.3798828125,\n",
              "  462.6794128417969,\n",
              "  466.8438415527344,\n",
              "  463.9429931640625,\n",
              "  461.0,\n",
              "  462.6282653808594,\n",
              "  464.1473388671875,\n",
              "  464.9149475097656,\n",
              "  455.7125549316406,\n",
              "  454.5963134765625,\n",
              "  457.3020935058594,\n",
              "  459.8968505859375,\n",
              "  454.6734313964844,\n",
              "  458.1718444824219,\n",
              "  465.9745178222656,\n",
              "  462.7824401855469,\n",
              "  465.9246826171875,\n",
              "  461.0069274902344,\n",
              "  458.2347412109375,\n",
              "  462.7322082519531,\n",
              "  459.0520935058594,\n",
              "  459.0520935058594,\n",
              "  459.3333435058594,\n",
              "  456.9823913574219,\n",
              "  462.3860778808594,\n",
              "  462.373046875,\n",
              "  460.6107177734375,\n",
              "  460.381103515625,\n",
              "  456.3313903808594,\n",
              "  461.3645935058594,\n",
              "  453.0993957519531,\n",
              "  460.1508483886719,\n",
              "  461.3645935058594,\n",
              "  462.2731628417969,\n",
              "  465.4683837890625,\n",
              "  461.6457824707031,\n",
              "  456.1507263183594,\n",
              "  462.5030517578125,\n",
              "  462.2085876464844,\n",
              "  459.3332824707031,\n",
              "  463.2828674316406,\n",
              "  459.4715881347656,\n",
              "  462.720703125,\n",
              "  461.6458435058594,\n",
              "  461.6458435058594,\n",
              "  466.5049133300781,\n",
              "  462.3878173828125,\n",
              "  459.0520935058594,\n",
              "  466.390625,\n",
              "  462.06884765625,\n",
              "  459.0520935058594,\n",
              "  461.678955078125,\n",
              "  457.3020935058594,\n",
              "  463.1145935058594,\n",
              "  461.6458435058594,\n",
              "  459.4630432128906,\n",
              "  462.260986328125,\n",
              "  465.16162109375,\n",
              "  461.0833435058594,\n",
              "  464.8308410644531,\n",
              "  465.0869445800781,\n",
              "  459.6145935058594,\n",
              "  459.3333435058594,\n",
              "  464.4501953125,\n",
              "  465.2694396972656,\n",
              "  459.3333435058594,\n",
              "  464.0832214355469,\n",
              "  461.8420104980469,\n",
              "  462.1102600097656,\n",
              "  461.5350036621094,\n",
              "  459.3333435058594,\n",
              "  461.0833435058594,\n",
              "  456.1912536621094,\n",
              "  459.7351989746094,\n",
              "  459.3333435058594,\n",
              "  460.9888610839844,\n",
              "  461.3645324707031,\n",
              "  458.9825134277344,\n",
              "  458.71435546875,\n",
              "  464.046630859375,\n",
              "  461.395751953125,\n",
              "  460.482666015625,\n",
              "  457.0208435058594,\n",
              "  463.6703796386719,\n",
              "  460.4561767578125,\n",
              "  459.3333435058594,\n",
              "  459.0520935058594,\n",
              "  462.3773498535156,\n",
              "  463.8210754394531,\n",
              "  456.8203125,\n",
              "  459.5211181640625,\n",
              "  461.9962463378906,\n",
              "  463.2772216796875,\n",
              "  461.6458435058594,\n",
              "  457.5833435058594,\n",
              "  461.4891662597656,\n",
              "  457.1893310546875,\n",
              "  461.28125,\n",
              "  459.3333435058594,\n",
              "  463.7519226074219,\n",
              "  461.1452331542969,\n",
              "  461.9766540527344,\n",
              "  457.3020935058594,\n",
              "  461.6706237792969,\n",
              "  461.0713195800781,\n",
              "  458.687744140625,\n",
              "  459.3333435058594,\n",
              "  459.0779113769531,\n",
              "  459.6145935058594,\n",
              "  463.3447265625,\n",
              "  463.2686767578125,\n",
              "  457.3778381347656,\n",
              "  459.3333435058594,\n",
              "  459.6145935058594,\n",
              "  461.5333557128906,\n",
              "  457.27099609375,\n",
              "  461.6458435058594,\n",
              "  461.2136535644531,\n",
              "  463.3509826660156,\n",
              "  461.751708984375,\n",
              "  459.4270935058594,\n",
              "  459.1331787109375,\n",
              "  461.0,\n",
              "  459.1072692871094,\n",
              "  463.157470703125,\n",
              "  461.1136474609375,\n",
              "  456.919921875,\n",
              "  459.609375,\n",
              "  463.0998840332031,\n",
              "  459.6145935058594,\n",
              "  459.0196838378906,\n",
              "  459.3780212402344,\n",
              "  459.6145935058594,\n",
              "  463.1145935058594,\n",
              "  457.6647033691406,\n",
              "  461.176025390625,\n",
              "  461.3645935058594,\n",
              "  462.9955749511719,\n",
              "  460.9521484375,\n",
              "  460.9393615722656,\n",
              "  460.4889221191406,\n",
              "  463.2270812988281,\n",
              "  461.8263244628906,\n",
              "  459.3333435058594,\n",
              "  463.1910095214844,\n",
              "  463.3958435058594,\n",
              "  461.1343078613281,\n",
              "  463.3958435058594,\n",
              "  458.5157775878906,\n",
              "  461.0971984863281,\n",
              "  459.335205078125,\n",
              "  462.5408935546875,\n",
              "  462.7865295410156,\n",
              "  461.3645935058594,\n",
              "  459.0520935058594,\n",
              "  459.9570617675781,\n",
              "  460.4471435546875,\n",
              "  460.9974060058594,\n",
              "  463.0164489746094,\n",
              "  462.0503234863281,\n",
              "  462.3078308105469,\n",
              "  461.3645935058594,\n",
              "  458.9044494628906,\n",
              "  460.3611755371094,\n",
              "  461.3645935058594,\n",
              "  462.1114807128906,\n",
              "  457.3021240234375,\n",
              "  461.0,\n",
              "  460.5800476074219,\n",
              "  459.849609375,\n",
              "  456.0809326171875,\n",
              "  459.8744812011719,\n",
              "  460.5401611328125,\n",
              "  457.0208435058594,\n",
              "  459.0520935058594,\n",
              "  463.1145935058594,\n",
              "  462.4284973144531,\n",
              "  459.6145935058594,\n",
              "  459.3333435058594,\n",
              "  458.4126892089844,\n",
              "  462.4631652832031,\n",
              "  461.3645935058594,\n",
              "  463.9884338378906,\n",
              "  461.6458435058594,\n",
              "  459.9640197753906,\n",
              "  464.3883361816406,\n",
              "  463.6770935058594,\n",
              "  461.6427917480469,\n",
              "  459.0520935058594,\n",
              "  460.0098876953125,\n",
              "  460.3865661621094,\n",
              "  457.3020935058594,\n",
              "  459.7731018066406,\n",
              "  461.8080749511719,\n",
              "  461.1915588378906,\n",
              "  459.4193115234375,\n",
              "  459.3941345214844,\n",
              "  462.8201904296875,\n",
              "  461.3763427734375,\n",
              "  460.2149353027344,\n",
              "  459.6145324707031,\n",
              "  460.3963623046875,\n",
              "  461.914306640625,\n",
              "  460.2641906738281,\n",
              "  461.6458435058594,\n",
              "  458.8922424316406,\n",
              "  458.3664245605469,\n",
              "  459.3333435058594,\n",
              "  457.3020935058594,\n",
              "  460.10498046875,\n",
              "  459.3332824707031,\n",
              "  460.6427917480469,\n",
              "  461.313720703125,\n",
              "  462.3429260253906,\n",
              "  457.9869384765625,\n",
              "  460.287841796875,\n",
              "  459.3333435058594,\n",
              "  457.5833435058594,\n",
              "  461.0833435058594,\n",
              "  459.3333435058594,\n",
              "  461.1194152832031,\n",
              "  457.9009094238281,\n",
              "  460.7885437011719,\n",
              "  460.2024230957031,\n",
              "  462.2087707519531,\n",
              "  460.7157287597656,\n",
              "  458.66015625,\n",
              "  460.5570068359375,\n",
              "  464.912841796875,\n",
              "  460.0789489746094,\n",
              "  458.8431396484375,\n",
              "  460.0923767089844,\n",
              "  461.6458435058594,\n",
              "  459.7865905761719,\n",
              "  464.70458984375,\n",
              "  461.7942810058594,\n",
              "  460.4496765136719,\n",
              "  465.0220642089844,\n",
              "  460.0074768066406,\n",
              "  459.7146911621094,\n",
              "  459.702392578125,\n",
              "  463.0391845703125,\n",
              "  463.3958435058594,\n",
              "  457.3020324707031,\n",
              "  463.6770935058594,\n",
              "  463.0880432128906,\n",
              "  458.7498474121094,\n",
              "  459.898193359375,\n",
              "  466.6340637207031,\n",
              "  458.7930603027344,\n",
              "  459.5798034667969,\n",
              "  459.6145935058594,\n",
              "  461.0833435058594,\n",
              "  459.5429382324219,\n",
              "  457.9796447753906,\n",
              "  459.8009338378906,\n",
              "  459.7891845703125,\n",
              "  461.6458435058594,\n",
              "  457.6000061035156,\n",
              "  461.7823791503906,\n",
              "  463.1145935058594,\n",
              "  459.6145935058594,\n",
              "  461.3645324707031,\n",
              "  463.3958435058594,\n",
              "  457.3020935058594,\n",
              "  457.1508483886719,\n",
              "  457.355224609375,\n",
              "  459.3743591308594,\n",
              "  459.6145935058594,\n",
              "  459.9126892089844,\n",
              "  459.0565185546875,\n",
              "  459.0520935058594,\n",
              "  459.314453125,\n",
              "  459.6145935058594,\n",
              "  461.1263732910156,\n",
              "  461.0833435058594,\n",
              "  459.76123046875,\n",
              "  459.0520935058594,\n",
              "  457.4924621582031,\n",
              "  457.6599426269531,\n",
              "  459.1659851074219,\n",
              "  457.5833435058594,\n",
              "  461.2210998535156,\n",
              "  461.3645935058594,\n",
              "  463.2762451171875,\n",
              "  461.1908874511719,\n",
              "  459.5254821777344,\n",
              "  459.5303649902344,\n",
              "  461.4375,\n",
              "  457.3020935058594,\n",
              "  456.8368835449219,\n",
              "  461.3645935058594,\n",
              "  457.3276062011719,\n",
              "  460.8155822753906,\n",
              "  461.6458435058594,\n",
              "  459.3226013183594,\n",
              "  459.3557434082031,\n",
              "  459.3333740234375,\n",
              "  459.0065612792969,\n",
              "  459.3912353515625,\n",
              "  456.6697692871094,\n",
              "  456.9395751953125,\n",
              "  459.3333435058594,\n",
              "  459.4386291503906,\n",
              "  458.8186340332031]}"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "mae = history.history['mae']\n",
        "val_mae = history.history['val_mae']\n",
        "\n",
        "epochs_x = range(len(loss))\n",
        "\n",
        "plt.plot(epochs_x, mae, 'co', label='Training MAE')\n",
        "plt.plot(epochs_x, val_mae, 'k', label='Validation MAE')\n",
        "plt.title('Training and Mean squared error')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "f6d228b8-7831-44ad-a45f-9745aa9d0c76"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/regression.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/regression.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "3c3a10f6-fbf4-44e8-9d16-b59555384edb"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bd9abe3d-5d15-4a5c-8299-8ff7f15a5ebc\", \"regression.h5\", 16623752)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Retrain"
      ],
      "metadata": {
        "id": "owI9dHky6eHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# sys.path.append('/content/drive/My Drive/new/regression.h5')\n",
        "\n",
        "# from efficientnet.layers import Swish, DropConnect\n",
        "# from efficientnet.model import ConvKernalInitializer\n",
        "# from tensorflow.keras.utils import get_custom_objects\n",
        "\n",
        "# get_custom_objects().update({\n",
        "#     'ConvKernalInitializer': ConvKernalInitializer,\n",
        "#     'Swish': Swish,\n",
        "#     'DropConnect':DropConnect\n",
        "# })"
      ],
      "metadata": {
        "id": "bNX5VLJw54yc"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #load model \n",
        "# from tensorflow.keras.models import load_model\n",
        "# model = load_model('/content/drive/My Drive/new/regression.h5')\n",
        "# height = width = model.input_shape[1]"
      ],
      "metadata": {
        "id": "zF2vlFE_54wK"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # set 'multiply_16' and following layers trainable (Unfreeze --> multiply_16 ) ให้เป็น layers ที่ train ชุดข้อมูลใหม่\n",
        "# model.trainable = True\n",
        "\n",
        "# set_trainable = False\n",
        "# for layer in conv_base.layers:\n",
        "#     if layer.name == 'multiply_15':\n",
        "#         set_trainable = True\n",
        "#     if set_trainable:\n",
        "#         layer.trainable = True\n",
        "#     else:\n",
        "#         layer.trainable = False\n",
        "# print('This is the number of trainable layers '\n",
        "#       'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "1OsX50Tk54tx"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.summary()"
      ],
      "metadata": {
        "id": "pzptFLAz54rW"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.compile(loss='mse',\n",
        "#               optimizer=Adam(learning_rate=2e-1),\n",
        "#               metrics=['mae'])\n",
        "# history = model.fit_generator(\n",
        "#       train_generator,\n",
        "#       steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "#       epochs=epochs,\n",
        "#       validation_data=validation_generator,\n",
        "#       validation_steps= NUM_TEST //batch_size,\n",
        "#       verbose=1,\n",
        "#       use_multiprocessing=True,\n",
        "#       workers=4)"
      ],
      "metadata": {
        "id": "5os97-2Q54o2"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss = history.history['loss']\n",
        "# val_loss = history.history['val_loss']\n",
        "# mae = history.history['mae']\n",
        "# val_mae = history.history['val_mae']\n",
        "\n",
        "# epochs_x = range(len(loss))\n",
        "\n",
        "# plt.plot(epochs_x, mae, 'co', label='Training MAE')\n",
        "# plt.plot(epochs_x, val_mae, 'k', label='Validation MAE')\n",
        "# plt.title('Training and Mean squared error')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.figure()\n",
        "\n",
        "# plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "# plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "# plt.title('Training and validation loss')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "Z-9GR-Qp6lE3"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PW_313T26lCd"
      },
      "execution_count": 76,
      "outputs": []
    }
  ]
}