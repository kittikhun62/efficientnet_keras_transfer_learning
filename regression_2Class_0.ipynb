{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOOFSbB5xBAK8deFNt392tN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/regression_2Class_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "outputId": "5eed39c7-bb88-4d7f-f190-d65561756afc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - 2 class Regress.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "outputId": "ac99f355-7aa6-49fc-b7bc-390fca4fa58a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "795  796  1-s2.0-S2095268622000210-main   \n",
              "796  797  1-s2.0-S2095268622000210-main   \n",
              "797  798  1-s2.0-S2095268622000210-main   \n",
              "798  799  1-s2.0-S2095268622000210-main   \n",
              "799  800  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "795  Integration of preparation of K, Na-embedded a...   \n",
              "796  Integration of preparation of K, Na-embedded a...   \n",
              "797  Integration of preparation of K, Na-embedded a...   \n",
              "798  Integration of preparation of K, Na-embedded a...   \n",
              "799  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "795  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "796  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "797  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "798  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "799  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-800  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-800  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-800  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-800  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-800  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "795  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-800  301.70   \n",
              "796  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-800  301.70   \n",
              "797  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-800  301.70   \n",
              "798  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-800  301.70   \n",
              "799  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-800  301.70   \n",
              "\n",
              "     Size(mico)  Class_01  \n",
              "0            10         0  \n",
              "1            10         0  \n",
              "2            10         0  \n",
              "3            10         0  \n",
              "4            10         0  \n",
              "..          ...       ...  \n",
              "795          10         0  \n",
              "796          10         0  \n",
              "797          10         0  \n",
              "798          10         0  \n",
              "799          10         0  \n",
              "\n",
              "[800 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-93ad4e8c-00c5-468c-b0ca-b9cdc62d5d44\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "      <th>Class_01</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>796</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>797</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>798</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>799</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>800</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-93ad4e8c-00c5-468c-b0ca-b9cdc62d5d44')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-93ad4e8c-00c5-468c-b0ca-b9cdc62d5d44 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-93ad4e8c-00c5-468c-b0ca-b9cdc62d5d44');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "outputId": "5855d691-e56a-4b3f-821f-a4c347ad9b99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "outputId": "2a6c8e44-76d7-463a-dec0-7246993f2252",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-800','801-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "outputId": "93ebe078-ad2f-4e8e-d7b4-5a13e088a937",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# base_dir = '/content/drive/My Drive/new Regress'\n",
        "# os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# # Directories for our training,\n",
        "# # validation and test splits\n",
        "# train_dir = os.path.join(base_dir, 'train')\n",
        "# os.makedirs(train_dir, exist_ok=True)\n",
        "# validation_dir = os.path.join(base_dir, 'validation')\n",
        "# os.makedirs(validation_dir, exist_ok=True)\n",
        "# test_dir = os.path.join(base_dir, 'test')\n",
        "# os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(599,699)]\n",
        "train = df[df['No'].between(1,598)]\n",
        "test = df[df['No'].between(700,800)] \n",
        "\n",
        "# #Path Train\n",
        "# T1_train = train[train['Class']=='0-800']\n",
        "# T1_path_train = T1_train['path_Picture'].tolist() \n",
        "# T2_train = train[train['Class']=='801-3200']\n",
        "# T2_path_train = T2_train['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "# #Path Validation\n",
        "# T1_val = val[val['Class']=='0-800']\n",
        "# T1_path_val = T1_val['path_Picture'].tolist() \n",
        "# T2_val = val[val['Class']=='801-3200']\n",
        "# T2_path_val = T2_val['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "\n",
        "# #Path Test\n",
        "# T1_test = test[test['Class']=='0-800']\n",
        "# T1_path_test = T1_test['path_Picture'].tolist() \n",
        "# T2_test = test[test['Class']=='801-3200']\n",
        "# T2_path_test = T2_test['path_Picture'].tolist() \n"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "r9N_9sFL1hYB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/content/drive/My Drive/new Regress'\n",
        "os.chdir(DATA_PATH)\n",
        "train_dir = os.path.join(DATA_PATH, 'train')\n",
        "print(train_dir)\n",
        "validation_dir = os.path.join(DATA_PATH, 'validation')\n",
        "print(validation_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyxDPsEi6yYJ",
        "outputId": "6add626f-6739-4613-ff2f-34ae49284917"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/new Regress/train\n",
            "/content/drive/My Drive/new Regress/validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "# print('total training 2 images:', len(os.listdir(train_2_dir)),'\\n')\n",
        "\n",
        "# print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "# print('total validation 2 images:', len(os.listdir(validation_2_dir)),'\\n')\n",
        "\n",
        "# print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "# print('total test 2 images:', len(os.listdir(test_2_dir)),'\\n')\n"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "tYeLut2ByGeC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 500 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 598  # จำนวนภาพ Train\n",
        "NUM_TEST = 101 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.3\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "c5a97a28-e5d3-4110-8a76-1cf7bbe57814",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94758391-7be6-43d3-d3e0-f398ee7ec650"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "7cd404ad-ace7-4231-901d-a93d6d861af7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(2, activation='softmax', name=\"fc_out\"))\n",
        "model.add(layers.Dense(1))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "outputId": "30567b12-1a51-48f8-c20a-52a83b1ccccc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 2)                 2562      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,052,129\n",
            "Trainable params: 4,010,113\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "outputId": "0988922f-ca58-4410-d5aa-6fceddde8c71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 215\n",
            "This is the number of trainable layers after freezing the conv base: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe = train,\n",
        "        directory = train_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'BET',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)\n",
        "\n",
        "validation_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe = val,\n",
        "        directory = validation_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'BET',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "outputId": "ce7202cb-a199-4a5e-91ac-86be008cfc64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 598 validated image filenames.\n",
            "Found 101 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer=optimizers.RMSprop(learning_rate=0.001),\n",
        "              metrics=['mse'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad3ed6b4-2b92-433b-82d0-4644373434d3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-28-529d676723b0>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37/37 [==============================] - 17s 252ms/step - loss: 1528629.0000 - mse: 1528629.0000 - val_loss: 525391.0625 - val_mse: 525391.0625\n",
            "Epoch 2/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1509688.3750 - mse: 1509688.3750 - val_loss: 547107.8125 - val_mse: 547107.8125\n",
            "Epoch 3/500\n",
            "37/37 [==============================] - 8s 206ms/step - loss: 1500127.1250 - mse: 1500127.1250 - val_loss: 525229.2500 - val_mse: 525229.2500\n",
            "Epoch 4/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1510955.7500 - mse: 1510955.7500 - val_loss: 533920.9375 - val_mse: 533920.9375\n",
            "Epoch 5/500\n",
            "37/37 [==============================] - 3s 85ms/step - loss: 1503262.6250 - mse: 1503262.6250 - val_loss: 537977.0625 - val_mse: 537977.0625\n",
            "Epoch 6/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 1501160.8750 - mse: 1501160.8750 - val_loss: 529181.2500 - val_mse: 529181.2500\n",
            "Epoch 7/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1499390.2500 - mse: 1499390.2500 - val_loss: 516246.6562 - val_mse: 516246.6562\n",
            "Epoch 8/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1505702.3750 - mse: 1505702.3750 - val_loss: 524442.4375 - val_mse: 524442.4375\n",
            "Epoch 9/500\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 1492077.2500 - mse: 1492077.2500 - val_loss: 524798.3125 - val_mse: 524798.3125\n",
            "Epoch 10/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1491255.1250 - mse: 1491255.1250 - val_loss: 550412.6250 - val_mse: 550412.6250\n",
            "Epoch 11/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1508117.8750 - mse: 1508117.8750 - val_loss: 550384.3125 - val_mse: 550384.3125\n",
            "Epoch 12/500\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 1503721.0000 - mse: 1503721.0000 - val_loss: 524610.8125 - val_mse: 524610.8125\n",
            "Epoch 13/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1522541.6250 - mse: 1522541.6250 - val_loss: 524476.5000 - val_mse: 524476.5000\n",
            "Epoch 14/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1512657.8750 - mse: 1512657.8750 - val_loss: 515635.2188 - val_mse: 515635.2188\n",
            "Epoch 15/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1502469.7500 - mse: 1502469.7500 - val_loss: 550100.6250 - val_mse: 550100.6250\n",
            "Epoch 16/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1493018.0000 - mse: 1493018.0000 - val_loss: 541206.5000 - val_mse: 541206.5000\n",
            "Epoch 17/500\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 1481325.3750 - mse: 1481325.3750 - val_loss: 545689.1875 - val_mse: 545689.1875\n",
            "Epoch 18/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1503619.7500 - mse: 1503619.7500 - val_loss: 536961.3750 - val_mse: 536961.3750\n",
            "Epoch 19/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1521810.6250 - mse: 1521810.6250 - val_loss: 532743.0000 - val_mse: 532743.0000\n",
            "Epoch 20/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1503182.2500 - mse: 1503182.2500 - val_loss: 536742.6875 - val_mse: 536742.6875\n",
            "Epoch 21/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1495744.3750 - mse: 1495744.3750 - val_loss: 523827.8438 - val_mse: 523827.8438\n",
            "Epoch 22/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1495858.5000 - mse: 1495858.5000 - val_loss: 540711.5000 - val_mse: 540711.5000\n",
            "Epoch 23/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1500126.0000 - mse: 1500126.0000 - val_loss: 510839.5938 - val_mse: 510839.5938\n",
            "Epoch 24/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1516981.3750 - mse: 1516981.3750 - val_loss: 536519.6875 - val_mse: 536519.6875\n",
            "Epoch 25/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1522503.6250 - mse: 1522503.6250 - val_loss: 544968.6875 - val_mse: 544968.6875\n",
            "Epoch 26/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1509456.8750 - mse: 1509456.8750 - val_loss: 549124.5000 - val_mse: 549124.5000\n",
            "Epoch 27/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1489832.8750 - mse: 1489832.8750 - val_loss: 535536.8750 - val_mse: 535536.8750\n",
            "Epoch 28/500\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 1518410.7500 - mse: 1518410.7500 - val_loss: 544824.9375 - val_mse: 544824.9375\n",
            "Epoch 29/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1495376.1250 - mse: 1495376.1250 - val_loss: 557667.0625 - val_mse: 557667.0625\n",
            "Epoch 30/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 1485730.7500 - mse: 1485730.7500 - val_loss: 510230.5938 - val_mse: 510230.5938\n",
            "Epoch 31/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 1489052.1250 - mse: 1489052.1250 - val_loss: 535784.1875 - val_mse: 535784.1875\n",
            "Epoch 32/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1521650.2500 - mse: 1521650.2500 - val_loss: 518389.9688 - val_mse: 518389.9688\n",
            "Epoch 33/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 1498526.7500 - mse: 1498526.7500 - val_loss: 514179.3438 - val_mse: 514179.3438\n",
            "Epoch 34/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1510755.2500 - mse: 1510755.2500 - val_loss: 518230.6562 - val_mse: 518230.6562\n",
            "Epoch 35/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1494383.1250 - mse: 1494383.1250 - val_loss: 548265.1875 - val_mse: 548265.1875\n",
            "Epoch 36/500\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 1524380.3750 - mse: 1524380.3750 - val_loss: 522672.5938 - val_mse: 522672.5938\n",
            "Epoch 37/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1508602.2500 - mse: 1508602.2500 - val_loss: 535398.1250 - val_mse: 535398.1250\n",
            "Epoch 38/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1511140.1250 - mse: 1511140.1250 - val_loss: 526586.1875 - val_mse: 526586.1875\n",
            "Epoch 39/500\n",
            "37/37 [==============================] - 5s 103ms/step - loss: 1503043.7500 - mse: 1503043.7500 - val_loss: 525770.3125 - val_mse: 525770.3125\n",
            "Epoch 40/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1504875.0000 - mse: 1504875.0000 - val_loss: 539226.6875 - val_mse: 539226.6875\n",
            "Epoch 41/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1488796.0000 - mse: 1488796.0000 - val_loss: 530885.8125 - val_mse: 530885.8125\n",
            "Epoch 42/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1498296.0000 - mse: 1498296.0000 - val_loss: 530751.6250 - val_mse: 530751.6250\n",
            "Epoch 43/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1506481.1250 - mse: 1506481.1250 - val_loss: 522053.6562 - val_mse: 522053.6562\n",
            "Epoch 44/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1492161.3750 - mse: 1492161.3750 - val_loss: 509177.2500 - val_mse: 509177.2500\n",
            "Epoch 45/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1512968.7500 - mse: 1512968.7500 - val_loss: 538183.9375 - val_mse: 538183.9375\n",
            "Epoch 46/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1490919.7500 - mse: 1490919.7500 - val_loss: 534603.5000 - val_mse: 534603.5000\n",
            "Epoch 47/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 1508695.6250 - mse: 1508695.6250 - val_loss: 534574.8125 - val_mse: 534574.8125\n",
            "Epoch 48/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1477521.8750 - mse: 1477521.8750 - val_loss: 529903.5625 - val_mse: 529903.5625\n",
            "Epoch 49/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1495139.8750 - mse: 1495139.8750 - val_loss: 517033.7812 - val_mse: 517033.7812\n",
            "Epoch 50/500\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 1492927.8750 - mse: 1492927.8750 - val_loss: 555774.6875 - val_mse: 555774.6875\n",
            "Epoch 51/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1509470.6250 - mse: 1509470.6250 - val_loss: 534245.8125 - val_mse: 534245.8125\n",
            "Epoch 52/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1487933.5000 - mse: 1487933.5000 - val_loss: 542770.4375 - val_mse: 542770.4375\n",
            "Epoch 53/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1510761.6250 - mse: 1510761.6250 - val_loss: 534029.3750 - val_mse: 534029.3750\n",
            "Epoch 54/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1502340.6250 - mse: 1502340.6250 - val_loss: 529876.7500 - val_mse: 529876.7500\n",
            "Epoch 55/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1493644.3750 - mse: 1493644.3750 - val_loss: 512379.5938 - val_mse: 512379.5938\n",
            "Epoch 56/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1513874.7500 - mse: 1513874.7500 - val_loss: 529711.6875 - val_mse: 529711.6875\n",
            "Epoch 57/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1515227.7500 - mse: 1515227.7500 - val_loss: 520873.9688 - val_mse: 520873.9688\n",
            "Epoch 58/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1494322.5000 - mse: 1494322.5000 - val_loss: 532991.5625 - val_mse: 532991.5625\n",
            "Epoch 59/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 1490711.8750 - mse: 1490711.8750 - val_loss: 529011.1875 - val_mse: 529011.1875\n",
            "Epoch 60/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1508471.6250 - mse: 1508471.6250 - val_loss: 524756.4375 - val_mse: 524756.4375\n",
            "Epoch 61/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1510193.7500 - mse: 1510193.7500 - val_loss: 519469.9062 - val_mse: 519469.9062\n",
            "Epoch 62/500\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 1494230.6250 - mse: 1494230.6250 - val_loss: 511827.5938 - val_mse: 511827.5938\n",
            "Epoch 63/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1496258.3750 - mse: 1496258.3750 - val_loss: 515922.5000 - val_mse: 515922.5000\n",
            "Epoch 64/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1504785.0000 - mse: 1504785.0000 - val_loss: 528660.2500 - val_mse: 528660.2500\n",
            "Epoch 65/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1493366.3750 - mse: 1493366.3750 - val_loss: 533100.4375 - val_mse: 533100.4375\n",
            "Epoch 66/500\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1526155.1250 - mse: 1526155.1250 - val_loss: 515684.1250 - val_mse: 515684.1250\n",
            "Epoch 67/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1510726.0000 - mse: 1510726.0000 - val_loss: 520124.6562 - val_mse: 520124.6562\n",
            "Epoch 68/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1498396.2500 - mse: 1498396.2500 - val_loss: 541493.8125 - val_mse: 541493.8125\n",
            "Epoch 69/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 1505367.5000 - mse: 1505367.5000 - val_loss: 519515.2812 - val_mse: 519515.2812\n",
            "Epoch 70/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1494151.8750 - mse: 1494151.8750 - val_loss: 524003.5000 - val_mse: 524003.5000\n",
            "Epoch 71/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1480397.8750 - mse: 1480397.8750 - val_loss: 519802.1562 - val_mse: 519802.1562\n",
            "Epoch 72/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1494529.6250 - mse: 1494529.6250 - val_loss: 523789.9062 - val_mse: 523789.9062\n",
            "Epoch 73/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1515024.0000 - mse: 1515024.0000 - val_loss: 515128.0000 - val_mse: 515128.0000\n",
            "Epoch 74/500\n",
            "37/37 [==============================] - 6s 143ms/step - loss: 1503779.2500 - mse: 1503779.2500 - val_loss: 532413.8125 - val_mse: 532413.8125\n",
            "Epoch 75/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1492757.5000 - mse: 1492757.5000 - val_loss: 519533.2188 - val_mse: 519533.2188\n",
            "Epoch 76/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1503654.0000 - mse: 1503654.0000 - val_loss: 501411.4688 - val_mse: 501411.4688\n",
            "Epoch 77/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1502619.5000 - mse: 1502619.5000 - val_loss: 544914.5000 - val_mse: 544914.5000\n",
            "Epoch 78/500\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 1489551.6250 - mse: 1489551.6250 - val_loss: 519241.7188 - val_mse: 519241.7188\n",
            "Epoch 79/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1489618.0000 - mse: 1489618.0000 - val_loss: 523277.0938 - val_mse: 523277.0938\n",
            "Epoch 80/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1480119.0000 - mse: 1480119.0000 - val_loss: 531924.8125 - val_mse: 531924.8125\n",
            "Epoch 81/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1510000.2500 - mse: 1510000.2500 - val_loss: 540414.2500 - val_mse: 540414.2500\n",
            "Epoch 82/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1494374.5000 - mse: 1494374.5000 - val_loss: 514415.6562 - val_mse: 514415.6562\n",
            "Epoch 83/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1499982.8750 - mse: 1499982.8750 - val_loss: 522956.3438 - val_mse: 522956.3438\n",
            "Epoch 84/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1471517.2500 - mse: 1471517.2500 - val_loss: 518759.1250 - val_mse: 518759.1250\n",
            "Epoch 85/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1496390.0000 - mse: 1496390.0000 - val_loss: 531464.4375 - val_mse: 531464.4375\n",
            "Epoch 86/500\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1498364.7500 - mse: 1498364.7500 - val_loss: 518547.9062 - val_mse: 518547.9062\n",
            "Epoch 87/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1478507.1250 - mse: 1478507.1250 - val_loss: 531301.0625 - val_mse: 531301.0625\n",
            "Epoch 88/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1499512.2500 - mse: 1499512.2500 - val_loss: 531220.3750 - val_mse: 531220.3750\n",
            "Epoch 89/500\n",
            "37/37 [==============================] - 5s 96ms/step - loss: 1506111.1250 - mse: 1506111.1250 - val_loss: 501083.8750 - val_mse: 501083.8750\n",
            "Epoch 90/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1494742.1250 - mse: 1494742.1250 - val_loss: 522341.1250 - val_mse: 522341.1250\n",
            "Epoch 91/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1489595.5000 - mse: 1489595.5000 - val_loss: 525797.5625 - val_mse: 525797.5625\n",
            "Epoch 92/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1506258.7500 - mse: 1506258.7500 - val_loss: 543665.8125 - val_mse: 543665.8125\n",
            "Epoch 93/500\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 1494259.5000 - mse: 1494259.5000 - val_loss: 530810.4375 - val_mse: 530810.4375\n",
            "Epoch 94/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1491560.8750 - mse: 1491560.8750 - val_loss: 513414.9062 - val_mse: 513414.9062\n",
            "Epoch 95/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1506146.7500 - mse: 1506146.7500 - val_loss: 513818.2188 - val_mse: 513818.2188\n",
            "Epoch 96/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1491882.1250 - mse: 1491882.1250 - val_loss: 521962.5938 - val_mse: 521962.5938\n",
            "Epoch 97/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1487028.1250 - mse: 1487028.1250 - val_loss: 543147.6875 - val_mse: 543147.6875\n",
            "Epoch 98/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1491191.7500 - mse: 1491191.7500 - val_loss: 517588.2812 - val_mse: 517588.2812\n",
            "Epoch 99/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1495232.6250 - mse: 1495232.6250 - val_loss: 551633.4375 - val_mse: 551633.4375\n",
            "Epoch 100/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1501514.3750 - mse: 1501514.3750 - val_loss: 525020.5625 - val_mse: 525020.5625\n",
            "Epoch 101/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1515127.2500 - mse: 1515127.2500 - val_loss: 542816.1875 - val_mse: 542816.1875\n",
            "Epoch 102/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1486890.6250 - mse: 1486890.6250 - val_loss: 542835.3750 - val_mse: 542835.3750\n",
            "Epoch 103/500\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 1480692.3750 - mse: 1480692.3750 - val_loss: 517240.0312 - val_mse: 517240.0312\n",
            "Epoch 104/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1509546.5000 - mse: 1509546.5000 - val_loss: 533973.5000 - val_mse: 533973.5000\n",
            "Epoch 105/500\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 1509720.0000 - mse: 1509720.0000 - val_loss: 517129.5312 - val_mse: 517129.5312\n",
            "Epoch 106/500\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 1488570.2500 - mse: 1488570.2500 - val_loss: 504298.5312 - val_mse: 504298.5312\n",
            "Epoch 107/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1494960.0000 - mse: 1494960.0000 - val_loss: 516918.7812 - val_mse: 516918.7812\n",
            "Epoch 108/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1485822.3750 - mse: 1485822.3750 - val_loss: 512308.7188 - val_mse: 512308.7188\n",
            "Epoch 109/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1494806.8750 - mse: 1494806.8750 - val_loss: 512701.4688 - val_mse: 512701.4688\n",
            "Epoch 110/500\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 1473055.7500 - mse: 1473055.7500 - val_loss: 542070.0000 - val_mse: 542070.0000\n",
            "Epoch 111/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 1486762.5000 - mse: 1486762.5000 - val_loss: 520708.8750 - val_mse: 520708.8750\n",
            "Epoch 112/500\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 1489324.7500 - mse: 1489324.7500 - val_loss: 498623.5000 - val_mse: 498623.5000\n",
            "Epoch 113/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1498937.0000 - mse: 1498937.0000 - val_loss: 520496.8438 - val_mse: 520496.8438\n",
            "Epoch 114/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1480930.3750 - mse: 1480930.3750 - val_loss: 516310.8438 - val_mse: 516310.8438\n",
            "Epoch 115/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1494033.2500 - mse: 1494033.2500 - val_loss: 520337.7500 - val_mse: 520337.7500\n",
            "Epoch 116/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1492016.3750 - mse: 1492016.3750 - val_loss: 528887.3750 - val_mse: 528887.3750\n",
            "Epoch 117/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1484502.7500 - mse: 1484502.7500 - val_loss: 511650.4062 - val_mse: 511650.4062\n",
            "Epoch 118/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1512359.7500 - mse: 1512359.7500 - val_loss: 511520.4688 - val_mse: 511520.4688\n",
            "Epoch 119/500\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 1475121.6250 - mse: 1475121.6250 - val_loss: 541324.6250 - val_mse: 541324.6250\n",
            "Epoch 120/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1482869.2500 - mse: 1482869.2500 - val_loss: 511777.1250 - val_mse: 511777.1250\n",
            "Epoch 121/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1511496.1250 - mse: 1511496.1250 - val_loss: 515801.7188 - val_mse: 515801.7188\n",
            "Epoch 122/500\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 1506341.0000 - mse: 1506341.0000 - val_loss: 528448.6250 - val_mse: 528448.6250\n",
            "Epoch 123/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1479081.2500 - mse: 1479081.2500 - val_loss: 511175.2500 - val_mse: 511175.2500\n",
            "Epoch 124/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1494807.7500 - mse: 1494807.7500 - val_loss: 540330.7500 - val_mse: 540330.7500\n",
            "Epoch 125/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1491236.3750 - mse: 1491236.3750 - val_loss: 524050.9688 - val_mse: 524050.9688\n",
            "Epoch 126/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1493403.2500 - mse: 1493403.2500 - val_loss: 536740.9375 - val_mse: 536740.9375\n",
            "Epoch 127/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1476301.6250 - mse: 1476301.6250 - val_loss: 515323.7812 - val_mse: 515323.7812\n",
            "Epoch 128/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1478484.1250 - mse: 1478484.1250 - val_loss: 519298.3750 - val_mse: 519298.3750\n",
            "Epoch 129/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1501970.1250 - mse: 1501970.1250 - val_loss: 540496.1875 - val_mse: 540496.1875\n",
            "Epoch 130/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1477658.3750 - mse: 1477658.3750 - val_loss: 527849.9375 - val_mse: 527849.9375\n",
            "Epoch 131/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1508795.0000 - mse: 1508795.0000 - val_loss: 527718.6875 - val_mse: 527718.6875\n",
            "Epoch 132/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1491612.1250 - mse: 1491612.1250 - val_loss: 527588.4375 - val_mse: 527588.4375\n",
            "Epoch 133/500\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 1510363.7500 - mse: 1510363.7500 - val_loss: 523406.3750 - val_mse: 523406.3750\n",
            "Epoch 134/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1485216.5000 - mse: 1485216.5000 - val_loss: 506161.8438 - val_mse: 506161.8438\n",
            "Epoch 135/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1500590.7500 - mse: 1500590.7500 - val_loss: 510588.1250 - val_mse: 510588.1250\n",
            "Epoch 136/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1508262.8750 - mse: 1508262.8750 - val_loss: 523114.7500 - val_mse: 523114.7500\n",
            "Epoch 137/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 1497643.8750 - mse: 1497643.8750 - val_loss: 531284.0000 - val_mse: 531284.0000\n",
            "Epoch 138/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1503455.3750 - mse: 1503455.3750 - val_loss: 493198.0938 - val_mse: 493198.0938\n",
            "Epoch 139/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 1487220.3750 - mse: 1487220.3750 - val_loss: 522970.9062 - val_mse: 522970.9062\n",
            "Epoch 140/500\n",
            "37/37 [==============================] - 4s 78ms/step - loss: 1515510.1250 - mse: 1515510.1250 - val_loss: 514242.0938 - val_mse: 514242.0938\n",
            "Epoch 141/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 1490435.3750 - mse: 1490435.3750 - val_loss: 530959.3125 - val_mse: 530959.3125\n",
            "Epoch 142/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 1491967.6250 - mse: 1491967.6250 - val_loss: 505536.5938 - val_mse: 505536.5938\n",
            "Epoch 143/500\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 1476237.6250 - mse: 1476237.6250 - val_loss: 514103.8438 - val_mse: 514103.8438\n",
            "Epoch 144/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1502547.7500 - mse: 1502547.7500 - val_loss: 539261.5625 - val_mse: 539261.5625\n",
            "Epoch 145/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1482334.3750 - mse: 1482334.3750 - val_loss: 522487.5000 - val_mse: 522487.5000\n",
            "Epoch 146/500\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 1486679.6250 - mse: 1486679.6250 - val_loss: 526407.1250 - val_mse: 526407.1250\n",
            "Epoch 147/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1488601.1250 - mse: 1488601.1250 - val_loss: 526326.1250 - val_mse: 526326.1250\n",
            "Epoch 148/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1448970.2500 - mse: 1448970.2500 - val_loss: 538979.1250 - val_mse: 538979.1250\n",
            "Epoch 149/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1488329.5000 - mse: 1488329.5000 - val_loss: 517625.3438 - val_mse: 517625.3438\n",
            "Epoch 150/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1472313.0000 - mse: 1472313.0000 - val_loss: 538765.0625 - val_mse: 538765.0625\n",
            "Epoch 151/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1490154.3750 - mse: 1490154.3750 - val_loss: 504881.7500 - val_mse: 504881.7500\n",
            "Epoch 152/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1513997.1250 - mse: 1513997.1250 - val_loss: 517384.7500 - val_mse: 517384.7500\n",
            "Epoch 153/500\n",
            "37/37 [==============================] - 9s 215ms/step - loss: 1488331.0000 - mse: 1488331.0000 - val_loss: 529984.0625 - val_mse: 529984.0625\n",
            "Epoch 154/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1497997.6250 - mse: 1497997.6250 - val_loss: 508016.3750 - val_mse: 508016.3750\n",
            "Epoch 155/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1515423.2500 - mse: 1515423.2500 - val_loss: 513050.3750 - val_mse: 513050.3750\n",
            "Epoch 156/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1490263.0000 - mse: 1490263.0000 - val_loss: 508584.8750 - val_mse: 508584.8750\n",
            "Epoch 157/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1501862.6250 - mse: 1501862.6250 - val_loss: 517035.6562 - val_mse: 517035.6562\n",
            "Epoch 158/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1485349.0000 - mse: 1485349.0000 - val_loss: 521343.0000 - val_mse: 521343.0000\n",
            "Epoch 159/500\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 1503998.8750 - mse: 1503998.8750 - val_loss: 512782.4062 - val_mse: 512782.4062\n",
            "Epoch 160/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1462573.7500 - mse: 1462573.7500 - val_loss: 500084.9688 - val_mse: 500084.9688\n",
            "Epoch 161/500\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 1488171.3750 - mse: 1488171.3750 - val_loss: 525241.3125 - val_mse: 525241.3125\n",
            "Epoch 162/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1468983.2500 - mse: 1468983.2500 - val_loss: 521115.6562 - val_mse: 521115.6562\n",
            "Epoch 163/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1484228.6250 - mse: 1484228.6250 - val_loss: 533650.1875 - val_mse: 533650.1875\n",
            "Epoch 164/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1472010.5000 - mse: 1472010.5000 - val_loss: 512339.6562 - val_mse: 512339.6562\n",
            "Epoch 165/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1482237.5000 - mse: 1482237.5000 - val_loss: 512307.7188 - val_mse: 512307.7188\n",
            "Epoch 166/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1485059.1250 - mse: 1485059.1250 - val_loss: 503663.6250 - val_mse: 503663.6250\n",
            "Epoch 167/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1490888.5000 - mse: 1490888.5000 - val_loss: 520283.4062 - val_mse: 520283.4062\n",
            "Epoch 168/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1455787.7500 - mse: 1455787.7500 - val_loss: 520633.7500 - val_mse: 520633.7500\n",
            "Epoch 169/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 1490143.0000 - mse: 1490143.0000 - val_loss: 507899.4062 - val_mse: 507899.4062\n",
            "Epoch 170/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1482362.2500 - mse: 1482362.2500 - val_loss: 516001.6250 - val_mse: 516001.6250\n",
            "Epoch 171/500\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 1482357.7500 - mse: 1482357.7500 - val_loss: 545642.2500 - val_mse: 545642.2500\n",
            "Epoch 172/500\n",
            "37/37 [==============================] - 9s 214ms/step - loss: 1496741.0000 - mse: 1496741.0000 - val_loss: 528490.9375 - val_mse: 528490.9375\n",
            "Epoch 173/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1473908.6250 - mse: 1473908.6250 - val_loss: 527640.1875 - val_mse: 527640.1875\n",
            "Epoch 174/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1495965.3750 - mse: 1495965.3750 - val_loss: 532701.0000 - val_mse: 532701.0000\n",
            "Epoch 175/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1504172.5000 - mse: 1504172.5000 - val_loss: 520070.8750 - val_mse: 520070.8750\n",
            "Epoch 176/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1493428.0000 - mse: 1493428.0000 - val_loss: 515572.0938 - val_mse: 515572.0938\n",
            "Epoch 177/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1493727.8750 - mse: 1493727.8750 - val_loss: 515446.4688 - val_mse: 515446.4688\n",
            "Epoch 178/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1510598.1250 - mse: 1510598.1250 - val_loss: 523916.8438 - val_mse: 523916.8438\n",
            "Epoch 179/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1489569.2500 - mse: 1489569.2500 - val_loss: 518699.0000 - val_mse: 518699.0000\n",
            "Epoch 180/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1473811.7500 - mse: 1473811.7500 - val_loss: 511167.0000 - val_mse: 511167.0000\n",
            "Epoch 181/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1490158.7500 - mse: 1490158.7500 - val_loss: 515128.0938 - val_mse: 515128.0938\n",
            "Epoch 182/500\n",
            "37/37 [==============================] - 8s 224ms/step - loss: 1494414.1250 - mse: 1494414.1250 - val_loss: 515048.6562 - val_mse: 515048.6562\n",
            "Epoch 183/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1463198.3750 - mse: 1463198.3750 - val_loss: 510883.1562 - val_mse: 510883.1562\n",
            "Epoch 184/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1490890.3750 - mse: 1490890.3750 - val_loss: 531883.0000 - val_mse: 531883.0000\n",
            "Epoch 185/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1491083.7500 - mse: 1491083.7500 - val_loss: 493689.0938 - val_mse: 493689.0938\n",
            "Epoch 186/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1494020.1250 - mse: 1494020.1250 - val_loss: 531858.9375 - val_mse: 531858.9375\n",
            "Epoch 187/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1472620.8750 - mse: 1472620.8750 - val_loss: 514652.2812 - val_mse: 514652.2812\n",
            "Epoch 188/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1480454.2500 - mse: 1480454.2500 - val_loss: 501274.0312 - val_mse: 501274.0312\n",
            "Epoch 189/500\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1471838.5000 - mse: 1471838.5000 - val_loss: 522982.5312 - val_mse: 522982.5312\n",
            "Epoch 190/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1490418.0000 - mse: 1490418.0000 - val_loss: 506198.4688 - val_mse: 506198.4688\n",
            "Epoch 191/500\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 1494680.3750 - mse: 1494680.3750 - val_loss: 509574.8438 - val_mse: 509574.8438\n",
            "Epoch 192/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1478535.6250 - mse: 1478535.6250 - val_loss: 501684.7188 - val_mse: 501684.7188\n",
            "Epoch 193/500\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 1477741.1250 - mse: 1477741.1250 - val_loss: 501560.0938 - val_mse: 501560.0938\n",
            "Epoch 194/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1469900.7500 - mse: 1469900.7500 - val_loss: 526709.5625 - val_mse: 526709.5625\n",
            "Epoch 195/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1481722.0000 - mse: 1481722.0000 - val_loss: 522546.7812 - val_mse: 522546.7812\n",
            "Epoch 196/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 1477737.8750 - mse: 1477737.8750 - val_loss: 518430.1562 - val_mse: 518430.1562\n",
            "Epoch 197/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1479074.0000 - mse: 1479074.0000 - val_loss: 517941.2812 - val_mse: 517941.2812\n",
            "Epoch 198/500\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 1488679.1250 - mse: 1488679.1250 - val_loss: 534914.2500 - val_mse: 534914.2500\n",
            "Epoch 199/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1475048.0000 - mse: 1475048.0000 - val_loss: 526261.5625 - val_mse: 526261.5625\n",
            "Epoch 200/500\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 1465777.5000 - mse: 1465777.5000 - val_loss: 501064.7500 - val_mse: 501064.7500\n",
            "Epoch 201/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 1476398.7500 - mse: 1476398.7500 - val_loss: 505069.3438 - val_mse: 505069.3438\n",
            "Epoch 202/500\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 1466995.6250 - mse: 1466995.6250 - val_loss: 543152.7500 - val_mse: 543152.7500\n",
            "Epoch 203/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1484137.3750 - mse: 1484137.3750 - val_loss: 509304.7500 - val_mse: 509304.7500\n",
            "Epoch 204/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1463722.6250 - mse: 1463722.6250 - val_loss: 513305.7812 - val_mse: 513305.7812\n",
            "Epoch 205/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1476893.6250 - mse: 1476893.6250 - val_loss: 509100.8438 - val_mse: 509100.8438\n",
            "Epoch 206/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1487317.0000 - mse: 1487317.0000 - val_loss: 513146.5938 - val_mse: 513146.5938\n",
            "Epoch 207/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1488002.6250 - mse: 1488002.6250 - val_loss: 496441.0000 - val_mse: 496441.0000\n",
            "Epoch 208/500\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 1482781.2500 - mse: 1482781.2500 - val_loss: 508909.3438 - val_mse: 508909.3438\n",
            "Epoch 209/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1482841.3750 - mse: 1482841.3750 - val_loss: 491853.8438 - val_mse: 491853.8438\n",
            "Epoch 210/500\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 1481274.8750 - mse: 1481274.8750 - val_loss: 521340.0312 - val_mse: 521340.0312\n",
            "Epoch 211/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 1462258.3750 - mse: 1462258.3750 - val_loss: 500208.9062 - val_mse: 500208.9062\n",
            "Epoch 212/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1456012.3750 - mse: 1456012.3750 - val_loss: 516749.2188 - val_mse: 516749.2188\n",
            "Epoch 213/500\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 1482880.6250 - mse: 1482880.6250 - val_loss: 521098.2188 - val_mse: 521098.2188\n",
            "Epoch 214/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1495483.7500 - mse: 1495483.7500 - val_loss: 508436.2500 - val_mse: 508436.2500\n",
            "Epoch 215/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1470030.3750 - mse: 1470030.3750 - val_loss: 483239.6250 - val_mse: 483239.6250\n",
            "Epoch 216/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1484838.5000 - mse: 1484838.5000 - val_loss: 507605.1250 - val_mse: 507605.1250\n",
            "Epoch 217/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1487139.1250 - mse: 1487139.1250 - val_loss: 524898.8125 - val_mse: 524898.8125\n",
            "Epoch 218/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1452664.1250 - mse: 1452664.1250 - val_loss: 486417.4062 - val_mse: 486417.4062\n",
            "Epoch 219/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 1460314.8750 - mse: 1460314.8750 - val_loss: 512071.2812 - val_mse: 512071.2812\n",
            "Epoch 220/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 1468410.0000 - mse: 1468410.0000 - val_loss: 499510.9688 - val_mse: 499510.9688\n",
            "Epoch 221/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1471140.1250 - mse: 1471140.1250 - val_loss: 507928.9062 - val_mse: 507928.9062\n",
            "Epoch 222/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1494678.1250 - mse: 1494678.1250 - val_loss: 503430.5938 - val_mse: 503430.5938\n",
            "Epoch 223/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1483518.0000 - mse: 1483518.0000 - val_loss: 520248.9062 - val_mse: 520248.9062\n",
            "Epoch 224/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 1480820.3750 - mse: 1480820.3750 - val_loss: 507693.0000 - val_mse: 507693.0000\n",
            "Epoch 225/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 1483200.5000 - mse: 1483200.5000 - val_loss: 511642.7188 - val_mse: 511642.7188\n",
            "Epoch 226/500\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 1485011.1250 - mse: 1485011.1250 - val_loss: 520052.9062 - val_mse: 520052.9062\n",
            "Epoch 227/500\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 1480098.3750 - mse: 1480098.3750 - val_loss: 519929.5938 - val_mse: 519929.5938\n",
            "Epoch 228/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1452909.2500 - mse: 1452909.2500 - val_loss: 511453.5312 - val_mse: 511453.5312\n",
            "Epoch 229/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 1474982.0000 - mse: 1474982.0000 - val_loss: 532418.0625 - val_mse: 532418.0625\n",
            "Epoch 230/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1479444.2500 - mse: 1479444.2500 - val_loss: 502764.5000 - val_mse: 502764.5000\n",
            "Epoch 231/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1468698.7500 - mse: 1468698.7500 - val_loss: 519654.1562 - val_mse: 519654.1562\n",
            "Epoch 232/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1486856.7500 - mse: 1486856.7500 - val_loss: 498538.3438 - val_mse: 498538.3438\n",
            "Epoch 233/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1489156.8750 - mse: 1489156.8750 - val_loss: 527974.6250 - val_mse: 527974.6250\n",
            "Epoch 234/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1461193.3750 - mse: 1461193.3750 - val_loss: 510890.0000 - val_mse: 510890.0000\n",
            "Epoch 235/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1484774.2500 - mse: 1484774.2500 - val_loss: 489873.9688 - val_mse: 489873.9688\n",
            "Epoch 236/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 1474316.3750 - mse: 1474316.3750 - val_loss: 490132.1250 - val_mse: 490132.1250\n",
            "Epoch 237/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 1463374.2500 - mse: 1463374.2500 - val_loss: 509981.8438 - val_mse: 509981.8438\n",
            "Epoch 238/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1454224.8750 - mse: 1454224.8750 - val_loss: 531639.6875 - val_mse: 531639.6875\n",
            "Epoch 239/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1470731.3750 - mse: 1470731.3750 - val_loss: 502445.7188 - val_mse: 502445.7188\n",
            "Epoch 240/500\n",
            "37/37 [==============================] - 9s 216ms/step - loss: 1481275.8750 - mse: 1481275.8750 - val_loss: 518889.5000 - val_mse: 518889.5000\n",
            "Epoch 241/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1462824.1250 - mse: 1462824.1250 - val_loss: 514784.8438 - val_mse: 514784.8438\n",
            "Epoch 242/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1485281.6250 - mse: 1485281.6250 - val_loss: 531222.6875 - val_mse: 531222.6875\n",
            "Epoch 243/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1471474.5000 - mse: 1471474.5000 - val_loss: 518648.1562 - val_mse: 518648.1562\n",
            "Epoch 244/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1460372.0000 - mse: 1460372.0000 - val_loss: 522637.3750 - val_mse: 522637.3750\n",
            "Epoch 245/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1495933.1250 - mse: 1495933.1250 - val_loss: 497580.0000 - val_mse: 497580.0000\n",
            "Epoch 246/500\n",
            "37/37 [==============================] - 9s 216ms/step - loss: 1474151.6250 - mse: 1474151.6250 - val_loss: 518409.2188 - val_mse: 518409.2188\n",
            "Epoch 247/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1468483.2500 - mse: 1468483.2500 - val_loss: 526747.3125 - val_mse: 526747.3125\n",
            "Epoch 248/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1474392.7500 - mse: 1474392.7500 - val_loss: 513898.7500 - val_mse: 513898.7500\n",
            "Epoch 249/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1494701.2500 - mse: 1494701.2500 - val_loss: 501662.6250 - val_mse: 501662.6250\n",
            "Epoch 250/500\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 1461774.5000 - mse: 1461774.5000 - val_loss: 501259.7812 - val_mse: 501259.7812\n",
            "Epoch 251/500\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 1470623.3750 - mse: 1470623.3750 - val_loss: 509552.1562 - val_mse: 509552.1562\n",
            "Epoch 252/500\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 1443984.6250 - mse: 1443984.6250 - val_loss: 526342.8125 - val_mse: 526342.8125\n",
            "Epoch 253/500\n",
            "37/37 [==============================] - 4s 110ms/step - loss: 1472046.7500 - mse: 1472046.7500 - val_loss: 517850.0312 - val_mse: 517850.0312\n",
            "Epoch 254/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1477265.2500 - mse: 1477265.2500 - val_loss: 530290.1875 - val_mse: 530290.1875\n",
            "Epoch 255/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1467429.0000 - mse: 1467429.0000 - val_loss: 509281.6250 - val_mse: 509281.6250\n",
            "Epoch 256/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1471277.5000 - mse: 1471277.5000 - val_loss: 521718.3438 - val_mse: 521718.3438\n",
            "Epoch 257/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1476529.7500 - mse: 1476529.7500 - val_loss: 521639.1250 - val_mse: 521639.1250\n",
            "Epoch 258/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1469410.0000 - mse: 1469410.0000 - val_loss: 513387.0000 - val_mse: 513387.0000\n",
            "Epoch 259/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1471914.5000 - mse: 1471914.5000 - val_loss: 495829.0000 - val_mse: 495829.0000\n",
            "Epoch 260/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1482840.7500 - mse: 1482840.7500 - val_loss: 500804.7812 - val_mse: 500804.7812\n",
            "Epoch 261/500\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 1468837.2500 - mse: 1468837.2500 - val_loss: 492282.6562 - val_mse: 492282.6562\n",
            "Epoch 262/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1457142.3750 - mse: 1457142.3750 - val_loss: 492161.8438 - val_mse: 492161.8438\n",
            "Epoch 263/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1484307.7500 - mse: 1484307.7500 - val_loss: 525451.8750 - val_mse: 525451.8750\n",
            "Epoch 264/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1480178.8750 - mse: 1480178.8750 - val_loss: 496114.9062 - val_mse: 496114.9062\n",
            "Epoch 265/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1461881.0000 - mse: 1461881.0000 - val_loss: 508496.7188 - val_mse: 508496.7188\n",
            "Epoch 266/500\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 1463292.7500 - mse: 1463292.7500 - val_loss: 499979.4688 - val_mse: 499979.4688\n",
            "Epoch 267/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1475698.5000 - mse: 1475698.5000 - val_loss: 516779.0938 - val_mse: 516779.0938\n",
            "Epoch 268/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1491431.2500 - mse: 1491431.2500 - val_loss: 504201.4688 - val_mse: 504201.4688\n",
            "Epoch 269/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1479065.1250 - mse: 1479065.1250 - val_loss: 537420.5000 - val_mse: 537420.5000\n",
            "Epoch 270/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1487667.0000 - mse: 1487667.0000 - val_loss: 508151.2500 - val_mse: 508151.2500\n",
            "Epoch 271/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1473744.0000 - mse: 1473744.0000 - val_loss: 537343.5625 - val_mse: 537343.5625\n",
            "Epoch 272/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1466243.6250 - mse: 1466243.6250 - val_loss: 516337.6562 - val_mse: 516337.6562\n",
            "Epoch 273/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1472114.2500 - mse: 1472114.2500 - val_loss: 512198.5312 - val_mse: 512198.5312\n",
            "Epoch 274/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1471553.3750 - mse: 1471553.3750 - val_loss: 503733.9688 - val_mse: 503733.9688\n",
            "Epoch 275/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1482746.6250 - mse: 1482746.6250 - val_loss: 507714.1250 - val_mse: 507714.1250\n",
            "Epoch 276/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1482157.0000 - mse: 1482157.0000 - val_loss: 482710.7188 - val_mse: 482710.7188\n",
            "Epoch 277/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1472240.7500 - mse: 1472240.7500 - val_loss: 520043.4688 - val_mse: 520043.4688\n",
            "Epoch 278/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 1469406.5000 - mse: 1469406.5000 - val_loss: 515861.5938 - val_mse: 515861.5938\n",
            "Epoch 279/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1482885.7500 - mse: 1482885.7500 - val_loss: 528260.9375 - val_mse: 528260.9375\n",
            "Epoch 280/500\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 1467693.0000 - mse: 1467693.0000 - val_loss: 503308.4688 - val_mse: 503308.4688\n",
            "Epoch 281/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1468893.2500 - mse: 1468893.2500 - val_loss: 528100.1875 - val_mse: 528100.1875\n",
            "Epoch 282/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1468128.3750 - mse: 1468128.3750 - val_loss: 515542.5938 - val_mse: 515542.5938\n",
            "Epoch 283/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1477911.2500 - mse: 1477911.2500 - val_loss: 511407.1562 - val_mse: 511407.1562\n",
            "Epoch 284/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 1470293.5000 - mse: 1470293.5000 - val_loss: 527856.0000 - val_mse: 527856.0000\n",
            "Epoch 285/500\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 1472771.2500 - mse: 1472771.2500 - val_loss: 502833.4062 - val_mse: 502833.4062\n",
            "Epoch 286/500\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 1446436.3750 - mse: 1446436.3750 - val_loss: 523639.1562 - val_mse: 523639.1562\n",
            "Epoch 287/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1484476.0000 - mse: 1484476.0000 - val_loss: 490252.2188 - val_mse: 490252.2188\n",
            "Epoch 288/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1443784.8750 - mse: 1443784.8750 - val_loss: 510795.1562 - val_mse: 510795.1562\n",
            "Epoch 289/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1476398.1250 - mse: 1476398.1250 - val_loss: 514987.2812 - val_mse: 514987.2812\n",
            "Epoch 290/500\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 1465378.6250 - mse: 1465378.6250 - val_loss: 523402.0312 - val_mse: 523402.0312\n",
            "Epoch 291/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1477580.1250 - mse: 1477580.1250 - val_loss: 498055.3750 - val_mse: 498055.3750\n",
            "Epoch 292/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1470515.6250 - mse: 1470515.6250 - val_loss: 498276.0312 - val_mse: 498276.0312\n",
            "Epoch 293/500\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 1462848.2500 - mse: 1462848.2500 - val_loss: 518765.8438 - val_mse: 518765.8438\n",
            "Epoch 294/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1476201.6250 - mse: 1476201.6250 - val_loss: 506228.6250 - val_mse: 506228.6250\n",
            "Epoch 295/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1480765.3750 - mse: 1480765.3750 - val_loss: 493738.1250 - val_mse: 493738.1250\n",
            "Epoch 296/500\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 1457967.8750 - mse: 1457967.8750 - val_loss: 506073.3750 - val_mse: 506073.3750\n",
            "Epoch 297/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1466725.0000 - mse: 1466725.0000 - val_loss: 514394.3438 - val_mse: 514394.3438\n",
            "Epoch 298/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1473465.8750 - mse: 1473465.8750 - val_loss: 514357.7500 - val_mse: 514357.7500\n",
            "Epoch 299/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1474206.1250 - mse: 1474206.1250 - val_loss: 505839.8438 - val_mse: 505839.8438\n",
            "Epoch 300/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1452747.2500 - mse: 1452747.2500 - val_loss: 505049.0000 - val_mse: 505049.0000\n",
            "Epoch 301/500\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1464201.0000 - mse: 1464201.0000 - val_loss: 497538.1250 - val_mse: 497538.1250\n",
            "Epoch 302/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1444396.1250 - mse: 1444396.1250 - val_loss: 526401.5625 - val_mse: 526401.5625\n",
            "Epoch 303/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1450411.3750 - mse: 1450411.3750 - val_loss: 526278.1875 - val_mse: 526278.1875\n",
            "Epoch 304/500\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 1454778.1250 - mse: 1454778.1250 - val_loss: 517849.3438 - val_mse: 517849.3438\n",
            "Epoch 305/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1469939.7500 - mse: 1469939.7500 - val_loss: 480534.7188 - val_mse: 480534.7188\n",
            "Epoch 306/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1449420.6250 - mse: 1449420.6250 - val_loss: 509631.3438 - val_mse: 509631.3438\n",
            "Epoch 307/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1482858.5000 - mse: 1482858.5000 - val_loss: 525954.3750 - val_mse: 525954.3750\n",
            "Epoch 308/500\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1470766.1250 - mse: 1470766.1250 - val_loss: 472209.9062 - val_mse: 472209.9062\n",
            "Epoch 309/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1462199.3750 - mse: 1462199.3750 - val_loss: 496674.7812 - val_mse: 496674.7812\n",
            "Epoch 310/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1462303.3750 - mse: 1462303.3750 - val_loss: 476109.1562 - val_mse: 476109.1562\n",
            "Epoch 311/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1462002.7500 - mse: 1462002.7500 - val_loss: 509192.8750 - val_mse: 509192.8750\n",
            "Epoch 312/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1456415.2500 - mse: 1456415.2500 - val_loss: 492436.5312 - val_mse: 492436.5312\n",
            "Epoch 313/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1467708.6250 - mse: 1467708.6250 - val_loss: 513042.0312 - val_mse: 513042.0312\n",
            "Epoch 314/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1478342.2500 - mse: 1478342.2500 - val_loss: 492241.8438 - val_mse: 492241.8438\n",
            "Epoch 315/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1481763.6250 - mse: 1481763.6250 - val_loss: 517013.2500 - val_mse: 517013.2500\n",
            "Epoch 316/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1441692.7500 - mse: 1441692.7500 - val_loss: 500464.7188 - val_mse: 500464.7188\n",
            "Epoch 317/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1455547.5000 - mse: 1455547.5000 - val_loss: 516229.5938 - val_mse: 516229.5938\n",
            "Epoch 318/500\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 1464164.1250 - mse: 1464164.1250 - val_loss: 525066.2500 - val_mse: 525066.2500\n",
            "Epoch 319/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1471124.1250 - mse: 1471124.1250 - val_loss: 512651.5312 - val_mse: 512651.5312\n",
            "Epoch 320/500\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 1455486.3750 - mse: 1455486.3750 - val_loss: 524986.7500 - val_mse: 524986.7500\n",
            "Epoch 321/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1441951.5000 - mse: 1441951.5000 - val_loss: 504083.4688 - val_mse: 504083.4688\n",
            "Epoch 322/500\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 1483067.1250 - mse: 1483067.1250 - val_loss: 512412.2500 - val_mse: 512412.2500\n",
            "Epoch 323/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1456591.6250 - mse: 1456591.6250 - val_loss: 503926.5312 - val_mse: 503926.5312\n",
            "Epoch 324/500\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 1459489.7500 - mse: 1459489.7500 - val_loss: 512254.8438 - val_mse: 512254.8438\n",
            "Epoch 325/500\n",
            "37/37 [==============================] - 6s 135ms/step - loss: 1464084.8750 - mse: 1464084.8750 - val_loss: 508046.8438 - val_mse: 508046.8438\n",
            "Epoch 326/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1469073.3750 - mse: 1469073.3750 - val_loss: 512095.7812 - val_mse: 512095.7812\n",
            "Epoch 327/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1468421.0000 - mse: 1468421.0000 - val_loss: 487247.5938 - val_mse: 487247.5938\n",
            "Epoch 328/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1474352.7500 - mse: 1474352.7500 - val_loss: 495491.5938 - val_mse: 495491.5938\n",
            "Epoch 329/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1458574.5000 - mse: 1458574.5000 - val_loss: 491139.9062 - val_mse: 491139.9062\n",
            "Epoch 330/500\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1458686.0000 - mse: 1458686.0000 - val_loss: 503464.6250 - val_mse: 503464.6250\n",
            "Epoch 331/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1476374.1250 - mse: 1476374.1250 - val_loss: 507658.7812 - val_mse: 507658.7812\n",
            "Epoch 332/500\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 1457157.3750 - mse: 1457157.3750 - val_loss: 511623.2188 - val_mse: 511623.2188\n",
            "Epoch 333/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1462553.1250 - mse: 1462553.1250 - val_loss: 511502.6562 - val_mse: 511502.6562\n",
            "Epoch 334/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 1457947.6250 - mse: 1457947.6250 - val_loss: 503072.1250 - val_mse: 503072.1250\n",
            "Epoch 335/500\n",
            "37/37 [==============================] - 10s 238ms/step - loss: 1464159.6250 - mse: 1464159.6250 - val_loss: 490642.6250 - val_mse: 490642.6250\n",
            "Epoch 336/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1475276.0000 - mse: 1475276.0000 - val_loss: 511264.5312 - val_mse: 511264.5312\n",
            "Epoch 337/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1469097.0000 - mse: 1469097.0000 - val_loss: 502877.8750 - val_mse: 502877.8750\n",
            "Epoch 338/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1454435.3750 - mse: 1454435.3750 - val_loss: 511107.0000 - val_mse: 511107.0000\n",
            "Epoch 339/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 1435807.1250 - mse: 1435807.1250 - val_loss: 515150.8438 - val_mse: 515150.8438\n",
            "Epoch 340/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 1453657.1250 - mse: 1453657.1250 - val_loss: 494342.0938 - val_mse: 494342.0938\n",
            "Epoch 341/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1471220.0000 - mse: 1471220.0000 - val_loss: 510912.0000 - val_mse: 510912.0000\n",
            "Epoch 342/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1450739.0000 - mse: 1450739.0000 - val_loss: 502490.4062 - val_mse: 502490.4062\n",
            "Epoch 343/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1472104.5000 - mse: 1472104.5000 - val_loss: 502412.5938 - val_mse: 502412.5938\n",
            "Epoch 344/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1455508.5000 - mse: 1455508.5000 - val_loss: 519054.4062 - val_mse: 519054.4062\n",
            "Epoch 345/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1455603.6250 - mse: 1455603.6250 - val_loss: 506295.6562 - val_mse: 506295.6562\n",
            "Epoch 346/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1452373.0000 - mse: 1452373.0000 - val_loss: 506437.5000 - val_mse: 506437.5000\n",
            "Epoch 347/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1485516.3750 - mse: 1485516.3750 - val_loss: 494024.3750 - val_mse: 494024.3750\n",
            "Epoch 348/500\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 1421529.2500 - mse: 1421529.2500 - val_loss: 509692.6562 - val_mse: 509692.6562\n",
            "Epoch 349/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1458349.3750 - mse: 1458349.3750 - val_loss: 510239.2500 - val_mse: 510239.2500\n",
            "Epoch 350/500\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 1466016.7500 - mse: 1466016.7500 - val_loss: 501868.6250 - val_mse: 501868.6250\n",
            "Epoch 351/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1465919.5000 - mse: 1465919.5000 - val_loss: 510082.5938 - val_mse: 510082.5938\n",
            "Epoch 352/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 1475963.0000 - mse: 1475963.0000 - val_loss: 496970.2500 - val_mse: 496970.2500\n",
            "Epoch 353/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1449044.6250 - mse: 1449044.6250 - val_loss: 476946.8750 - val_mse: 476946.8750\n",
            "Epoch 354/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1461460.0000 - mse: 1461460.0000 - val_loss: 493272.5938 - val_mse: 493272.5938\n",
            "Epoch 355/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1452380.3750 - mse: 1452380.3750 - val_loss: 505732.2812 - val_mse: 505732.2812\n",
            "Epoch 356/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1453063.3750 - mse: 1453063.3750 - val_loss: 485008.2812 - val_mse: 485008.2812\n",
            "Epoch 357/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1433369.1250 - mse: 1433369.1250 - val_loss: 513726.1250 - val_mse: 513726.1250\n",
            "Epoch 358/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1447226.5000 - mse: 1447226.5000 - val_loss: 492969.3438 - val_mse: 492969.3438\n",
            "Epoch 359/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1463285.3750 - mse: 1463285.3750 - val_loss: 509452.6562 - val_mse: 509452.6562\n",
            "Epoch 360/500\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 1471065.6250 - mse: 1471065.6250 - val_loss: 513367.7188 - val_mse: 513367.7188\n",
            "Epoch 361/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1455639.3750 - mse: 1455639.3750 - val_loss: 500977.6250 - val_mse: 500977.6250\n",
            "Epoch 362/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1472818.7500 - mse: 1472818.7500 - val_loss: 488629.2812 - val_mse: 488629.2812\n",
            "Epoch 363/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1439254.7500 - mse: 1439254.7500 - val_loss: 509217.4688 - val_mse: 509217.4688\n",
            "Epoch 364/500\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 1455098.6250 - mse: 1455098.6250 - val_loss: 517413.3750 - val_mse: 517413.3750\n",
            "Epoch 365/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1457141.8750 - mse: 1457141.8750 - val_loss: 509018.6250 - val_mse: 509018.6250\n",
            "Epoch 366/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1440309.5000 - mse: 1440309.5000 - val_loss: 504868.7500 - val_mse: 504868.7500\n",
            "Epoch 367/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1465734.5000 - mse: 1465734.5000 - val_loss: 496559.1250 - val_mse: 496559.1250\n",
            "Epoch 368/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1459703.3750 - mse: 1459703.3750 - val_loss: 512815.0000 - val_mse: 512815.0000\n",
            "Epoch 369/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1443785.2500 - mse: 1443785.2500 - val_loss: 512657.1250 - val_mse: 512657.1250\n",
            "Epoch 370/500\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 1437608.5000 - mse: 1437608.5000 - val_loss: 508665.7812 - val_mse: 508665.7812\n",
            "Epoch 371/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1464066.7500 - mse: 1464066.7500 - val_loss: 500241.8438 - val_mse: 500241.8438\n",
            "Epoch 372/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1476212.7500 - mse: 1476212.7500 - val_loss: 496136.1562 - val_mse: 496136.1562\n",
            "Epoch 373/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 1456770.7500 - mse: 1456770.7500 - val_loss: 504282.6250 - val_mse: 504282.6250\n",
            "Epoch 374/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1435092.1250 - mse: 1435092.1250 - val_loss: 483648.1562 - val_mse: 483648.1562\n",
            "Epoch 375/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1439548.8750 - mse: 1439548.8750 - val_loss: 487603.6250 - val_mse: 487603.6250\n",
            "Epoch 376/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1455792.0000 - mse: 1455792.0000 - val_loss: 499857.6250 - val_mse: 499857.6250\n",
            "Epoch 377/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1440763.0000 - mse: 1440763.0000 - val_loss: 512106.2500 - val_mse: 512106.2500\n",
            "Epoch 378/500\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 1462734.5000 - mse: 1462734.5000 - val_loss: 520324.1562 - val_mse: 520324.1562\n",
            "Epoch 379/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1457110.0000 - mse: 1457110.0000 - val_loss: 495558.1562 - val_mse: 495558.1562\n",
            "Epoch 380/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1459679.6250 - mse: 1459679.6250 - val_loss: 495519.4688 - val_mse: 495519.4688\n",
            "Epoch 381/500\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 1447823.5000 - mse: 1447823.5000 - val_loss: 516017.2500 - val_mse: 516017.2500\n",
            "Epoch 382/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1437948.8750 - mse: 1437948.8750 - val_loss: 507606.2188 - val_mse: 507606.2188\n",
            "Epoch 383/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1433061.1250 - mse: 1433061.1250 - val_loss: 503304.1562 - val_mse: 503304.1562\n",
            "Epoch 384/500\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 1451311.3750 - mse: 1451311.3750 - val_loss: 495211.9062 - val_mse: 495211.9062\n",
            "Epoch 385/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1461183.5000 - mse: 1461183.5000 - val_loss: 519686.1562 - val_mse: 519686.1562\n",
            "Epoch 386/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1470910.7500 - mse: 1470910.7500 - val_loss: 511397.7188 - val_mse: 511397.7188\n",
            "Epoch 387/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1442175.3750 - mse: 1442175.3750 - val_loss: 482708.0938 - val_mse: 482708.0938\n",
            "Epoch 388/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1451431.2500 - mse: 1451431.2500 - val_loss: 527848.5625 - val_mse: 527848.5625\n",
            "Epoch 389/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1452609.7500 - mse: 1452609.7500 - val_loss: 527650.9375 - val_mse: 527650.9375\n",
            "Epoch 390/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1465045.8750 - mse: 1465045.8750 - val_loss: 506980.7500 - val_mse: 506980.7500\n",
            "Epoch 391/500\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 1446904.5000 - mse: 1446904.5000 - val_loss: 494674.6562 - val_mse: 494674.6562\n",
            "Epoch 392/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1452361.8750 - mse: 1452361.8750 - val_loss: 510849.6562 - val_mse: 510849.6562\n",
            "Epoch 393/500\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 1460103.7500 - mse: 1460103.7500 - val_loss: 482258.2812 - val_mse: 482258.2812\n",
            "Epoch 394/500\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 1472314.0000 - mse: 1472314.0000 - val_loss: 502454.9688 - val_mse: 502454.9688\n",
            "Epoch 395/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1437997.1250 - mse: 1437997.1250 - val_loss: 482069.9062 - val_mse: 482069.9062\n",
            "Epoch 396/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1445962.5000 - mse: 1445962.5000 - val_loss: 506589.6562 - val_mse: 506589.6562\n",
            "Epoch 397/500\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 1453578.7500 - mse: 1453578.7500 - val_loss: 498198.7500 - val_mse: 498198.7500\n",
            "Epoch 398/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1471091.7500 - mse: 1471091.7500 - val_loss: 518725.1250 - val_mse: 518725.1250\n",
            "Epoch 399/500\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1459239.8750 - mse: 1459239.8750 - val_loss: 505727.8750 - val_mse: 505727.8750\n",
            "Epoch 400/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1479086.3750 - mse: 1479086.3750 - val_loss: 502066.8438 - val_mse: 502066.8438\n",
            "Epoch 401/500\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1451100.7500 - mse: 1451100.7500 - val_loss: 526716.5625 - val_mse: 526716.5625\n",
            "Epoch 402/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1447483.8750 - mse: 1447483.8750 - val_loss: 497851.8438 - val_mse: 497851.8438\n",
            "Epoch 403/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1444632.2500 - mse: 1444632.2500 - val_loss: 509984.6250 - val_mse: 509984.6250\n",
            "Epoch 404/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1450427.7500 - mse: 1450427.7500 - val_loss: 477409.6562 - val_mse: 477409.6562\n",
            "Epoch 405/500\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 1465336.1250 - mse: 1465336.1250 - val_loss: 518166.4062 - val_mse: 518166.4062\n",
            "Epoch 406/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1451489.6250 - mse: 1451489.6250 - val_loss: 497543.8438 - val_mse: 497543.8438\n",
            "Epoch 407/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1448046.1250 - mse: 1448046.1250 - val_loss: 513948.8750 - val_mse: 513948.8750\n",
            "Epoch 408/500\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 1451032.6250 - mse: 1451032.6250 - val_loss: 472206.7812 - val_mse: 472206.7812\n",
            "Epoch 409/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 1444628.5000 - mse: 1444628.5000 - val_loss: 513751.1562 - val_mse: 513751.1562\n",
            "Epoch 410/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1449571.2500 - mse: 1449571.2500 - val_loss: 501434.2500 - val_mse: 501434.2500\n",
            "Epoch 411/500\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 1455792.8750 - mse: 1455792.8750 - val_loss: 501140.0312 - val_mse: 501140.0312\n",
            "Epoch 412/500\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 1442704.5000 - mse: 1442704.5000 - val_loss: 493101.3438 - val_mse: 493101.3438\n",
            "Epoch 413/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 1449464.3750 - mse: 1449464.3750 - val_loss: 500985.5938 - val_mse: 500985.5938\n",
            "Epoch 414/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1462257.2500 - mse: 1462257.2500 - val_loss: 505178.7500 - val_mse: 505178.7500\n",
            "Epoch 415/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1467965.2500 - mse: 1467965.2500 - val_loss: 492794.3750 - val_mse: 492794.3750\n",
            "Epoch 416/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1435433.1250 - mse: 1435433.1250 - val_loss: 513233.0000 - val_mse: 513233.0000\n",
            "Epoch 417/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1470503.3750 - mse: 1470503.3750 - val_loss: 508922.9688 - val_mse: 508922.9688\n",
            "Epoch 418/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1421192.3750 - mse: 1421192.3750 - val_loss: 483732.5312 - val_mse: 483732.5312\n",
            "Epoch 419/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1456653.3750 - mse: 1456653.3750 - val_loss: 480302.0938 - val_mse: 480302.0938\n",
            "Epoch 420/500\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 1453086.3750 - mse: 1453086.3750 - val_loss: 488222.7188 - val_mse: 488222.7188\n",
            "Epoch 421/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1432955.8750 - mse: 1432955.8750 - val_loss: 516189.5938 - val_mse: 516189.5938\n",
            "Epoch 422/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1431060.2500 - mse: 1431060.2500 - val_loss: 504553.5312 - val_mse: 504553.5312\n",
            "Epoch 423/500\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 1468357.3750 - mse: 1468357.3750 - val_loss: 500250.8750 - val_mse: 500250.8750\n",
            "Epoch 424/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1436790.7500 - mse: 1436790.7500 - val_loss: 504322.2500 - val_mse: 504322.2500\n",
            "Epoch 425/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1457907.1250 - mse: 1457907.1250 - val_loss: 496119.3438 - val_mse: 496119.3438\n",
            "Epoch 426/500\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 1437194.7500 - mse: 1437194.7500 - val_loss: 512438.5938 - val_mse: 512438.5938\n",
            "Epoch 427/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1451802.2500 - mse: 1451802.2500 - val_loss: 495927.3750 - val_mse: 495927.3750\n",
            "Epoch 428/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1460650.5000 - mse: 1460650.5000 - val_loss: 491799.4062 - val_mse: 491799.4062\n",
            "Epoch 429/500\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1439991.2500 - mse: 1439991.2500 - val_loss: 495772.7188 - val_mse: 495772.7188\n",
            "Epoch 430/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1443959.6250 - mse: 1443959.6250 - val_loss: 512119.3750 - val_mse: 512119.3750\n",
            "Epoch 431/500\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 1464823.3750 - mse: 1464823.3750 - val_loss: 499631.7812 - val_mse: 499631.7812\n",
            "Epoch 432/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1429784.7500 - mse: 1429784.7500 - val_loss: 507783.6562 - val_mse: 507783.6562\n",
            "Epoch 433/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1462602.0000 - mse: 1462602.0000 - val_loss: 507705.2500 - val_mse: 507705.2500\n",
            "Epoch 434/500\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 1447711.8750 - mse: 1447711.8750 - val_loss: 515816.6562 - val_mse: 515816.6562\n",
            "Epoch 435/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1440678.0000 - mse: 1440678.0000 - val_loss: 507512.3750 - val_mse: 507512.3750\n",
            "Epoch 436/500\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 1451983.3750 - mse: 1451983.3750 - val_loss: 506734.7188 - val_mse: 506734.7188\n",
            "Epoch 437/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1454751.3750 - mse: 1454751.3750 - val_loss: 499370.0000 - val_mse: 499370.0000\n",
            "Epoch 438/500\n",
            "37/37 [==============================] - 5s 120ms/step - loss: 1449327.2500 - mse: 1449327.2500 - val_loss: 490211.8438 - val_mse: 490211.8438\n",
            "Epoch 439/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1447071.7500 - mse: 1447071.7500 - val_loss: 499214.6562 - val_mse: 499214.6562\n",
            "Epoch 440/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 1438770.3750 - mse: 1438770.3750 - val_loss: 507084.6562 - val_mse: 507084.6562\n",
            "Epoch 441/500\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 1456949.1250 - mse: 1456949.1250 - val_loss: 499059.2188 - val_mse: 499059.2188\n",
            "Epoch 442/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1436567.7500 - mse: 1436567.7500 - val_loss: 490766.7812 - val_mse: 490766.7812\n",
            "Epoch 443/500\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 1444984.8750 - mse: 1444984.8750 - val_loss: 486681.7188 - val_mse: 486681.7188\n",
            "Epoch 444/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1434581.8750 - mse: 1434581.8750 - val_loss: 515060.2188 - val_mse: 515060.2188\n",
            "Epoch 445/500\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 1447516.5000 - mse: 1447516.5000 - val_loss: 510971.1562 - val_mse: 510971.1562\n",
            "Epoch 446/500\n",
            "37/37 [==============================] - 5s 109ms/step - loss: 1433396.6250 - mse: 1433396.6250 - val_loss: 494434.4062 - val_mse: 494434.4062\n",
            "Epoch 447/500\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 1449978.7500 - mse: 1449978.7500 - val_loss: 502605.0312 - val_mse: 502605.0312\n",
            "Epoch 448/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1443497.2500 - mse: 1443497.2500 - val_loss: 482103.2500 - val_mse: 482103.2500\n",
            "Epoch 449/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1446761.0000 - mse: 1446761.0000 - val_loss: 514626.9062 - val_mse: 514626.9062\n",
            "Epoch 450/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1418039.3750 - mse: 1418039.3750 - val_loss: 498172.8438 - val_mse: 498172.8438\n",
            "Epoch 451/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 1427666.2500 - mse: 1427666.2500 - val_loss: 502293.2500 - val_mse: 502293.2500\n",
            "Epoch 452/500\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1455861.5000 - mse: 1455861.5000 - val_loss: 497983.2500 - val_mse: 497983.2500\n",
            "Epoch 453/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1433570.7500 - mse: 1433570.7500 - val_loss: 514345.6250 - val_mse: 514345.6250\n",
            "Epoch 454/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 1444355.5000 - mse: 1444355.5000 - val_loss: 514193.6562 - val_mse: 514193.6562\n",
            "Epoch 455/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1458216.5000 - mse: 1458216.5000 - val_loss: 501944.7500 - val_mse: 501944.7500\n",
            "Epoch 456/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1436722.8750 - mse: 1436722.8750 - val_loss: 514035.8438 - val_mse: 514035.8438\n",
            "Epoch 457/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1448134.1250 - mse: 1448134.1250 - val_loss: 510022.5000 - val_mse: 510022.5000\n",
            "Epoch 458/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1426942.5000 - mse: 1426942.5000 - val_loss: 485394.5000 - val_mse: 485394.5000\n",
            "Epoch 459/500\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1419994.0000 - mse: 1419994.0000 - val_loss: 469115.6250 - val_mse: 469115.6250\n",
            "Epoch 460/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1435454.5000 - mse: 1435454.5000 - val_loss: 497405.0312 - val_mse: 497405.0312\n",
            "Epoch 461/500\n",
            "37/37 [==============================] - 5s 107ms/step - loss: 1452243.8750 - mse: 1452243.8750 - val_loss: 501480.7500 - val_mse: 501480.7500\n",
            "Epoch 462/500\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 1429875.6250 - mse: 1429875.6250 - val_loss: 505370.7500 - val_mse: 505370.7500\n",
            "Epoch 463/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1460825.8750 - mse: 1460825.8750 - val_loss: 509584.8750 - val_mse: 509584.8750\n",
            "Epoch 464/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1417028.6250 - mse: 1417028.6250 - val_loss: 497099.0938 - val_mse: 497099.0938\n",
            "Epoch 465/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1426161.1250 - mse: 1426161.1250 - val_loss: 493021.2812 - val_mse: 493021.2812\n",
            "Epoch 466/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1449283.1250 - mse: 1449283.1250 - val_loss: 496946.0000 - val_mse: 496946.0000\n",
            "Epoch 467/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1436837.8750 - mse: 1436837.8750 - val_loss: 488864.0000 - val_mse: 488864.0000\n",
            "Epoch 468/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 1444536.1250 - mse: 1444536.1250 - val_loss: 504939.2812 - val_mse: 504939.2812\n",
            "Epoch 469/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1431849.1250 - mse: 1431849.1250 - val_loss: 509040.4062 - val_mse: 509040.4062\n",
            "Epoch 470/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1434611.0000 - mse: 1434611.0000 - val_loss: 492601.4688 - val_mse: 492601.4688\n",
            "Epoch 471/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1435799.8750 - mse: 1435799.8750 - val_loss: 508918.2812 - val_mse: 508918.2812\n",
            "Epoch 472/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1438534.2500 - mse: 1438534.2500 - val_loss: 500629.0000 - val_mse: 500629.0000\n",
            "Epoch 473/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1441141.0000 - mse: 1441141.0000 - val_loss: 500551.0312 - val_mse: 500551.0312\n",
            "Epoch 474/500\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 1430789.0000 - mse: 1430789.0000 - val_loss: 496471.4688 - val_mse: 496471.4688\n",
            "Epoch 475/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1453357.8750 - mse: 1453357.8750 - val_loss: 520671.0312 - val_mse: 520671.0312\n",
            "Epoch 476/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1441169.3750 - mse: 1441169.3750 - val_loss: 500317.8438 - val_mse: 500317.8438\n",
            "Epoch 477/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1440411.0000 - mse: 1440411.0000 - val_loss: 512375.6250 - val_mse: 512375.6250\n",
            "Epoch 478/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1426975.6250 - mse: 1426975.6250 - val_loss: 492028.1562 - val_mse: 492028.1562\n",
            "Epoch 479/500\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1451906.6250 - mse: 1451906.6250 - val_loss: 500085.6250 - val_mse: 500085.6250\n",
            "Epoch 480/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1420869.0000 - mse: 1420869.0000 - val_loss: 467579.5312 - val_mse: 467579.5312\n",
            "Epoch 481/500\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 1428391.7500 - mse: 1428391.7500 - val_loss: 479635.5938 - val_mse: 479635.5938\n",
            "Epoch 482/500\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 1457892.5000 - mse: 1457892.5000 - val_loss: 495757.2500 - val_mse: 495757.2500\n",
            "Epoch 483/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1456055.6250 - mse: 1456055.6250 - val_loss: 487650.2188 - val_mse: 487650.2188\n",
            "Epoch 484/500\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 1451500.3750 - mse: 1451500.3750 - val_loss: 483443.9062 - val_mse: 483443.9062\n",
            "Epoch 485/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1443200.0000 - mse: 1443200.0000 - val_loss: 499690.0000 - val_mse: 499690.0000\n",
            "Epoch 486/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1428875.2500 - mse: 1428875.2500 - val_loss: 483293.5000 - val_mse: 483293.5000\n",
            "Epoch 487/500\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 1421723.3750 - mse: 1421723.3750 - val_loss: 479187.0000 - val_mse: 479187.0000\n",
            "Epoch 488/500\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 1418865.3750 - mse: 1418865.3750 - val_loss: 491299.4688 - val_mse: 491299.4688\n",
            "Epoch 489/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1453230.2500 - mse: 1453230.2500 - val_loss: 483034.6562 - val_mse: 483034.6562\n",
            "Epoch 490/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1422523.6250 - mse: 1422523.6250 - val_loss: 474967.2188 - val_mse: 474967.2188\n",
            "Epoch 491/500\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 1426545.6250 - mse: 1426545.6250 - val_loss: 487007.4688 - val_mse: 487007.4688\n",
            "Epoch 492/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1431819.3750 - mse: 1431819.3750 - val_loss: 486840.0312 - val_mse: 486840.0312\n",
            "Epoch 493/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1433702.1250 - mse: 1433702.1250 - val_loss: 490921.0000 - val_mse: 490921.0000\n",
            "Epoch 494/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1456976.0000 - mse: 1456976.0000 - val_loss: 478700.7500 - val_mse: 478700.7500\n",
            "Epoch 495/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1441563.8750 - mse: 1441563.8750 - val_loss: 510990.0000 - val_mse: 510990.0000\n",
            "Epoch 496/500\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 1449451.0000 - mse: 1449451.0000 - val_loss: 482545.1562 - val_mse: 482545.1562\n",
            "Epoch 497/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1438185.7500 - mse: 1438185.7500 - val_loss: 506872.8750 - val_mse: 506872.8750\n",
            "Epoch 498/500\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 1426088.6250 - mse: 1426088.6250 - val_loss: 474408.4688 - val_mse: 474408.4688\n",
            "Epoch 499/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1424107.1250 - mse: 1424107.1250 - val_loss: 490430.3750 - val_mse: 490430.3750\n",
            "Epoch 500/500\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 1447948.3750 - mse: 1447948.3750 - val_loss: 494468.7188 - val_mse: 494468.7188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history "
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "outputId": "3c275249-42df-46cd-ed0f-2389548b4739",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [1528629.0,\n",
              "  1509688.375,\n",
              "  1500127.125,\n",
              "  1510955.75,\n",
              "  1503262.625,\n",
              "  1501160.875,\n",
              "  1499390.25,\n",
              "  1505702.375,\n",
              "  1492077.25,\n",
              "  1491255.125,\n",
              "  1508117.875,\n",
              "  1503721.0,\n",
              "  1522541.625,\n",
              "  1512657.875,\n",
              "  1502469.75,\n",
              "  1493018.0,\n",
              "  1481325.375,\n",
              "  1503619.75,\n",
              "  1521810.625,\n",
              "  1503182.25,\n",
              "  1495744.375,\n",
              "  1495858.5,\n",
              "  1500126.0,\n",
              "  1516981.375,\n",
              "  1522503.625,\n",
              "  1509456.875,\n",
              "  1489832.875,\n",
              "  1518410.75,\n",
              "  1495376.125,\n",
              "  1485730.75,\n",
              "  1489052.125,\n",
              "  1521650.25,\n",
              "  1498526.75,\n",
              "  1510755.25,\n",
              "  1494383.125,\n",
              "  1524380.375,\n",
              "  1508602.25,\n",
              "  1511140.125,\n",
              "  1503043.75,\n",
              "  1504875.0,\n",
              "  1488796.0,\n",
              "  1498296.0,\n",
              "  1506481.125,\n",
              "  1492161.375,\n",
              "  1512968.75,\n",
              "  1490919.75,\n",
              "  1508695.625,\n",
              "  1477521.875,\n",
              "  1495139.875,\n",
              "  1492927.875,\n",
              "  1509470.625,\n",
              "  1487933.5,\n",
              "  1510761.625,\n",
              "  1502340.625,\n",
              "  1493644.375,\n",
              "  1513874.75,\n",
              "  1515227.75,\n",
              "  1494322.5,\n",
              "  1490711.875,\n",
              "  1508471.625,\n",
              "  1510193.75,\n",
              "  1494230.625,\n",
              "  1496258.375,\n",
              "  1504785.0,\n",
              "  1493366.375,\n",
              "  1526155.125,\n",
              "  1510726.0,\n",
              "  1498396.25,\n",
              "  1505367.5,\n",
              "  1494151.875,\n",
              "  1480397.875,\n",
              "  1494529.625,\n",
              "  1515024.0,\n",
              "  1503779.25,\n",
              "  1492757.5,\n",
              "  1503654.0,\n",
              "  1502619.5,\n",
              "  1489551.625,\n",
              "  1489618.0,\n",
              "  1480119.0,\n",
              "  1510000.25,\n",
              "  1494374.5,\n",
              "  1499982.875,\n",
              "  1471517.25,\n",
              "  1496390.0,\n",
              "  1498364.75,\n",
              "  1478507.125,\n",
              "  1499512.25,\n",
              "  1506111.125,\n",
              "  1494742.125,\n",
              "  1489595.5,\n",
              "  1506258.75,\n",
              "  1494259.5,\n",
              "  1491560.875,\n",
              "  1506146.75,\n",
              "  1491882.125,\n",
              "  1487028.125,\n",
              "  1491191.75,\n",
              "  1495232.625,\n",
              "  1501514.375,\n",
              "  1515127.25,\n",
              "  1486890.625,\n",
              "  1480692.375,\n",
              "  1509546.5,\n",
              "  1509720.0,\n",
              "  1488570.25,\n",
              "  1494960.0,\n",
              "  1485822.375,\n",
              "  1494806.875,\n",
              "  1473055.75,\n",
              "  1486762.5,\n",
              "  1489324.75,\n",
              "  1498937.0,\n",
              "  1480930.375,\n",
              "  1494033.25,\n",
              "  1492016.375,\n",
              "  1484502.75,\n",
              "  1512359.75,\n",
              "  1475121.625,\n",
              "  1482869.25,\n",
              "  1511496.125,\n",
              "  1506341.0,\n",
              "  1479081.25,\n",
              "  1494807.75,\n",
              "  1491236.375,\n",
              "  1493403.25,\n",
              "  1476301.625,\n",
              "  1478484.125,\n",
              "  1501970.125,\n",
              "  1477658.375,\n",
              "  1508795.0,\n",
              "  1491612.125,\n",
              "  1510363.75,\n",
              "  1485216.5,\n",
              "  1500590.75,\n",
              "  1508262.875,\n",
              "  1497643.875,\n",
              "  1503455.375,\n",
              "  1487220.375,\n",
              "  1515510.125,\n",
              "  1490435.375,\n",
              "  1491967.625,\n",
              "  1476237.625,\n",
              "  1502547.75,\n",
              "  1482334.375,\n",
              "  1486679.625,\n",
              "  1488601.125,\n",
              "  1448970.25,\n",
              "  1488329.5,\n",
              "  1472313.0,\n",
              "  1490154.375,\n",
              "  1513997.125,\n",
              "  1488331.0,\n",
              "  1497997.625,\n",
              "  1515423.25,\n",
              "  1490263.0,\n",
              "  1501862.625,\n",
              "  1485349.0,\n",
              "  1503998.875,\n",
              "  1462573.75,\n",
              "  1488171.375,\n",
              "  1468983.25,\n",
              "  1484228.625,\n",
              "  1472010.5,\n",
              "  1482237.5,\n",
              "  1485059.125,\n",
              "  1490888.5,\n",
              "  1455787.75,\n",
              "  1490143.0,\n",
              "  1482362.25,\n",
              "  1482357.75,\n",
              "  1496741.0,\n",
              "  1473908.625,\n",
              "  1495965.375,\n",
              "  1504172.5,\n",
              "  1493428.0,\n",
              "  1493727.875,\n",
              "  1510598.125,\n",
              "  1489569.25,\n",
              "  1473811.75,\n",
              "  1490158.75,\n",
              "  1494414.125,\n",
              "  1463198.375,\n",
              "  1490890.375,\n",
              "  1491083.75,\n",
              "  1494020.125,\n",
              "  1472620.875,\n",
              "  1480454.25,\n",
              "  1471838.5,\n",
              "  1490418.0,\n",
              "  1494680.375,\n",
              "  1478535.625,\n",
              "  1477741.125,\n",
              "  1469900.75,\n",
              "  1481722.0,\n",
              "  1477737.875,\n",
              "  1479074.0,\n",
              "  1488679.125,\n",
              "  1475048.0,\n",
              "  1465777.5,\n",
              "  1476398.75,\n",
              "  1466995.625,\n",
              "  1484137.375,\n",
              "  1463722.625,\n",
              "  1476893.625,\n",
              "  1487317.0,\n",
              "  1488002.625,\n",
              "  1482781.25,\n",
              "  1482841.375,\n",
              "  1481274.875,\n",
              "  1462258.375,\n",
              "  1456012.375,\n",
              "  1482880.625,\n",
              "  1495483.75,\n",
              "  1470030.375,\n",
              "  1484838.5,\n",
              "  1487139.125,\n",
              "  1452664.125,\n",
              "  1460314.875,\n",
              "  1468410.0,\n",
              "  1471140.125,\n",
              "  1494678.125,\n",
              "  1483518.0,\n",
              "  1480820.375,\n",
              "  1483200.5,\n",
              "  1485011.125,\n",
              "  1480098.375,\n",
              "  1452909.25,\n",
              "  1474982.0,\n",
              "  1479444.25,\n",
              "  1468698.75,\n",
              "  1486856.75,\n",
              "  1489156.875,\n",
              "  1461193.375,\n",
              "  1484774.25,\n",
              "  1474316.375,\n",
              "  1463374.25,\n",
              "  1454224.875,\n",
              "  1470731.375,\n",
              "  1481275.875,\n",
              "  1462824.125,\n",
              "  1485281.625,\n",
              "  1471474.5,\n",
              "  1460372.0,\n",
              "  1495933.125,\n",
              "  1474151.625,\n",
              "  1468483.25,\n",
              "  1474392.75,\n",
              "  1494701.25,\n",
              "  1461774.5,\n",
              "  1470623.375,\n",
              "  1443984.625,\n",
              "  1472046.75,\n",
              "  1477265.25,\n",
              "  1467429.0,\n",
              "  1471277.5,\n",
              "  1476529.75,\n",
              "  1469410.0,\n",
              "  1471914.5,\n",
              "  1482840.75,\n",
              "  1468837.25,\n",
              "  1457142.375,\n",
              "  1484307.75,\n",
              "  1480178.875,\n",
              "  1461881.0,\n",
              "  1463292.75,\n",
              "  1475698.5,\n",
              "  1491431.25,\n",
              "  1479065.125,\n",
              "  1487667.0,\n",
              "  1473744.0,\n",
              "  1466243.625,\n",
              "  1472114.25,\n",
              "  1471553.375,\n",
              "  1482746.625,\n",
              "  1482157.0,\n",
              "  1472240.75,\n",
              "  1469406.5,\n",
              "  1482885.75,\n",
              "  1467693.0,\n",
              "  1468893.25,\n",
              "  1468128.375,\n",
              "  1477911.25,\n",
              "  1470293.5,\n",
              "  1472771.25,\n",
              "  1446436.375,\n",
              "  1484476.0,\n",
              "  1443784.875,\n",
              "  1476398.125,\n",
              "  1465378.625,\n",
              "  1477580.125,\n",
              "  1470515.625,\n",
              "  1462848.25,\n",
              "  1476201.625,\n",
              "  1480765.375,\n",
              "  1457967.875,\n",
              "  1466725.0,\n",
              "  1473465.875,\n",
              "  1474206.125,\n",
              "  1452747.25,\n",
              "  1464201.0,\n",
              "  1444396.125,\n",
              "  1450411.375,\n",
              "  1454778.125,\n",
              "  1469939.75,\n",
              "  1449420.625,\n",
              "  1482858.5,\n",
              "  1470766.125,\n",
              "  1462199.375,\n",
              "  1462303.375,\n",
              "  1462002.75,\n",
              "  1456415.25,\n",
              "  1467708.625,\n",
              "  1478342.25,\n",
              "  1481763.625,\n",
              "  1441692.75,\n",
              "  1455547.5,\n",
              "  1464164.125,\n",
              "  1471124.125,\n",
              "  1455486.375,\n",
              "  1441951.5,\n",
              "  1483067.125,\n",
              "  1456591.625,\n",
              "  1459489.75,\n",
              "  1464084.875,\n",
              "  1469073.375,\n",
              "  1468421.0,\n",
              "  1474352.75,\n",
              "  1458574.5,\n",
              "  1458686.0,\n",
              "  1476374.125,\n",
              "  1457157.375,\n",
              "  1462553.125,\n",
              "  1457947.625,\n",
              "  1464159.625,\n",
              "  1475276.0,\n",
              "  1469097.0,\n",
              "  1454435.375,\n",
              "  1435807.125,\n",
              "  1453657.125,\n",
              "  1471220.0,\n",
              "  1450739.0,\n",
              "  1472104.5,\n",
              "  1455508.5,\n",
              "  1455603.625,\n",
              "  1452373.0,\n",
              "  1485516.375,\n",
              "  1421529.25,\n",
              "  1458349.375,\n",
              "  1466016.75,\n",
              "  1465919.5,\n",
              "  1475963.0,\n",
              "  1449044.625,\n",
              "  1461460.0,\n",
              "  1452380.375,\n",
              "  1453063.375,\n",
              "  1433369.125,\n",
              "  1447226.5,\n",
              "  1463285.375,\n",
              "  1471065.625,\n",
              "  1455639.375,\n",
              "  1472818.75,\n",
              "  1439254.75,\n",
              "  1455098.625,\n",
              "  1457141.875,\n",
              "  1440309.5,\n",
              "  1465734.5,\n",
              "  1459703.375,\n",
              "  1443785.25,\n",
              "  1437608.5,\n",
              "  1464066.75,\n",
              "  1476212.75,\n",
              "  1456770.75,\n",
              "  1435092.125,\n",
              "  1439548.875,\n",
              "  1455792.0,\n",
              "  1440763.0,\n",
              "  1462734.5,\n",
              "  1457110.0,\n",
              "  1459679.625,\n",
              "  1447823.5,\n",
              "  1437948.875,\n",
              "  1433061.125,\n",
              "  1451311.375,\n",
              "  1461183.5,\n",
              "  1470910.75,\n",
              "  1442175.375,\n",
              "  1451431.25,\n",
              "  1452609.75,\n",
              "  1465045.875,\n",
              "  1446904.5,\n",
              "  1452361.875,\n",
              "  1460103.75,\n",
              "  1472314.0,\n",
              "  1437997.125,\n",
              "  1445962.5,\n",
              "  1453578.75,\n",
              "  1471091.75,\n",
              "  1459239.875,\n",
              "  1479086.375,\n",
              "  1451100.75,\n",
              "  1447483.875,\n",
              "  1444632.25,\n",
              "  1450427.75,\n",
              "  1465336.125,\n",
              "  1451489.625,\n",
              "  1448046.125,\n",
              "  1451032.625,\n",
              "  1444628.5,\n",
              "  1449571.25,\n",
              "  1455792.875,\n",
              "  1442704.5,\n",
              "  1449464.375,\n",
              "  1462257.25,\n",
              "  1467965.25,\n",
              "  1435433.125,\n",
              "  1470503.375,\n",
              "  1421192.375,\n",
              "  1456653.375,\n",
              "  1453086.375,\n",
              "  1432955.875,\n",
              "  1431060.25,\n",
              "  1468357.375,\n",
              "  1436790.75,\n",
              "  1457907.125,\n",
              "  1437194.75,\n",
              "  1451802.25,\n",
              "  1460650.5,\n",
              "  1439991.25,\n",
              "  1443959.625,\n",
              "  1464823.375,\n",
              "  1429784.75,\n",
              "  1462602.0,\n",
              "  1447711.875,\n",
              "  1440678.0,\n",
              "  1451983.375,\n",
              "  1454751.375,\n",
              "  1449327.25,\n",
              "  1447071.75,\n",
              "  1438770.375,\n",
              "  1456949.125,\n",
              "  1436567.75,\n",
              "  1444984.875,\n",
              "  1434581.875,\n",
              "  1447516.5,\n",
              "  1433396.625,\n",
              "  1449978.75,\n",
              "  1443497.25,\n",
              "  1446761.0,\n",
              "  1418039.375,\n",
              "  1427666.25,\n",
              "  1455861.5,\n",
              "  1433570.75,\n",
              "  1444355.5,\n",
              "  1458216.5,\n",
              "  1436722.875,\n",
              "  1448134.125,\n",
              "  1426942.5,\n",
              "  1419994.0,\n",
              "  1435454.5,\n",
              "  1452243.875,\n",
              "  1429875.625,\n",
              "  1460825.875,\n",
              "  1417028.625,\n",
              "  1426161.125,\n",
              "  1449283.125,\n",
              "  1436837.875,\n",
              "  1444536.125,\n",
              "  1431849.125,\n",
              "  1434611.0,\n",
              "  1435799.875,\n",
              "  1438534.25,\n",
              "  1441141.0,\n",
              "  1430789.0,\n",
              "  1453357.875,\n",
              "  1441169.375,\n",
              "  1440411.0,\n",
              "  1426975.625,\n",
              "  1451906.625,\n",
              "  1420869.0,\n",
              "  1428391.75,\n",
              "  1457892.5,\n",
              "  1456055.625,\n",
              "  1451500.375,\n",
              "  1443200.0,\n",
              "  1428875.25,\n",
              "  1421723.375,\n",
              "  1418865.375,\n",
              "  1453230.25,\n",
              "  1422523.625,\n",
              "  1426545.625,\n",
              "  1431819.375,\n",
              "  1433702.125,\n",
              "  1456976.0,\n",
              "  1441563.875,\n",
              "  1449451.0,\n",
              "  1438185.75,\n",
              "  1426088.625,\n",
              "  1424107.125,\n",
              "  1447948.375],\n",
              " 'mse': [1528629.0,\n",
              "  1509688.375,\n",
              "  1500127.125,\n",
              "  1510955.75,\n",
              "  1503262.625,\n",
              "  1501160.875,\n",
              "  1499390.25,\n",
              "  1505702.375,\n",
              "  1492077.25,\n",
              "  1491255.125,\n",
              "  1508117.875,\n",
              "  1503721.0,\n",
              "  1522541.625,\n",
              "  1512657.875,\n",
              "  1502469.75,\n",
              "  1493018.0,\n",
              "  1481325.375,\n",
              "  1503619.75,\n",
              "  1521810.625,\n",
              "  1503182.25,\n",
              "  1495744.375,\n",
              "  1495858.5,\n",
              "  1500126.0,\n",
              "  1516981.375,\n",
              "  1522503.625,\n",
              "  1509456.875,\n",
              "  1489832.875,\n",
              "  1518410.75,\n",
              "  1495376.125,\n",
              "  1485730.75,\n",
              "  1489052.125,\n",
              "  1521650.25,\n",
              "  1498526.75,\n",
              "  1510755.25,\n",
              "  1494383.125,\n",
              "  1524380.375,\n",
              "  1508602.25,\n",
              "  1511140.125,\n",
              "  1503043.75,\n",
              "  1504875.0,\n",
              "  1488796.0,\n",
              "  1498296.0,\n",
              "  1506481.125,\n",
              "  1492161.375,\n",
              "  1512968.75,\n",
              "  1490919.75,\n",
              "  1508695.625,\n",
              "  1477521.875,\n",
              "  1495139.875,\n",
              "  1492927.875,\n",
              "  1509470.625,\n",
              "  1487933.5,\n",
              "  1510761.625,\n",
              "  1502340.625,\n",
              "  1493644.375,\n",
              "  1513874.75,\n",
              "  1515227.75,\n",
              "  1494322.5,\n",
              "  1490711.875,\n",
              "  1508471.625,\n",
              "  1510193.75,\n",
              "  1494230.625,\n",
              "  1496258.375,\n",
              "  1504785.0,\n",
              "  1493366.375,\n",
              "  1526155.125,\n",
              "  1510726.0,\n",
              "  1498396.25,\n",
              "  1505367.5,\n",
              "  1494151.875,\n",
              "  1480397.875,\n",
              "  1494529.625,\n",
              "  1515024.0,\n",
              "  1503779.25,\n",
              "  1492757.5,\n",
              "  1503654.0,\n",
              "  1502619.5,\n",
              "  1489551.625,\n",
              "  1489618.0,\n",
              "  1480119.0,\n",
              "  1510000.25,\n",
              "  1494374.5,\n",
              "  1499982.875,\n",
              "  1471517.25,\n",
              "  1496390.0,\n",
              "  1498364.75,\n",
              "  1478507.125,\n",
              "  1499512.25,\n",
              "  1506111.125,\n",
              "  1494742.125,\n",
              "  1489595.5,\n",
              "  1506258.75,\n",
              "  1494259.5,\n",
              "  1491560.875,\n",
              "  1506146.75,\n",
              "  1491882.125,\n",
              "  1487028.125,\n",
              "  1491191.75,\n",
              "  1495232.625,\n",
              "  1501514.375,\n",
              "  1515127.25,\n",
              "  1486890.625,\n",
              "  1480692.375,\n",
              "  1509546.5,\n",
              "  1509720.0,\n",
              "  1488570.25,\n",
              "  1494960.0,\n",
              "  1485822.375,\n",
              "  1494806.875,\n",
              "  1473055.75,\n",
              "  1486762.5,\n",
              "  1489324.75,\n",
              "  1498937.0,\n",
              "  1480930.375,\n",
              "  1494033.25,\n",
              "  1492016.375,\n",
              "  1484502.75,\n",
              "  1512359.75,\n",
              "  1475121.625,\n",
              "  1482869.25,\n",
              "  1511496.125,\n",
              "  1506341.0,\n",
              "  1479081.25,\n",
              "  1494807.75,\n",
              "  1491236.375,\n",
              "  1493403.25,\n",
              "  1476301.625,\n",
              "  1478484.125,\n",
              "  1501970.125,\n",
              "  1477658.375,\n",
              "  1508795.0,\n",
              "  1491612.125,\n",
              "  1510363.75,\n",
              "  1485216.5,\n",
              "  1500590.75,\n",
              "  1508262.875,\n",
              "  1497643.875,\n",
              "  1503455.375,\n",
              "  1487220.375,\n",
              "  1515510.125,\n",
              "  1490435.375,\n",
              "  1491967.625,\n",
              "  1476237.625,\n",
              "  1502547.75,\n",
              "  1482334.375,\n",
              "  1486679.625,\n",
              "  1488601.125,\n",
              "  1448970.25,\n",
              "  1488329.5,\n",
              "  1472313.0,\n",
              "  1490154.375,\n",
              "  1513997.125,\n",
              "  1488331.0,\n",
              "  1497997.625,\n",
              "  1515423.25,\n",
              "  1490263.0,\n",
              "  1501862.625,\n",
              "  1485349.0,\n",
              "  1503998.875,\n",
              "  1462573.75,\n",
              "  1488171.375,\n",
              "  1468983.25,\n",
              "  1484228.625,\n",
              "  1472010.5,\n",
              "  1482237.5,\n",
              "  1485059.125,\n",
              "  1490888.5,\n",
              "  1455787.75,\n",
              "  1490143.0,\n",
              "  1482362.25,\n",
              "  1482357.75,\n",
              "  1496741.0,\n",
              "  1473908.625,\n",
              "  1495965.375,\n",
              "  1504172.5,\n",
              "  1493428.0,\n",
              "  1493727.875,\n",
              "  1510598.125,\n",
              "  1489569.25,\n",
              "  1473811.75,\n",
              "  1490158.75,\n",
              "  1494414.125,\n",
              "  1463198.375,\n",
              "  1490890.375,\n",
              "  1491083.75,\n",
              "  1494020.125,\n",
              "  1472620.875,\n",
              "  1480454.25,\n",
              "  1471838.5,\n",
              "  1490418.0,\n",
              "  1494680.375,\n",
              "  1478535.625,\n",
              "  1477741.125,\n",
              "  1469900.75,\n",
              "  1481722.0,\n",
              "  1477737.875,\n",
              "  1479074.0,\n",
              "  1488679.125,\n",
              "  1475048.0,\n",
              "  1465777.5,\n",
              "  1476398.75,\n",
              "  1466995.625,\n",
              "  1484137.375,\n",
              "  1463722.625,\n",
              "  1476893.625,\n",
              "  1487317.0,\n",
              "  1488002.625,\n",
              "  1482781.25,\n",
              "  1482841.375,\n",
              "  1481274.875,\n",
              "  1462258.375,\n",
              "  1456012.375,\n",
              "  1482880.625,\n",
              "  1495483.75,\n",
              "  1470030.375,\n",
              "  1484838.5,\n",
              "  1487139.125,\n",
              "  1452664.125,\n",
              "  1460314.875,\n",
              "  1468410.0,\n",
              "  1471140.125,\n",
              "  1494678.125,\n",
              "  1483518.0,\n",
              "  1480820.375,\n",
              "  1483200.5,\n",
              "  1485011.125,\n",
              "  1480098.375,\n",
              "  1452909.25,\n",
              "  1474982.0,\n",
              "  1479444.25,\n",
              "  1468698.75,\n",
              "  1486856.75,\n",
              "  1489156.875,\n",
              "  1461193.375,\n",
              "  1484774.25,\n",
              "  1474316.375,\n",
              "  1463374.25,\n",
              "  1454224.875,\n",
              "  1470731.375,\n",
              "  1481275.875,\n",
              "  1462824.125,\n",
              "  1485281.625,\n",
              "  1471474.5,\n",
              "  1460372.0,\n",
              "  1495933.125,\n",
              "  1474151.625,\n",
              "  1468483.25,\n",
              "  1474392.75,\n",
              "  1494701.25,\n",
              "  1461774.5,\n",
              "  1470623.375,\n",
              "  1443984.625,\n",
              "  1472046.75,\n",
              "  1477265.25,\n",
              "  1467429.0,\n",
              "  1471277.5,\n",
              "  1476529.75,\n",
              "  1469410.0,\n",
              "  1471914.5,\n",
              "  1482840.75,\n",
              "  1468837.25,\n",
              "  1457142.375,\n",
              "  1484307.75,\n",
              "  1480178.875,\n",
              "  1461881.0,\n",
              "  1463292.75,\n",
              "  1475698.5,\n",
              "  1491431.25,\n",
              "  1479065.125,\n",
              "  1487667.0,\n",
              "  1473744.0,\n",
              "  1466243.625,\n",
              "  1472114.25,\n",
              "  1471553.375,\n",
              "  1482746.625,\n",
              "  1482157.0,\n",
              "  1472240.75,\n",
              "  1469406.5,\n",
              "  1482885.75,\n",
              "  1467693.0,\n",
              "  1468893.25,\n",
              "  1468128.375,\n",
              "  1477911.25,\n",
              "  1470293.5,\n",
              "  1472771.25,\n",
              "  1446436.375,\n",
              "  1484476.0,\n",
              "  1443784.875,\n",
              "  1476398.125,\n",
              "  1465378.625,\n",
              "  1477580.125,\n",
              "  1470515.625,\n",
              "  1462848.25,\n",
              "  1476201.625,\n",
              "  1480765.375,\n",
              "  1457967.875,\n",
              "  1466725.0,\n",
              "  1473465.875,\n",
              "  1474206.125,\n",
              "  1452747.25,\n",
              "  1464201.0,\n",
              "  1444396.125,\n",
              "  1450411.375,\n",
              "  1454778.125,\n",
              "  1469939.75,\n",
              "  1449420.625,\n",
              "  1482858.5,\n",
              "  1470766.125,\n",
              "  1462199.375,\n",
              "  1462303.375,\n",
              "  1462002.75,\n",
              "  1456415.25,\n",
              "  1467708.625,\n",
              "  1478342.25,\n",
              "  1481763.625,\n",
              "  1441692.75,\n",
              "  1455547.5,\n",
              "  1464164.125,\n",
              "  1471124.125,\n",
              "  1455486.375,\n",
              "  1441951.5,\n",
              "  1483067.125,\n",
              "  1456591.625,\n",
              "  1459489.75,\n",
              "  1464084.875,\n",
              "  1469073.375,\n",
              "  1468421.0,\n",
              "  1474352.75,\n",
              "  1458574.5,\n",
              "  1458686.0,\n",
              "  1476374.125,\n",
              "  1457157.375,\n",
              "  1462553.125,\n",
              "  1457947.625,\n",
              "  1464159.625,\n",
              "  1475276.0,\n",
              "  1469097.0,\n",
              "  1454435.375,\n",
              "  1435807.125,\n",
              "  1453657.125,\n",
              "  1471220.0,\n",
              "  1450739.0,\n",
              "  1472104.5,\n",
              "  1455508.5,\n",
              "  1455603.625,\n",
              "  1452373.0,\n",
              "  1485516.375,\n",
              "  1421529.25,\n",
              "  1458349.375,\n",
              "  1466016.75,\n",
              "  1465919.5,\n",
              "  1475963.0,\n",
              "  1449044.625,\n",
              "  1461460.0,\n",
              "  1452380.375,\n",
              "  1453063.375,\n",
              "  1433369.125,\n",
              "  1447226.5,\n",
              "  1463285.375,\n",
              "  1471065.625,\n",
              "  1455639.375,\n",
              "  1472818.75,\n",
              "  1439254.75,\n",
              "  1455098.625,\n",
              "  1457141.875,\n",
              "  1440309.5,\n",
              "  1465734.5,\n",
              "  1459703.375,\n",
              "  1443785.25,\n",
              "  1437608.5,\n",
              "  1464066.75,\n",
              "  1476212.75,\n",
              "  1456770.75,\n",
              "  1435092.125,\n",
              "  1439548.875,\n",
              "  1455792.0,\n",
              "  1440763.0,\n",
              "  1462734.5,\n",
              "  1457110.0,\n",
              "  1459679.625,\n",
              "  1447823.5,\n",
              "  1437948.875,\n",
              "  1433061.125,\n",
              "  1451311.375,\n",
              "  1461183.5,\n",
              "  1470910.75,\n",
              "  1442175.375,\n",
              "  1451431.25,\n",
              "  1452609.75,\n",
              "  1465045.875,\n",
              "  1446904.5,\n",
              "  1452361.875,\n",
              "  1460103.75,\n",
              "  1472314.0,\n",
              "  1437997.125,\n",
              "  1445962.5,\n",
              "  1453578.75,\n",
              "  1471091.75,\n",
              "  1459239.875,\n",
              "  1479086.375,\n",
              "  1451100.75,\n",
              "  1447483.875,\n",
              "  1444632.25,\n",
              "  1450427.75,\n",
              "  1465336.125,\n",
              "  1451489.625,\n",
              "  1448046.125,\n",
              "  1451032.625,\n",
              "  1444628.5,\n",
              "  1449571.25,\n",
              "  1455792.875,\n",
              "  1442704.5,\n",
              "  1449464.375,\n",
              "  1462257.25,\n",
              "  1467965.25,\n",
              "  1435433.125,\n",
              "  1470503.375,\n",
              "  1421192.375,\n",
              "  1456653.375,\n",
              "  1453086.375,\n",
              "  1432955.875,\n",
              "  1431060.25,\n",
              "  1468357.375,\n",
              "  1436790.75,\n",
              "  1457907.125,\n",
              "  1437194.75,\n",
              "  1451802.25,\n",
              "  1460650.5,\n",
              "  1439991.25,\n",
              "  1443959.625,\n",
              "  1464823.375,\n",
              "  1429784.75,\n",
              "  1462602.0,\n",
              "  1447711.875,\n",
              "  1440678.0,\n",
              "  1451983.375,\n",
              "  1454751.375,\n",
              "  1449327.25,\n",
              "  1447071.75,\n",
              "  1438770.375,\n",
              "  1456949.125,\n",
              "  1436567.75,\n",
              "  1444984.875,\n",
              "  1434581.875,\n",
              "  1447516.5,\n",
              "  1433396.625,\n",
              "  1449978.75,\n",
              "  1443497.25,\n",
              "  1446761.0,\n",
              "  1418039.375,\n",
              "  1427666.25,\n",
              "  1455861.5,\n",
              "  1433570.75,\n",
              "  1444355.5,\n",
              "  1458216.5,\n",
              "  1436722.875,\n",
              "  1448134.125,\n",
              "  1426942.5,\n",
              "  1419994.0,\n",
              "  1435454.5,\n",
              "  1452243.875,\n",
              "  1429875.625,\n",
              "  1460825.875,\n",
              "  1417028.625,\n",
              "  1426161.125,\n",
              "  1449283.125,\n",
              "  1436837.875,\n",
              "  1444536.125,\n",
              "  1431849.125,\n",
              "  1434611.0,\n",
              "  1435799.875,\n",
              "  1438534.25,\n",
              "  1441141.0,\n",
              "  1430789.0,\n",
              "  1453357.875,\n",
              "  1441169.375,\n",
              "  1440411.0,\n",
              "  1426975.625,\n",
              "  1451906.625,\n",
              "  1420869.0,\n",
              "  1428391.75,\n",
              "  1457892.5,\n",
              "  1456055.625,\n",
              "  1451500.375,\n",
              "  1443200.0,\n",
              "  1428875.25,\n",
              "  1421723.375,\n",
              "  1418865.375,\n",
              "  1453230.25,\n",
              "  1422523.625,\n",
              "  1426545.625,\n",
              "  1431819.375,\n",
              "  1433702.125,\n",
              "  1456976.0,\n",
              "  1441563.875,\n",
              "  1449451.0,\n",
              "  1438185.75,\n",
              "  1426088.625,\n",
              "  1424107.125,\n",
              "  1447948.375],\n",
              " 'val_loss': [525391.0625,\n",
              "  547107.8125,\n",
              "  525229.25,\n",
              "  533920.9375,\n",
              "  537977.0625,\n",
              "  529181.25,\n",
              "  516246.65625,\n",
              "  524442.4375,\n",
              "  524798.3125,\n",
              "  550412.625,\n",
              "  550384.3125,\n",
              "  524610.8125,\n",
              "  524476.5,\n",
              "  515635.21875,\n",
              "  550100.625,\n",
              "  541206.5,\n",
              "  545689.1875,\n",
              "  536961.375,\n",
              "  532743.0,\n",
              "  536742.6875,\n",
              "  523827.84375,\n",
              "  540711.5,\n",
              "  510839.59375,\n",
              "  536519.6875,\n",
              "  544968.6875,\n",
              "  549124.5,\n",
              "  535536.875,\n",
              "  544824.9375,\n",
              "  557667.0625,\n",
              "  510230.59375,\n",
              "  535784.1875,\n",
              "  518389.96875,\n",
              "  514179.34375,\n",
              "  518230.65625,\n",
              "  548265.1875,\n",
              "  522672.59375,\n",
              "  535398.125,\n",
              "  526586.1875,\n",
              "  525770.3125,\n",
              "  539226.6875,\n",
              "  530885.8125,\n",
              "  530751.625,\n",
              "  522053.65625,\n",
              "  509177.25,\n",
              "  538183.9375,\n",
              "  534603.5,\n",
              "  534574.8125,\n",
              "  529903.5625,\n",
              "  517033.78125,\n",
              "  555774.6875,\n",
              "  534245.8125,\n",
              "  542770.4375,\n",
              "  534029.375,\n",
              "  529876.75,\n",
              "  512379.59375,\n",
              "  529711.6875,\n",
              "  520873.96875,\n",
              "  532991.5625,\n",
              "  529011.1875,\n",
              "  524756.4375,\n",
              "  519469.90625,\n",
              "  511827.59375,\n",
              "  515922.5,\n",
              "  528660.25,\n",
              "  533100.4375,\n",
              "  515684.125,\n",
              "  520124.65625,\n",
              "  541493.8125,\n",
              "  519515.28125,\n",
              "  524003.5,\n",
              "  519802.15625,\n",
              "  523789.90625,\n",
              "  515128.0,\n",
              "  532413.8125,\n",
              "  519533.21875,\n",
              "  501411.46875,\n",
              "  544914.5,\n",
              "  519241.71875,\n",
              "  523277.09375,\n",
              "  531924.8125,\n",
              "  540414.25,\n",
              "  514415.65625,\n",
              "  522956.34375,\n",
              "  518759.125,\n",
              "  531464.4375,\n",
              "  518547.90625,\n",
              "  531301.0625,\n",
              "  531220.375,\n",
              "  501083.875,\n",
              "  522341.125,\n",
              "  525797.5625,\n",
              "  543665.8125,\n",
              "  530810.4375,\n",
              "  513414.90625,\n",
              "  513818.21875,\n",
              "  521962.59375,\n",
              "  543147.6875,\n",
              "  517588.28125,\n",
              "  551633.4375,\n",
              "  525020.5625,\n",
              "  542816.1875,\n",
              "  542835.375,\n",
              "  517240.03125,\n",
              "  533973.5,\n",
              "  517129.53125,\n",
              "  504298.53125,\n",
              "  516918.78125,\n",
              "  512308.71875,\n",
              "  512701.46875,\n",
              "  542070.0,\n",
              "  520708.875,\n",
              "  498623.5,\n",
              "  520496.84375,\n",
              "  516310.84375,\n",
              "  520337.75,\n",
              "  528887.375,\n",
              "  511650.40625,\n",
              "  511520.46875,\n",
              "  541324.625,\n",
              "  511777.125,\n",
              "  515801.71875,\n",
              "  528448.625,\n",
              "  511175.25,\n",
              "  540330.75,\n",
              "  524050.96875,\n",
              "  536740.9375,\n",
              "  515323.78125,\n",
              "  519298.375,\n",
              "  540496.1875,\n",
              "  527849.9375,\n",
              "  527718.6875,\n",
              "  527588.4375,\n",
              "  523406.375,\n",
              "  506161.84375,\n",
              "  510588.125,\n",
              "  523114.75,\n",
              "  531284.0,\n",
              "  493198.09375,\n",
              "  522970.90625,\n",
              "  514242.09375,\n",
              "  530959.3125,\n",
              "  505536.59375,\n",
              "  514103.84375,\n",
              "  539261.5625,\n",
              "  522487.5,\n",
              "  526407.125,\n",
              "  526326.125,\n",
              "  538979.125,\n",
              "  517625.34375,\n",
              "  538765.0625,\n",
              "  504881.75,\n",
              "  517384.75,\n",
              "  529984.0625,\n",
              "  508016.375,\n",
              "  513050.375,\n",
              "  508584.875,\n",
              "  517035.65625,\n",
              "  521343.0,\n",
              "  512782.40625,\n",
              "  500084.96875,\n",
              "  525241.3125,\n",
              "  521115.65625,\n",
              "  533650.1875,\n",
              "  512339.65625,\n",
              "  512307.71875,\n",
              "  503663.625,\n",
              "  520283.40625,\n",
              "  520633.75,\n",
              "  507899.40625,\n",
              "  516001.625,\n",
              "  545642.25,\n",
              "  528490.9375,\n",
              "  527640.1875,\n",
              "  532701.0,\n",
              "  520070.875,\n",
              "  515572.09375,\n",
              "  515446.46875,\n",
              "  523916.84375,\n",
              "  518699.0,\n",
              "  511167.0,\n",
              "  515128.09375,\n",
              "  515048.65625,\n",
              "  510883.15625,\n",
              "  531883.0,\n",
              "  493689.09375,\n",
              "  531858.9375,\n",
              "  514652.28125,\n",
              "  501274.03125,\n",
              "  522982.53125,\n",
              "  506198.46875,\n",
              "  509574.84375,\n",
              "  501684.71875,\n",
              "  501560.09375,\n",
              "  526709.5625,\n",
              "  522546.78125,\n",
              "  518430.15625,\n",
              "  517941.28125,\n",
              "  534914.25,\n",
              "  526261.5625,\n",
              "  501064.75,\n",
              "  505069.34375,\n",
              "  543152.75,\n",
              "  509304.75,\n",
              "  513305.78125,\n",
              "  509100.84375,\n",
              "  513146.59375,\n",
              "  496441.0,\n",
              "  508909.34375,\n",
              "  491853.84375,\n",
              "  521340.03125,\n",
              "  500208.90625,\n",
              "  516749.21875,\n",
              "  521098.21875,\n",
              "  508436.25,\n",
              "  483239.625,\n",
              "  507605.125,\n",
              "  524898.8125,\n",
              "  486417.40625,\n",
              "  512071.28125,\n",
              "  499510.96875,\n",
              "  507928.90625,\n",
              "  503430.59375,\n",
              "  520248.90625,\n",
              "  507693.0,\n",
              "  511642.71875,\n",
              "  520052.90625,\n",
              "  519929.59375,\n",
              "  511453.53125,\n",
              "  532418.0625,\n",
              "  502764.5,\n",
              "  519654.15625,\n",
              "  498538.34375,\n",
              "  527974.625,\n",
              "  510890.0,\n",
              "  489873.96875,\n",
              "  490132.125,\n",
              "  509981.84375,\n",
              "  531639.6875,\n",
              "  502445.71875,\n",
              "  518889.5,\n",
              "  514784.84375,\n",
              "  531222.6875,\n",
              "  518648.15625,\n",
              "  522637.375,\n",
              "  497580.0,\n",
              "  518409.21875,\n",
              "  526747.3125,\n",
              "  513898.75,\n",
              "  501662.625,\n",
              "  501259.78125,\n",
              "  509552.15625,\n",
              "  526342.8125,\n",
              "  517850.03125,\n",
              "  530290.1875,\n",
              "  509281.625,\n",
              "  521718.34375,\n",
              "  521639.125,\n",
              "  513387.0,\n",
              "  495829.0,\n",
              "  500804.78125,\n",
              "  492282.65625,\n",
              "  492161.84375,\n",
              "  525451.875,\n",
              "  496114.90625,\n",
              "  508496.71875,\n",
              "  499979.46875,\n",
              "  516779.09375,\n",
              "  504201.46875,\n",
              "  537420.5,\n",
              "  508151.25,\n",
              "  537343.5625,\n",
              "  516337.65625,\n",
              "  512198.53125,\n",
              "  503733.96875,\n",
              "  507714.125,\n",
              "  482710.71875,\n",
              "  520043.46875,\n",
              "  515861.59375,\n",
              "  528260.9375,\n",
              "  503308.46875,\n",
              "  528100.1875,\n",
              "  515542.59375,\n",
              "  511407.15625,\n",
              "  527856.0,\n",
              "  502833.40625,\n",
              "  523639.15625,\n",
              "  490252.21875,\n",
              "  510795.15625,\n",
              "  514987.28125,\n",
              "  523402.03125,\n",
              "  498055.375,\n",
              "  498276.03125,\n",
              "  518765.84375,\n",
              "  506228.625,\n",
              "  493738.125,\n",
              "  506073.375,\n",
              "  514394.34375,\n",
              "  514357.75,\n",
              "  505839.84375,\n",
              "  505049.0,\n",
              "  497538.125,\n",
              "  526401.5625,\n",
              "  526278.1875,\n",
              "  517849.34375,\n",
              "  480534.71875,\n",
              "  509631.34375,\n",
              "  525954.375,\n",
              "  472209.90625,\n",
              "  496674.78125,\n",
              "  476109.15625,\n",
              "  509192.875,\n",
              "  492436.53125,\n",
              "  513042.03125,\n",
              "  492241.84375,\n",
              "  517013.25,\n",
              "  500464.71875,\n",
              "  516229.59375,\n",
              "  525066.25,\n",
              "  512651.53125,\n",
              "  524986.75,\n",
              "  504083.46875,\n",
              "  512412.25,\n",
              "  503926.53125,\n",
              "  512254.84375,\n",
              "  508046.84375,\n",
              "  512095.78125,\n",
              "  487247.59375,\n",
              "  495491.59375,\n",
              "  491139.90625,\n",
              "  503464.625,\n",
              "  507658.78125,\n",
              "  511623.21875,\n",
              "  511502.65625,\n",
              "  503072.125,\n",
              "  490642.625,\n",
              "  511264.53125,\n",
              "  502877.875,\n",
              "  511107.0,\n",
              "  515150.84375,\n",
              "  494342.09375,\n",
              "  510912.0,\n",
              "  502490.40625,\n",
              "  502412.59375,\n",
              "  519054.40625,\n",
              "  506295.65625,\n",
              "  506437.5,\n",
              "  494024.375,\n",
              "  509692.65625,\n",
              "  510239.25,\n",
              "  501868.625,\n",
              "  510082.59375,\n",
              "  496970.25,\n",
              "  476946.875,\n",
              "  493272.59375,\n",
              "  505732.28125,\n",
              "  485008.28125,\n",
              "  513726.125,\n",
              "  492969.34375,\n",
              "  509452.65625,\n",
              "  513367.71875,\n",
              "  500977.625,\n",
              "  488629.28125,\n",
              "  509217.46875,\n",
              "  517413.375,\n",
              "  509018.625,\n",
              "  504868.75,\n",
              "  496559.125,\n",
              "  512815.0,\n",
              "  512657.125,\n",
              "  508665.78125,\n",
              "  500241.84375,\n",
              "  496136.15625,\n",
              "  504282.625,\n",
              "  483648.15625,\n",
              "  487603.625,\n",
              "  499857.625,\n",
              "  512106.25,\n",
              "  520324.15625,\n",
              "  495558.15625,\n",
              "  495519.46875,\n",
              "  516017.25,\n",
              "  507606.21875,\n",
              "  503304.15625,\n",
              "  495211.90625,\n",
              "  519686.15625,\n",
              "  511397.71875,\n",
              "  482708.09375,\n",
              "  527848.5625,\n",
              "  527650.9375,\n",
              "  506980.75,\n",
              "  494674.65625,\n",
              "  510849.65625,\n",
              "  482258.28125,\n",
              "  502454.96875,\n",
              "  482069.90625,\n",
              "  506589.65625,\n",
              "  498198.75,\n",
              "  518725.125,\n",
              "  505727.875,\n",
              "  502066.84375,\n",
              "  526716.5625,\n",
              "  497851.84375,\n",
              "  509984.625,\n",
              "  477409.65625,\n",
              "  518166.40625,\n",
              "  497543.84375,\n",
              "  513948.875,\n",
              "  472206.78125,\n",
              "  513751.15625,\n",
              "  501434.25,\n",
              "  501140.03125,\n",
              "  493101.34375,\n",
              "  500985.59375,\n",
              "  505178.75,\n",
              "  492794.375,\n",
              "  513233.0,\n",
              "  508922.96875,\n",
              "  483732.53125,\n",
              "  480302.09375,\n",
              "  488222.71875,\n",
              "  516189.59375,\n",
              "  504553.53125,\n",
              "  500250.875,\n",
              "  504322.25,\n",
              "  496119.34375,\n",
              "  512438.59375,\n",
              "  495927.375,\n",
              "  491799.40625,\n",
              "  495772.71875,\n",
              "  512119.375,\n",
              "  499631.78125,\n",
              "  507783.65625,\n",
              "  507705.25,\n",
              "  515816.65625,\n",
              "  507512.375,\n",
              "  506734.71875,\n",
              "  499370.0,\n",
              "  490211.84375,\n",
              "  499214.65625,\n",
              "  507084.65625,\n",
              "  499059.21875,\n",
              "  490766.78125,\n",
              "  486681.71875,\n",
              "  515060.21875,\n",
              "  510971.15625,\n",
              "  494434.40625,\n",
              "  502605.03125,\n",
              "  482103.25,\n",
              "  514626.90625,\n",
              "  498172.84375,\n",
              "  502293.25,\n",
              "  497983.25,\n",
              "  514345.625,\n",
              "  514193.65625,\n",
              "  501944.75,\n",
              "  514035.84375,\n",
              "  510022.5,\n",
              "  485394.5,\n",
              "  469115.625,\n",
              "  497405.03125,\n",
              "  501480.75,\n",
              "  505370.75,\n",
              "  509584.875,\n",
              "  497099.09375,\n",
              "  493021.28125,\n",
              "  496946.0,\n",
              "  488864.0,\n",
              "  504939.28125,\n",
              "  509040.40625,\n",
              "  492601.46875,\n",
              "  508918.28125,\n",
              "  500629.0,\n",
              "  500551.03125,\n",
              "  496471.46875,\n",
              "  520671.03125,\n",
              "  500317.84375,\n",
              "  512375.625,\n",
              "  492028.15625,\n",
              "  500085.625,\n",
              "  467579.53125,\n",
              "  479635.59375,\n",
              "  495757.25,\n",
              "  487650.21875,\n",
              "  483443.90625,\n",
              "  499690.0,\n",
              "  483293.5,\n",
              "  479187.0,\n",
              "  491299.46875,\n",
              "  483034.65625,\n",
              "  474967.21875,\n",
              "  487007.46875,\n",
              "  486840.03125,\n",
              "  490921.0,\n",
              "  478700.75,\n",
              "  510990.0,\n",
              "  482545.15625,\n",
              "  506872.875,\n",
              "  474408.46875,\n",
              "  490430.375,\n",
              "  494468.71875],\n",
              " 'val_mse': [525391.0625,\n",
              "  547107.8125,\n",
              "  525229.25,\n",
              "  533920.9375,\n",
              "  537977.0625,\n",
              "  529181.25,\n",
              "  516246.65625,\n",
              "  524442.4375,\n",
              "  524798.3125,\n",
              "  550412.625,\n",
              "  550384.3125,\n",
              "  524610.8125,\n",
              "  524476.5,\n",
              "  515635.21875,\n",
              "  550100.625,\n",
              "  541206.5,\n",
              "  545689.1875,\n",
              "  536961.375,\n",
              "  532743.0,\n",
              "  536742.6875,\n",
              "  523827.84375,\n",
              "  540711.5,\n",
              "  510839.59375,\n",
              "  536519.6875,\n",
              "  544968.6875,\n",
              "  549124.5,\n",
              "  535536.875,\n",
              "  544824.9375,\n",
              "  557667.0625,\n",
              "  510230.59375,\n",
              "  535784.1875,\n",
              "  518389.96875,\n",
              "  514179.34375,\n",
              "  518230.65625,\n",
              "  548265.1875,\n",
              "  522672.59375,\n",
              "  535398.125,\n",
              "  526586.1875,\n",
              "  525770.3125,\n",
              "  539226.6875,\n",
              "  530885.8125,\n",
              "  530751.625,\n",
              "  522053.65625,\n",
              "  509177.25,\n",
              "  538183.9375,\n",
              "  534603.5,\n",
              "  534574.8125,\n",
              "  529903.5625,\n",
              "  517033.78125,\n",
              "  555774.6875,\n",
              "  534245.8125,\n",
              "  542770.4375,\n",
              "  534029.375,\n",
              "  529876.75,\n",
              "  512379.59375,\n",
              "  529711.6875,\n",
              "  520873.96875,\n",
              "  532991.5625,\n",
              "  529011.1875,\n",
              "  524756.4375,\n",
              "  519469.90625,\n",
              "  511827.59375,\n",
              "  515922.5,\n",
              "  528660.25,\n",
              "  533100.4375,\n",
              "  515684.125,\n",
              "  520124.65625,\n",
              "  541493.8125,\n",
              "  519515.28125,\n",
              "  524003.5,\n",
              "  519802.15625,\n",
              "  523789.90625,\n",
              "  515128.0,\n",
              "  532413.8125,\n",
              "  519533.21875,\n",
              "  501411.46875,\n",
              "  544914.5,\n",
              "  519241.71875,\n",
              "  523277.09375,\n",
              "  531924.8125,\n",
              "  540414.25,\n",
              "  514415.65625,\n",
              "  522956.34375,\n",
              "  518759.125,\n",
              "  531464.4375,\n",
              "  518547.90625,\n",
              "  531301.0625,\n",
              "  531220.375,\n",
              "  501083.875,\n",
              "  522341.125,\n",
              "  525797.5625,\n",
              "  543665.8125,\n",
              "  530810.4375,\n",
              "  513414.90625,\n",
              "  513818.21875,\n",
              "  521962.59375,\n",
              "  543147.6875,\n",
              "  517588.28125,\n",
              "  551633.4375,\n",
              "  525020.5625,\n",
              "  542816.1875,\n",
              "  542835.375,\n",
              "  517240.03125,\n",
              "  533973.5,\n",
              "  517129.53125,\n",
              "  504298.53125,\n",
              "  516918.78125,\n",
              "  512308.71875,\n",
              "  512701.46875,\n",
              "  542070.0,\n",
              "  520708.875,\n",
              "  498623.5,\n",
              "  520496.84375,\n",
              "  516310.84375,\n",
              "  520337.75,\n",
              "  528887.375,\n",
              "  511650.40625,\n",
              "  511520.46875,\n",
              "  541324.625,\n",
              "  511777.125,\n",
              "  515801.71875,\n",
              "  528448.625,\n",
              "  511175.25,\n",
              "  540330.75,\n",
              "  524050.96875,\n",
              "  536740.9375,\n",
              "  515323.78125,\n",
              "  519298.375,\n",
              "  540496.1875,\n",
              "  527849.9375,\n",
              "  527718.6875,\n",
              "  527588.4375,\n",
              "  523406.375,\n",
              "  506161.84375,\n",
              "  510588.125,\n",
              "  523114.75,\n",
              "  531284.0,\n",
              "  493198.09375,\n",
              "  522970.90625,\n",
              "  514242.09375,\n",
              "  530959.3125,\n",
              "  505536.59375,\n",
              "  514103.84375,\n",
              "  539261.5625,\n",
              "  522487.5,\n",
              "  526407.125,\n",
              "  526326.125,\n",
              "  538979.125,\n",
              "  517625.34375,\n",
              "  538765.0625,\n",
              "  504881.75,\n",
              "  517384.75,\n",
              "  529984.0625,\n",
              "  508016.375,\n",
              "  513050.375,\n",
              "  508584.875,\n",
              "  517035.65625,\n",
              "  521343.0,\n",
              "  512782.40625,\n",
              "  500084.96875,\n",
              "  525241.3125,\n",
              "  521115.65625,\n",
              "  533650.1875,\n",
              "  512339.65625,\n",
              "  512307.71875,\n",
              "  503663.625,\n",
              "  520283.40625,\n",
              "  520633.75,\n",
              "  507899.40625,\n",
              "  516001.625,\n",
              "  545642.25,\n",
              "  528490.9375,\n",
              "  527640.1875,\n",
              "  532701.0,\n",
              "  520070.875,\n",
              "  515572.09375,\n",
              "  515446.46875,\n",
              "  523916.84375,\n",
              "  518699.0,\n",
              "  511167.0,\n",
              "  515128.09375,\n",
              "  515048.65625,\n",
              "  510883.15625,\n",
              "  531883.0,\n",
              "  493689.09375,\n",
              "  531858.9375,\n",
              "  514652.28125,\n",
              "  501274.03125,\n",
              "  522982.53125,\n",
              "  506198.46875,\n",
              "  509574.84375,\n",
              "  501684.71875,\n",
              "  501560.09375,\n",
              "  526709.5625,\n",
              "  522546.78125,\n",
              "  518430.15625,\n",
              "  517941.28125,\n",
              "  534914.25,\n",
              "  526261.5625,\n",
              "  501064.75,\n",
              "  505069.34375,\n",
              "  543152.75,\n",
              "  509304.75,\n",
              "  513305.78125,\n",
              "  509100.84375,\n",
              "  513146.59375,\n",
              "  496441.0,\n",
              "  508909.34375,\n",
              "  491853.84375,\n",
              "  521340.03125,\n",
              "  500208.90625,\n",
              "  516749.21875,\n",
              "  521098.21875,\n",
              "  508436.25,\n",
              "  483239.625,\n",
              "  507605.125,\n",
              "  524898.8125,\n",
              "  486417.40625,\n",
              "  512071.28125,\n",
              "  499510.96875,\n",
              "  507928.90625,\n",
              "  503430.59375,\n",
              "  520248.90625,\n",
              "  507693.0,\n",
              "  511642.71875,\n",
              "  520052.90625,\n",
              "  519929.59375,\n",
              "  511453.53125,\n",
              "  532418.0625,\n",
              "  502764.5,\n",
              "  519654.15625,\n",
              "  498538.34375,\n",
              "  527974.625,\n",
              "  510890.0,\n",
              "  489873.96875,\n",
              "  490132.125,\n",
              "  509981.84375,\n",
              "  531639.6875,\n",
              "  502445.71875,\n",
              "  518889.5,\n",
              "  514784.84375,\n",
              "  531222.6875,\n",
              "  518648.15625,\n",
              "  522637.375,\n",
              "  497580.0,\n",
              "  518409.21875,\n",
              "  526747.3125,\n",
              "  513898.75,\n",
              "  501662.625,\n",
              "  501259.78125,\n",
              "  509552.15625,\n",
              "  526342.8125,\n",
              "  517850.03125,\n",
              "  530290.1875,\n",
              "  509281.625,\n",
              "  521718.34375,\n",
              "  521639.125,\n",
              "  513387.0,\n",
              "  495829.0,\n",
              "  500804.78125,\n",
              "  492282.65625,\n",
              "  492161.84375,\n",
              "  525451.875,\n",
              "  496114.90625,\n",
              "  508496.71875,\n",
              "  499979.46875,\n",
              "  516779.09375,\n",
              "  504201.46875,\n",
              "  537420.5,\n",
              "  508151.25,\n",
              "  537343.5625,\n",
              "  516337.65625,\n",
              "  512198.53125,\n",
              "  503733.96875,\n",
              "  507714.125,\n",
              "  482710.71875,\n",
              "  520043.46875,\n",
              "  515861.59375,\n",
              "  528260.9375,\n",
              "  503308.46875,\n",
              "  528100.1875,\n",
              "  515542.59375,\n",
              "  511407.15625,\n",
              "  527856.0,\n",
              "  502833.40625,\n",
              "  523639.15625,\n",
              "  490252.21875,\n",
              "  510795.15625,\n",
              "  514987.28125,\n",
              "  523402.03125,\n",
              "  498055.375,\n",
              "  498276.03125,\n",
              "  518765.84375,\n",
              "  506228.625,\n",
              "  493738.125,\n",
              "  506073.375,\n",
              "  514394.34375,\n",
              "  514357.75,\n",
              "  505839.84375,\n",
              "  505049.0,\n",
              "  497538.125,\n",
              "  526401.5625,\n",
              "  526278.1875,\n",
              "  517849.34375,\n",
              "  480534.71875,\n",
              "  509631.34375,\n",
              "  525954.375,\n",
              "  472209.90625,\n",
              "  496674.78125,\n",
              "  476109.15625,\n",
              "  509192.875,\n",
              "  492436.53125,\n",
              "  513042.03125,\n",
              "  492241.84375,\n",
              "  517013.25,\n",
              "  500464.71875,\n",
              "  516229.59375,\n",
              "  525066.25,\n",
              "  512651.53125,\n",
              "  524986.75,\n",
              "  504083.46875,\n",
              "  512412.25,\n",
              "  503926.53125,\n",
              "  512254.84375,\n",
              "  508046.84375,\n",
              "  512095.78125,\n",
              "  487247.59375,\n",
              "  495491.59375,\n",
              "  491139.90625,\n",
              "  503464.625,\n",
              "  507658.78125,\n",
              "  511623.21875,\n",
              "  511502.65625,\n",
              "  503072.125,\n",
              "  490642.625,\n",
              "  511264.53125,\n",
              "  502877.875,\n",
              "  511107.0,\n",
              "  515150.84375,\n",
              "  494342.09375,\n",
              "  510912.0,\n",
              "  502490.40625,\n",
              "  502412.59375,\n",
              "  519054.40625,\n",
              "  506295.65625,\n",
              "  506437.5,\n",
              "  494024.375,\n",
              "  509692.65625,\n",
              "  510239.25,\n",
              "  501868.625,\n",
              "  510082.59375,\n",
              "  496970.25,\n",
              "  476946.875,\n",
              "  493272.59375,\n",
              "  505732.28125,\n",
              "  485008.28125,\n",
              "  513726.125,\n",
              "  492969.34375,\n",
              "  509452.65625,\n",
              "  513367.71875,\n",
              "  500977.625,\n",
              "  488629.28125,\n",
              "  509217.46875,\n",
              "  517413.375,\n",
              "  509018.625,\n",
              "  504868.75,\n",
              "  496559.125,\n",
              "  512815.0,\n",
              "  512657.125,\n",
              "  508665.78125,\n",
              "  500241.84375,\n",
              "  496136.15625,\n",
              "  504282.625,\n",
              "  483648.15625,\n",
              "  487603.625,\n",
              "  499857.625,\n",
              "  512106.25,\n",
              "  520324.15625,\n",
              "  495558.15625,\n",
              "  495519.46875,\n",
              "  516017.25,\n",
              "  507606.21875,\n",
              "  503304.15625,\n",
              "  495211.90625,\n",
              "  519686.15625,\n",
              "  511397.71875,\n",
              "  482708.09375,\n",
              "  527848.5625,\n",
              "  527650.9375,\n",
              "  506980.75,\n",
              "  494674.65625,\n",
              "  510849.65625,\n",
              "  482258.28125,\n",
              "  502454.96875,\n",
              "  482069.90625,\n",
              "  506589.65625,\n",
              "  498198.75,\n",
              "  518725.125,\n",
              "  505727.875,\n",
              "  502066.84375,\n",
              "  526716.5625,\n",
              "  497851.84375,\n",
              "  509984.625,\n",
              "  477409.65625,\n",
              "  518166.40625,\n",
              "  497543.84375,\n",
              "  513948.875,\n",
              "  472206.78125,\n",
              "  513751.15625,\n",
              "  501434.25,\n",
              "  501140.03125,\n",
              "  493101.34375,\n",
              "  500985.59375,\n",
              "  505178.75,\n",
              "  492794.375,\n",
              "  513233.0,\n",
              "  508922.96875,\n",
              "  483732.53125,\n",
              "  480302.09375,\n",
              "  488222.71875,\n",
              "  516189.59375,\n",
              "  504553.53125,\n",
              "  500250.875,\n",
              "  504322.25,\n",
              "  496119.34375,\n",
              "  512438.59375,\n",
              "  495927.375,\n",
              "  491799.40625,\n",
              "  495772.71875,\n",
              "  512119.375,\n",
              "  499631.78125,\n",
              "  507783.65625,\n",
              "  507705.25,\n",
              "  515816.65625,\n",
              "  507512.375,\n",
              "  506734.71875,\n",
              "  499370.0,\n",
              "  490211.84375,\n",
              "  499214.65625,\n",
              "  507084.65625,\n",
              "  499059.21875,\n",
              "  490766.78125,\n",
              "  486681.71875,\n",
              "  515060.21875,\n",
              "  510971.15625,\n",
              "  494434.40625,\n",
              "  502605.03125,\n",
              "  482103.25,\n",
              "  514626.90625,\n",
              "  498172.84375,\n",
              "  502293.25,\n",
              "  497983.25,\n",
              "  514345.625,\n",
              "  514193.65625,\n",
              "  501944.75,\n",
              "  514035.84375,\n",
              "  510022.5,\n",
              "  485394.5,\n",
              "  469115.625,\n",
              "  497405.03125,\n",
              "  501480.75,\n",
              "  505370.75,\n",
              "  509584.875,\n",
              "  497099.09375,\n",
              "  493021.28125,\n",
              "  496946.0,\n",
              "  488864.0,\n",
              "  504939.28125,\n",
              "  509040.40625,\n",
              "  492601.46875,\n",
              "  508918.28125,\n",
              "  500629.0,\n",
              "  500551.03125,\n",
              "  496471.46875,\n",
              "  520671.03125,\n",
              "  500317.84375,\n",
              "  512375.625,\n",
              "  492028.15625,\n",
              "  500085.625,\n",
              "  467579.53125,\n",
              "  479635.59375,\n",
              "  495757.25,\n",
              "  487650.21875,\n",
              "  483443.90625,\n",
              "  499690.0,\n",
              "  483293.5,\n",
              "  479187.0,\n",
              "  491299.46875,\n",
              "  483034.65625,\n",
              "  474967.21875,\n",
              "  487007.46875,\n",
              "  486840.03125,\n",
              "  490921.0,\n",
              "  478700.75,\n",
              "  510990.0,\n",
              "  482545.15625,\n",
              "  506872.875,\n",
              "  474408.46875,\n",
              "  490430.375,\n",
              "  494468.71875]}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "mse = history.history['mse']\n",
        "val_mse = history.history['val_mse']\n",
        "\n",
        "epochs_x = range(len(loss))\n",
        "\n",
        "plt.plot(epochs_x, mse, 'co', label='Training MAE')\n",
        "plt.plot(epochs_x, val_mse, 'k', label='Validation MAE')\n",
        "plt.title('Training and Mean squared error')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "outputId": "bea1347d-0390-485e-f211-47317b4d3e86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "# model.save('./content/drive/My Drive/new/2Class_regression_freeze_500.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# files.download(\"./content/drive/My Drive/new/2Class_regression_freeze_500.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}