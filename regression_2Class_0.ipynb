{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO5j91QLpWPHDM+dYbi9K2n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/regression_2Class_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "_2DRC-anSxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BVIqfqC1DtDt",
        "outputId": "138ac610-17fb-4ce5-f97e-525d2eea9809",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - 2 class Regress.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "gABRUdVwDtBk",
        "outputId": "efb1faae-e1bd-4f56-f38e-31291cef6ecf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "795  796  1-s2.0-S2095268622000210-main   \n",
              "796  797  1-s2.0-S2095268622000210-main   \n",
              "797  798  1-s2.0-S2095268622000210-main   \n",
              "798  799  1-s2.0-S2095268622000210-main   \n",
              "799  800  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "795  Integration of preparation of K, Na-embedded a...   \n",
              "796  Integration of preparation of K, Na-embedded a...   \n",
              "797  Integration of preparation of K, Na-embedded a...   \n",
              "798  Integration of preparation of K, Na-embedded a...   \n",
              "799  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "795  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "796  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "797  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "798  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "799  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-800  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-800  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-800  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-800  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-800  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "795  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-800  301.70   \n",
              "796  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-800  301.70   \n",
              "797  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-800  301.70   \n",
              "798  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-800  301.70   \n",
              "799  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-800  301.70   \n",
              "\n",
              "     Size(mico)  Class_01  \n",
              "0            10         0  \n",
              "1            10         0  \n",
              "2            10         0  \n",
              "3            10         0  \n",
              "4            10         0  \n",
              "..          ...       ...  \n",
              "795          10         0  \n",
              "796          10         0  \n",
              "797          10         0  \n",
              "798          10         0  \n",
              "799          10         0  \n",
              "\n",
              "[800 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-01fab3d2-9675-450f-913d-7ed2659d1c27\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "      <th>Class_01</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>796</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>797</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>798</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>799</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>800</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-01fab3d2-9675-450f-913d-7ed2659d1c27')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-01fab3d2-9675-450f-913d-7ed2659d1c27 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-01fab3d2-9675-450f-913d-7ed2659d1c27');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hist check class"
      ],
      "metadata": {
        "id": "WMazXBQcTMl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "W34NcexJDs_Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist();"
      ],
      "metadata": {
        "id": "Fm07UpEbDs5X",
        "outputId": "a03008c9-f87c-4045-880d-bd0d2f8feea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df['BET']\n",
        "\n",
        "fig, ax = plt.subplots(figsize =(10, 5))\n",
        "ax.hist(a, bins = 200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bxfNFKW2TXAY",
        "outputId": "5e952274-0b02-46bf-e6ee-02415c5263b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0-800','801-3200']\n",
        "len(classes)"
      ],
      "metadata": {
        "id": "TBsLszHgTW-D",
        "outputId": "f8388f34-7945-4014-9795-266a19e713b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# base_dir = '/content/drive/My Drive/new Regress'\n",
        "# os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# # Directories for our training,\n",
        "# # validation and test splits\n",
        "# train_dir = os.path.join(base_dir, 'train')\n",
        "# os.makedirs(train_dir, exist_ok=True)\n",
        "# validation_dir = os.path.join(base_dir, 'validation')\n",
        "# os.makedirs(validation_dir, exist_ok=True)\n",
        "# test_dir = os.path.join(base_dir, 'test')\n",
        "# os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(599,699)]\n",
        "train = df[df['No'].between(1,598)]\n",
        "test = df[df['No'].between(700,800)] \n",
        "\n",
        "# #Path Train\n",
        "# T1_train = train[train['Class']=='0-800']\n",
        "# T1_path_train = T1_train['path_Picture'].tolist() \n",
        "# T2_train = train[train['Class']=='801-3200']\n",
        "# T2_path_train = T2_train['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "# #Path Validation\n",
        "# T1_val = val[val['Class']=='0-800']\n",
        "# T1_path_val = T1_val['path_Picture'].tolist() \n",
        "# T2_val = val[val['Class']=='801-3200']\n",
        "# T2_path_val = T2_val['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "\n",
        "# #Path Test\n",
        "# T1_test = test[test['Class']=='0-800']\n",
        "# T1_path_test = T1_test['path_Picture'].tolist() \n",
        "# T2_test = test[test['Class']=='801-3200']\n",
        "# T2_path_test = T2_test['path_Picture'].tolist() \n"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "r9N_9sFL1hYB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/content/drive/My Drive/new Regress'\n",
        "os.chdir(DATA_PATH)\n",
        "train_dir = os.path.join(DATA_PATH, 'train')\n",
        "print(train_dir)\n",
        "validation_dir = os.path.join(DATA_PATH, 'validation')\n",
        "print(validation_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyxDPsEi6yYJ",
        "outputId": "eeecb1f5-c782-4504-ce3d-4c9cc30f4fe0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/new Regress/train\n",
            "/content/drive/My Drive/new Regress/validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_train\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_test\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fnames = T1_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n",
        "\n",
        "# fnames = T2_path_val\n",
        "# for fname in fnames:\n",
        "#     dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "#     shutil.copyfile(fname, dst)\n"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "# print('total training 2 images:', len(os.listdir(train_2_dir)),'\\n')\n",
        "\n",
        "# print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "# print('total validation 2 images:', len(os.listdir(validation_2_dir)),'\\n')\n",
        "\n",
        "# print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "# print('total test 2 images:', len(os.listdir(test_2_dir)),'\\n')\n"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "tYeLut2ByGeC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 500 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 598  # จำนวนภาพ Train\n",
        "NUM_TEST = 101 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.3\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "3ab9ece4-5c41-4c0a-fb92-4ce404cd9584",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'efficientnet_keras_transfer_learning'...\n",
            "remote: Enumerating objects: 837, done.\u001b[K\n",
            "remote: Counting objects: 100% (359/359), done.\u001b[K\n",
            "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
            "remote: Total 837 (delta 255), reused 328 (delta 235), pack-reused 478\u001b[K\n",
            "Receiving objects: 100% (837/837), 13.82 MiB | 38.45 MiB/s, done.\n",
            "Resolving deltas: 100% (495/495), done.\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a1c96c2-7f64-456d-a69e-36fc6e5c63d4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "2d8bd87e-d2ab-4ec6-c2e1-652ae8f84eb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(2, activation='softmax', name=\"fc_out\"))\n",
        "model.add(layers.Dense(1))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "outputId": "f3403d66-7791-4675-bb3b-3428687ad2cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 2)                 2562      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,052,129\n",
            "Trainable params: 4,010,113\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "outputId": "0760fd42-0aca-47e3-9f71-1dbcc81d6f0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 215\n",
            "This is the number of trainable layers after freezing the conv base: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe = train,\n",
        "        directory = train_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'BET',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)\n",
        "\n",
        "validation_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe = val,\n",
        "        directory = validation_dir,\n",
        "        x_col = 'path_Picture',\n",
        "        y_col = 'BET',\n",
        "        class_mode = 'other',\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size)"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "outputId": "2f8afc68-8289-4ea4-8c0e-d47504294db4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 598 validated image filenames.\n",
            "Found 101 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer=Adam(lr=2e-6),\n",
        "              metrics=['mse'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40202b09-8a7b-49ae-b24f-2390a1c5ef06"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "<ipython-input-28-d06c655a9a87>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "37/37 [==============================] - 65s 1s/step - loss: 1516518.7500 - mse: 1516518.7500 - val_loss: 500053.4062 - val_mse: 500053.4062\n",
            "Epoch 2/500\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 1506363.1250 - mse: 1506363.1250 - val_loss: 521621.4062 - val_mse: 521621.4062\n",
            "Epoch 3/500\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 1507930.0000 - mse: 1507930.0000 - val_loss: 547272.3750 - val_mse: 547272.3750\n",
            "Epoch 4/500\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1508656.6250 - mse: 1508656.6250 - val_loss: 538520.4375 - val_mse: 538520.4375\n",
            "Epoch 5/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1505641.5000 - mse: 1505641.5000 - val_loss: 538382.4375 - val_mse: 538382.4375\n",
            "Epoch 6/500\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1492480.3750 - mse: 1492480.3750 - val_loss: 547021.9375 - val_mse: 547021.9375\n",
            "Epoch 7/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1514817.1250 - mse: 1514817.1250 - val_loss: 508306.4688 - val_mse: 508306.4688\n",
            "Epoch 8/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1508224.5000 - mse: 1508224.5000 - val_loss: 550995.0625 - val_mse: 550995.0625\n",
            "Epoch 9/500\n",
            "37/37 [==============================] - 3s 71ms/step - loss: 1496289.6250 - mse: 1496289.6250 - val_loss: 542301.6875 - val_mse: 542301.6875\n",
            "Epoch 10/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 1516412.8750 - mse: 1516412.8750 - val_loss: 520485.1250 - val_mse: 520485.1250\n",
            "Epoch 11/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 1521321.1250 - mse: 1521321.1250 - val_loss: 537996.4375 - val_mse: 537996.4375\n",
            "Epoch 12/500\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 1526450.7500 - mse: 1526450.7500 - val_loss: 520378.5938 - val_mse: 520378.5938\n",
            "Epoch 13/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 1521686.0000 - mse: 1521686.0000 - val_loss: 516159.1562 - val_mse: 516159.1562\n",
            "Epoch 14/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1504394.8750 - mse: 1504394.8750 - val_loss: 546458.7500 - val_mse: 546458.7500\n",
            "Epoch 15/500\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 1501763.5000 - mse: 1501763.5000 - val_loss: 524655.2500 - val_mse: 524655.2500\n",
            "Epoch 16/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1503003.8750 - mse: 1503003.8750 - val_loss: 528765.9375 - val_mse: 528765.9375\n",
            "Epoch 17/500\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1491755.0000 - mse: 1491755.0000 - val_loss: 528740.6875 - val_mse: 528740.6875\n",
            "Epoch 18/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1489553.1250 - mse: 1489553.1250 - val_loss: 528604.3125 - val_mse: 528604.3125\n",
            "Epoch 19/500\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 1512119.1250 - mse: 1512119.1250 - val_loss: 537227.5625 - val_mse: 537227.5625\n",
            "Epoch 20/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1494952.7500 - mse: 1494952.7500 - val_loss: 520223.1250 - val_mse: 520223.1250\n",
            "Epoch 21/500\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1504440.2500 - mse: 1504440.2500 - val_loss: 541254.6875 - val_mse: 541254.6875\n",
            "Epoch 22/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1500903.2500 - mse: 1500903.2500 - val_loss: 536980.8125 - val_mse: 536980.8125\n",
            "Epoch 23/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1501924.5000 - mse: 1501924.5000 - val_loss: 511174.0312 - val_mse: 511174.0312\n",
            "Epoch 24/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1498644.0000 - mse: 1498644.0000 - val_loss: 523983.2500 - val_mse: 523983.2500\n",
            "Epoch 25/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1499875.7500 - mse: 1499875.7500 - val_loss: 515151.4688 - val_mse: 515151.4688\n",
            "Epoch 26/500\n",
            "37/37 [==============================] - 3s 72ms/step - loss: 1518455.2500 - mse: 1518455.2500 - val_loss: 545402.3750 - val_mse: 545402.3750\n",
            "Epoch 27/500\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1510430.3750 - mse: 1510430.3750 - val_loss: 545317.5625 - val_mse: 545317.5625\n",
            "Epoch 28/500\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 1515438.1250 - mse: 1515438.1250 - val_loss: 514964.8750 - val_mse: 514964.8750\n",
            "Epoch 29/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1498455.7500 - mse: 1498455.7500 - val_loss: 523577.9688 - val_mse: 523577.9688\n",
            "Epoch 30/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 1499913.3750 - mse: 1499913.3750 - val_loss: 549252.9375 - val_mse: 549252.9375\n",
            "Epoch 31/500\n",
            "37/37 [==============================] - 4s 76ms/step - loss: 1504160.8750 - mse: 1504160.8750 - val_loss: 536346.5625 - val_mse: 536346.5625\n",
            "Epoch 32/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1508111.1250 - mse: 1508111.1250 - val_loss: 527522.1875 - val_mse: 527522.1875\n",
            "Epoch 33/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1506441.3750 - mse: 1506441.3750 - val_loss: 527387.8125 - val_mse: 527387.8125\n",
            "Epoch 34/500\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 1505892.6250 - mse: 1505892.6250 - val_loss: 519042.0938 - val_mse: 519042.0938\n",
            "Epoch 35/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 1526953.6250 - mse: 1526953.6250 - val_loss: 540093.9375 - val_mse: 540093.9375\n",
            "Epoch 36/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1521998.2500 - mse: 1521998.2500 - val_loss: 522956.6250 - val_mse: 522956.6250\n",
            "Epoch 37/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 1529106.3750 - mse: 1529106.3750 - val_loss: 539137.0000 - val_mse: 539137.0000\n",
            "Epoch 38/500\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1509687.6250 - mse: 1509687.6250 - val_loss: 531164.4375 - val_mse: 531164.4375\n",
            "Epoch 39/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1498651.7500 - mse: 1498651.7500 - val_loss: 518218.2500 - val_mse: 518218.2500\n",
            "Epoch 40/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1513464.6250 - mse: 1513464.6250 - val_loss: 534760.8125 - val_mse: 534760.8125\n",
            "Epoch 41/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1501841.6250 - mse: 1501841.6250 - val_loss: 531284.2500 - val_mse: 531284.2500\n",
            "Epoch 42/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1514421.2500 - mse: 1514421.2500 - val_loss: 539515.2500 - val_mse: 539515.2500\n",
            "Epoch 43/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1511862.5000 - mse: 1511862.5000 - val_loss: 509169.9688 - val_mse: 509169.9688\n",
            "Epoch 44/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1494262.8750 - mse: 1494262.8750 - val_loss: 539351.0625 - val_mse: 539351.0625\n",
            "Epoch 45/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1514574.0000 - mse: 1514574.0000 - val_loss: 530541.5000 - val_mse: 530541.5000\n",
            "Epoch 46/500\n",
            "37/37 [==============================] - 8s 205ms/step - loss: 1501229.7500 - mse: 1501229.7500 - val_loss: 535057.3125 - val_mse: 535057.3125\n",
            "Epoch 47/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1518473.3750 - mse: 1518473.3750 - val_loss: 526199.6250 - val_mse: 526199.6250\n",
            "Epoch 48/500\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1510203.8750 - mse: 1510203.8750 - val_loss: 543562.6875 - val_mse: 543562.6875\n",
            "Epoch 49/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1489474.0000 - mse: 1489474.0000 - val_loss: 534811.0000 - val_mse: 534811.0000\n",
            "Epoch 50/500\n",
            "37/37 [==============================] - 9s 216ms/step - loss: 1489945.1250 - mse: 1489945.1250 - val_loss: 526062.4375 - val_mse: 526062.4375\n",
            "Epoch 51/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 1512915.1250 - mse: 1512915.1250 - val_loss: 546704.6875 - val_mse: 546704.6875\n",
            "Epoch 52/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1483195.7500 - mse: 1483195.7500 - val_loss: 534511.5625 - val_mse: 534511.5625\n",
            "Epoch 53/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1499369.3750 - mse: 1499369.3750 - val_loss: 521692.7188 - val_mse: 521692.7188\n",
            "Epoch 54/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1514341.3750 - mse: 1514341.3750 - val_loss: 530275.9375 - val_mse: 530275.9375\n",
            "Epoch 55/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1496078.2500 - mse: 1496078.2500 - val_loss: 508641.5938 - val_mse: 508641.5938\n",
            "Epoch 56/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1493722.7500 - mse: 1493722.7500 - val_loss: 530112.3125 - val_mse: 530112.3125\n",
            "Epoch 57/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1503530.0000 - mse: 1503530.0000 - val_loss: 529924.4375 - val_mse: 529924.4375\n",
            "Epoch 58/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1500965.2500 - mse: 1500965.2500 - val_loss: 521240.5312 - val_mse: 521240.5312\n",
            "Epoch 59/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1503888.7500 - mse: 1503888.7500 - val_loss: 516575.5938 - val_mse: 516575.5938\n",
            "Epoch 60/500\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 1496408.2500 - mse: 1496408.2500 - val_loss: 533856.1875 - val_mse: 533856.1875\n",
            "Epoch 61/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1495677.5000 - mse: 1495677.5000 - val_loss: 542480.3125 - val_mse: 542480.3125\n",
            "Epoch 62/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1503660.1250 - mse: 1503660.1250 - val_loss: 529621.3125 - val_mse: 529621.3125\n",
            "Epoch 63/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1482232.0000 - mse: 1482232.0000 - val_loss: 533663.3125 - val_mse: 533663.3125\n",
            "Epoch 64/500\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1502316.3750 - mse: 1502316.3750 - val_loss: 524931.9375 - val_mse: 524931.9375\n",
            "Epoch 65/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1520752.1250 - mse: 1520752.1250 - val_loss: 520728.7812 - val_mse: 520728.7812\n",
            "Epoch 66/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1520212.0000 - mse: 1520212.0000 - val_loss: 533364.0625 - val_mse: 533364.0625\n",
            "Epoch 67/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1500745.8750 - mse: 1500745.8750 - val_loss: 536775.1875 - val_mse: 536775.1875\n",
            "Epoch 68/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1490273.1250 - mse: 1490273.1250 - val_loss: 541898.0625 - val_mse: 541898.0625\n",
            "Epoch 69/500\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 1502591.8750 - mse: 1502591.8750 - val_loss: 520302.8438 - val_mse: 520302.8438\n",
            "Epoch 70/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1483654.1250 - mse: 1483654.1250 - val_loss: 520327.0000 - val_mse: 520327.0000\n",
            "Epoch 71/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1508383.5000 - mse: 1508383.5000 - val_loss: 554463.0000 - val_mse: 554463.0000\n",
            "Epoch 72/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1499282.5000 - mse: 1499282.5000 - val_loss: 532823.4375 - val_mse: 532823.4375\n",
            "Epoch 73/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1491994.3750 - mse: 1491994.3750 - val_loss: 524102.7188 - val_mse: 524102.7188\n",
            "Epoch 74/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1506359.1250 - mse: 1506359.1250 - val_loss: 528643.5625 - val_mse: 528643.5625\n",
            "Epoch 75/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1503887.2500 - mse: 1503887.2500 - val_loss: 511238.9062 - val_mse: 511238.9062\n",
            "Epoch 76/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 1501965.1250 - mse: 1501965.1250 - val_loss: 541130.5625 - val_mse: 541130.5625\n",
            "Epoch 77/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1506014.8750 - mse: 1506014.8750 - val_loss: 523884.3750 - val_mse: 523884.3750\n",
            "Epoch 78/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1514910.8750 - mse: 1514910.8750 - val_loss: 523699.7188 - val_mse: 523699.7188\n",
            "Epoch 79/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1504018.7500 - mse: 1504018.7500 - val_loss: 523671.5938 - val_mse: 523671.5938\n",
            "Epoch 80/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 1468060.2500 - mse: 1468060.2500 - val_loss: 514961.3438 - val_mse: 514961.3438\n",
            "Epoch 81/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 1500011.5000 - mse: 1500011.5000 - val_loss: 506647.6562 - val_mse: 506647.6562\n",
            "Epoch 82/500\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1507869.1250 - mse: 1507869.1250 - val_loss: 506125.8750 - val_mse: 506125.8750\n",
            "Epoch 83/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1496374.1250 - mse: 1496374.1250 - val_loss: 531926.3125 - val_mse: 531926.3125\n",
            "Epoch 84/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 1504766.6250 - mse: 1504766.6250 - val_loss: 540572.1875 - val_mse: 540572.1875\n",
            "Epoch 85/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1485917.6250 - mse: 1485917.6250 - val_loss: 540540.3125 - val_mse: 540540.3125\n",
            "Epoch 86/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1473575.8750 - mse: 1473575.8750 - val_loss: 510371.5000 - val_mse: 510371.5000\n",
            "Epoch 87/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1486936.2500 - mse: 1486936.2500 - val_loss: 531074.0625 - val_mse: 531074.0625\n",
            "Epoch 88/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1496843.6250 - mse: 1496843.6250 - val_loss: 510162.3438 - val_mse: 510162.3438\n",
            "Epoch 89/500\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1492259.5000 - mse: 1492259.5000 - val_loss: 531539.3125 - val_mse: 531539.3125\n",
            "Epoch 90/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1501368.5000 - mse: 1501368.5000 - val_loss: 531459.1250 - val_mse: 531459.1250\n",
            "Epoch 91/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1488260.0000 - mse: 1488260.0000 - val_loss: 518594.8750 - val_mse: 518594.8750\n",
            "Epoch 92/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1497936.5000 - mse: 1497936.5000 - val_loss: 531295.6875 - val_mse: 531295.6875\n",
            "Epoch 93/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1503674.0000 - mse: 1503674.0000 - val_loss: 522548.0000 - val_mse: 522548.0000\n",
            "Epoch 94/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1498933.6250 - mse: 1498933.6250 - val_loss: 543859.4375 - val_mse: 543859.4375\n",
            "Epoch 95/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1487127.3750 - mse: 1487127.3750 - val_loss: 522388.7500 - val_mse: 522388.7500\n",
            "Epoch 96/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1506876.7500 - mse: 1506876.7500 - val_loss: 522256.4062 - val_mse: 522256.4062\n",
            "Epoch 97/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1495722.7500 - mse: 1495722.7500 - val_loss: 522226.7812 - val_mse: 522226.7812\n",
            "Epoch 98/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1479400.5000 - mse: 1479400.5000 - val_loss: 530805.8750 - val_mse: 530805.8750\n",
            "Epoch 99/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1499250.7500 - mse: 1499250.7500 - val_loss: 539281.0000 - val_mse: 539281.0000\n",
            "Epoch 100/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 1467238.2500 - mse: 1467238.2500 - val_loss: 552016.3125 - val_mse: 552016.3125\n",
            "Epoch 101/500\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 1519997.2500 - mse: 1519997.2500 - val_loss: 543226.9375 - val_mse: 543226.9375\n",
            "Epoch 102/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1487971.8750 - mse: 1487971.8750 - val_loss: 504948.4062 - val_mse: 504948.4062\n",
            "Epoch 103/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1511044.5000 - mse: 1511044.5000 - val_loss: 517583.7812 - val_mse: 517583.7812\n",
            "Epoch 104/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 1503621.6250 - mse: 1503621.6250 - val_loss: 517554.5938 - val_mse: 517554.5938\n",
            "Epoch 105/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1496572.8750 - mse: 1496572.8750 - val_loss: 512986.2500 - val_mse: 512986.2500\n",
            "Epoch 106/500\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 1477199.3750 - mse: 1477199.3750 - val_loss: 521454.6562 - val_mse: 521454.6562\n",
            "Epoch 107/500\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 1504871.0000 - mse: 1504871.0000 - val_loss: 512098.8438 - val_mse: 512098.8438\n",
            "Epoch 108/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1481985.3750 - mse: 1481985.3750 - val_loss: 499993.9062 - val_mse: 499993.9062\n",
            "Epoch 109/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1484818.7500 - mse: 1484818.7500 - val_loss: 508512.5938 - val_mse: 508512.5938\n",
            "Epoch 110/500\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 1505331.3750 - mse: 1505331.3750 - val_loss: 491619.3438 - val_mse: 491619.3438\n",
            "Epoch 111/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1489390.7500 - mse: 1489390.7500 - val_loss: 516947.8438 - val_mse: 516947.8438\n",
            "Epoch 112/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1487005.5000 - mse: 1487005.5000 - val_loss: 529615.2500 - val_mse: 529615.2500\n",
            "Epoch 113/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1502418.6250 - mse: 1502418.6250 - val_loss: 525424.8125 - val_mse: 525424.8125\n",
            "Epoch 114/500\n",
            "37/37 [==============================] - 4s 78ms/step - loss: 1503317.7500 - mse: 1503317.7500 - val_loss: 512227.6562 - val_mse: 512227.6562\n",
            "Epoch 115/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1493833.8750 - mse: 1493833.8750 - val_loss: 499873.5000 - val_mse: 499873.5000\n",
            "Epoch 116/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1495257.3750 - mse: 1495257.3750 - val_loss: 503853.8438 - val_mse: 503853.8438\n",
            "Epoch 117/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1489792.1250 - mse: 1489792.1250 - val_loss: 516467.0000 - val_mse: 516467.0000\n",
            "Epoch 118/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1498479.1250 - mse: 1498479.1250 - val_loss: 533234.5625 - val_mse: 533234.5625\n",
            "Epoch 119/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1484321.0000 - mse: 1484321.0000 - val_loss: 529095.4375 - val_mse: 529095.4375\n",
            "Epoch 120/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1513805.6250 - mse: 1513805.6250 - val_loss: 516276.8750 - val_mse: 516276.8750\n",
            "Epoch 121/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 1494163.2500 - mse: 1494163.2500 - val_loss: 528883.1250 - val_mse: 528883.1250\n",
            "Epoch 122/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 1501193.5000 - mse: 1501193.5000 - val_loss: 524695.5000 - val_mse: 524695.5000\n",
            "Epoch 123/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1483598.6250 - mse: 1483598.6250 - val_loss: 541453.0000 - val_mse: 541453.0000\n",
            "Epoch 124/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1507464.6250 - mse: 1507464.6250 - val_loss: 520063.0312 - val_mse: 520063.0312\n",
            "Epoch 125/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1481781.7500 - mse: 1481781.7500 - val_loss: 528608.6875 - val_mse: 528608.6875\n",
            "Epoch 126/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1498002.6250 - mse: 1498002.6250 - val_loss: 515434.8438 - val_mse: 515434.8438\n",
            "Epoch 127/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1506425.1250 - mse: 1506425.1250 - val_loss: 519872.9688 - val_mse: 519872.9688\n",
            "Epoch 128/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1474123.7500 - mse: 1474123.7500 - val_loss: 541040.4375 - val_mse: 541040.4375\n",
            "Epoch 129/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 1504510.6250 - mse: 1504510.6250 - val_loss: 528233.4375 - val_mse: 528233.4375\n",
            "Epoch 130/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1504499.5000 - mse: 1504499.5000 - val_loss: 532256.9375 - val_mse: 532256.9375\n",
            "Epoch 131/500\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1501263.7500 - mse: 1501263.7500 - val_loss: 524065.9062 - val_mse: 524065.9062\n",
            "Epoch 132/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1496033.2500 - mse: 1496033.2500 - val_loss: 523528.0000 - val_mse: 523528.0000\n",
            "Epoch 133/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1497339.8750 - mse: 1497339.8750 - val_loss: 531333.8125 - val_mse: 531333.8125\n",
            "Epoch 134/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1494796.2500 - mse: 1494796.2500 - val_loss: 515212.4062 - val_mse: 515212.4062\n",
            "Epoch 135/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1484730.7500 - mse: 1484730.7500 - val_loss: 493808.0938 - val_mse: 493808.0938\n",
            "Epoch 136/500\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1511896.3750 - mse: 1511896.3750 - val_loss: 540377.4375 - val_mse: 540377.4375\n",
            "Epoch 137/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1470312.3750 - mse: 1470312.3750 - val_loss: 540247.4375 - val_mse: 540247.4375\n",
            "Epoch 138/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1484227.6250 - mse: 1484227.6250 - val_loss: 523047.4688 - val_mse: 523047.4688\n",
            "Epoch 139/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1478168.5000 - mse: 1478168.5000 - val_loss: 502107.8438 - val_mse: 502107.8438\n",
            "Epoch 140/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1492062.0000 - mse: 1492062.0000 - val_loss: 527343.8125 - val_mse: 527343.8125\n",
            "Epoch 141/500\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1484031.5000 - mse: 1484031.5000 - val_loss: 497802.2500 - val_mse: 497802.2500\n",
            "Epoch 142/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1506140.1250 - mse: 1506140.1250 - val_loss: 518676.4688 - val_mse: 518676.4688\n",
            "Epoch 143/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 1491647.1250 - mse: 1491647.1250 - val_loss: 526372.0625 - val_mse: 526372.0625\n",
            "Epoch 144/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 1471848.7500 - mse: 1471848.7500 - val_loss: 548269.3125 - val_mse: 548269.3125\n",
            "Epoch 145/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1479649.3750 - mse: 1479649.3750 - val_loss: 526260.5625 - val_mse: 526260.5625\n",
            "Epoch 146/500\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 1479439.3750 - mse: 1479439.3750 - val_loss: 518262.0000 - val_mse: 518262.0000\n",
            "Epoch 147/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1495394.7500 - mse: 1495394.7500 - val_loss: 539522.1250 - val_mse: 539522.1250\n",
            "Epoch 148/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1469196.3750 - mse: 1469196.3750 - val_loss: 501010.4688 - val_mse: 501010.4688\n",
            "Epoch 149/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1492060.8750 - mse: 1492060.8750 - val_loss: 500255.6562 - val_mse: 500255.6562\n",
            "Epoch 150/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1501797.7500 - mse: 1501797.7500 - val_loss: 517362.8438 - val_mse: 517362.8438\n",
            "Epoch 151/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1491230.7500 - mse: 1491230.7500 - val_loss: 513418.3438 - val_mse: 513418.3438\n",
            "Epoch 152/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1487680.6250 - mse: 1487680.6250 - val_loss: 539062.0000 - val_mse: 539062.0000\n",
            "Epoch 153/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1496848.1250 - mse: 1496848.1250 - val_loss: 534833.8750 - val_mse: 534833.8750\n",
            "Epoch 154/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1483839.5000 - mse: 1483839.5000 - val_loss: 530309.6875 - val_mse: 530309.6875\n",
            "Epoch 155/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1492969.0000 - mse: 1492969.0000 - val_loss: 521987.8438 - val_mse: 521987.8438\n",
            "Epoch 156/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1456627.3750 - mse: 1456627.3750 - val_loss: 534635.5625 - val_mse: 534635.5625\n",
            "Epoch 157/500\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1487008.5000 - mse: 1487008.5000 - val_loss: 492124.5000 - val_mse: 492124.5000\n",
            "Epoch 158/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1491890.1250 - mse: 1491890.1250 - val_loss: 513259.7500 - val_mse: 513259.7500\n",
            "Epoch 159/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1497044.7500 - mse: 1497044.7500 - val_loss: 496064.3750 - val_mse: 496064.3750\n",
            "Epoch 160/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1497930.0000 - mse: 1497930.0000 - val_loss: 525725.0625 - val_mse: 525725.0625\n",
            "Epoch 161/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1497295.6250 - mse: 1497295.6250 - val_loss: 513068.0000 - val_mse: 513068.0000\n",
            "Epoch 162/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1479832.8750 - mse: 1479832.8750 - val_loss: 534189.6875 - val_mse: 534189.6875\n",
            "Epoch 163/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1476368.7500 - mse: 1476368.7500 - val_loss: 512910.2812 - val_mse: 512910.2812\n",
            "Epoch 164/500\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 1485433.8750 - mse: 1485433.8750 - val_loss: 525499.9375 - val_mse: 525499.9375\n",
            "Epoch 165/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1472559.2500 - mse: 1472559.2500 - val_loss: 520891.8750 - val_mse: 520891.8750\n",
            "Epoch 166/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1490059.2500 - mse: 1490059.2500 - val_loss: 516717.2500 - val_mse: 516717.2500\n",
            "Epoch 167/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1482861.0000 - mse: 1482861.0000 - val_loss: 525162.0000 - val_mse: 525162.0000\n",
            "Epoch 168/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 1497962.5000 - mse: 1497962.5000 - val_loss: 525127.9375 - val_mse: 525127.9375\n",
            "Epoch 169/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1492519.7500 - mse: 1492519.7500 - val_loss: 520906.5938 - val_mse: 520906.5938\n",
            "Epoch 170/500\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 1492599.3750 - mse: 1492599.3750 - val_loss: 533485.0000 - val_mse: 533485.0000\n",
            "Epoch 171/500\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 1482512.0000 - mse: 1482512.0000 - val_loss: 516318.3438 - val_mse: 516318.3438\n",
            "Epoch 172/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1478388.5000 - mse: 1478388.5000 - val_loss: 516287.3438 - val_mse: 516287.3438\n",
            "Epoch 173/500\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 1493616.1250 - mse: 1493616.1250 - val_loss: 520297.7500 - val_mse: 520297.7500\n",
            "Epoch 174/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1488450.0000 - mse: 1488450.0000 - val_loss: 524642.6875 - val_mse: 524642.6875\n",
            "Epoch 175/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1482475.2500 - mse: 1482475.2500 - val_loss: 532979.7500 - val_mse: 532979.7500\n",
            "Epoch 176/500\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 1463809.3750 - mse: 1463809.3750 - val_loss: 520297.4062 - val_mse: 520297.4062\n",
            "Epoch 177/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1482926.6250 - mse: 1482926.6250 - val_loss: 524354.7500 - val_mse: 524354.7500\n",
            "Epoch 178/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1499718.6250 - mse: 1499718.6250 - val_loss: 536919.3125 - val_mse: 536919.3125\n",
            "Epoch 179/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1497092.1250 - mse: 1497092.1250 - val_loss: 515683.4062 - val_mse: 515683.4062\n",
            "Epoch 180/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1488690.2500 - mse: 1488690.2500 - val_loss: 524112.1562 - val_mse: 524112.1562\n",
            "Epoch 181/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1481407.0000 - mse: 1481407.0000 - val_loss: 519659.7500 - val_mse: 519659.7500\n",
            "Epoch 182/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1487622.6250 - mse: 1487622.6250 - val_loss: 519955.4688 - val_mse: 519955.4688\n",
            "Epoch 183/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1481105.8750 - mse: 1481105.8750 - val_loss: 528004.6875 - val_mse: 528004.6875\n",
            "Epoch 184/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1471481.0000 - mse: 1471481.0000 - val_loss: 544927.5625 - val_mse: 544927.5625\n",
            "Epoch 185/500\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 1488059.2500 - mse: 1488059.2500 - val_loss: 511073.5938 - val_mse: 511073.5938\n",
            "Epoch 186/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1495652.1250 - mse: 1495652.1250 - val_loss: 515127.7500 - val_mse: 515127.7500\n",
            "Epoch 187/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1485886.8750 - mse: 1485886.8750 - val_loss: 523594.2500 - val_mse: 523594.2500\n",
            "Epoch 188/500\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 1502375.3750 - mse: 1502375.3750 - val_loss: 523466.3438 - val_mse: 523466.3438\n",
            "Epoch 189/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1477885.1250 - mse: 1477885.1250 - val_loss: 510757.5938 - val_mse: 510757.5938\n",
            "Epoch 190/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1487396.2500 - mse: 1487396.2500 - val_loss: 518944.4062 - val_mse: 518944.4062\n",
            "Epoch 191/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1474520.7500 - mse: 1474520.7500 - val_loss: 523179.4688 - val_mse: 523179.4688\n",
            "Epoch 192/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1493876.2500 - mse: 1493876.2500 - val_loss: 523193.0312 - val_mse: 523193.0312\n",
            "Epoch 193/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1476568.7500 - mse: 1476568.7500 - val_loss: 518980.4062 - val_mse: 518980.4062\n",
            "Epoch 194/500\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 1485193.8750 - mse: 1485193.8750 - val_loss: 544096.1875 - val_mse: 544096.1875\n",
            "Epoch 195/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1486338.8750 - mse: 1486338.8750 - val_loss: 505253.5312 - val_mse: 505253.5312\n",
            "Epoch 196/500\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 1500524.3750 - mse: 1500524.3750 - val_loss: 510299.3750 - val_mse: 510299.3750\n",
            "Epoch 197/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1489019.6250 - mse: 1489019.6250 - val_loss: 535361.1875 - val_mse: 535361.1875\n",
            "Epoch 198/500\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 1476191.6250 - mse: 1476191.6250 - val_loss: 514179.1250 - val_mse: 514179.1250\n",
            "Epoch 199/500\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 1474556.3750 - mse: 1474556.3750 - val_loss: 509969.9688 - val_mse: 509969.9688\n",
            "Epoch 200/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1479547.5000 - mse: 1479547.5000 - val_loss: 509936.8438 - val_mse: 509936.8438\n",
            "Epoch 201/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1450085.0000 - mse: 1450085.0000 - val_loss: 513989.7188 - val_mse: 513989.7188\n",
            "Epoch 202/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1495337.2500 - mse: 1495337.2500 - val_loss: 522388.5312 - val_mse: 522388.5312\n",
            "Epoch 203/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1490369.7500 - mse: 1490369.7500 - val_loss: 522262.3438 - val_mse: 522262.3438\n",
            "Epoch 204/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1485237.7500 - mse: 1485237.7500 - val_loss: 526263.1875 - val_mse: 526263.1875\n",
            "Epoch 205/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1485919.8750 - mse: 1485919.8750 - val_loss: 517973.8750 - val_mse: 517973.8750\n",
            "Epoch 206/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1480749.7500 - mse: 1480749.7500 - val_loss: 513547.1562 - val_mse: 513547.1562\n",
            "Epoch 207/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1483843.6250 - mse: 1483843.6250 - val_loss: 521987.5312 - val_mse: 521987.5312\n",
            "Epoch 208/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1459975.8750 - mse: 1459975.8750 - val_loss: 530332.0625 - val_mse: 530332.0625\n",
            "Epoch 209/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1484836.2500 - mse: 1484836.2500 - val_loss: 525861.3125 - val_mse: 525861.3125\n",
            "Epoch 210/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1483460.3750 - mse: 1483460.3750 - val_loss: 521746.2500 - val_mse: 521746.2500\n",
            "Epoch 211/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1469287.8750 - mse: 1469287.8750 - val_loss: 509070.8750 - val_mse: 509070.8750\n",
            "Epoch 212/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1501247.2500 - mse: 1501247.2500 - val_loss: 496445.5000 - val_mse: 496445.5000\n",
            "Epoch 213/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1466388.6250 - mse: 1466388.6250 - val_loss: 512993.2188 - val_mse: 512993.2188\n",
            "Epoch 214/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1481379.6250 - mse: 1481379.6250 - val_loss: 533971.6250 - val_mse: 533971.6250\n",
            "Epoch 215/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1478357.1250 - mse: 1478357.1250 - val_loss: 500246.0312 - val_mse: 500246.0312\n",
            "Epoch 216/500\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 1491691.2500 - mse: 1491691.2500 - val_loss: 500214.0312 - val_mse: 500214.0312\n",
            "Epoch 217/500\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 1474337.1250 - mse: 1474337.1250 - val_loss: 512003.8438 - val_mse: 512003.8438\n",
            "Epoch 218/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1454816.6250 - mse: 1454816.6250 - val_loss: 521059.5000 - val_mse: 521059.5000\n",
            "Epoch 219/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1470378.5000 - mse: 1470378.5000 - val_loss: 495860.3438 - val_mse: 495860.3438\n",
            "Epoch 220/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1465462.8750 - mse: 1465462.8750 - val_loss: 511814.1562 - val_mse: 511814.1562\n",
            "Epoch 221/500\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 1471956.5000 - mse: 1471956.5000 - val_loss: 524941.6875 - val_mse: 524941.6875\n",
            "Epoch 222/500\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 1488950.6250 - mse: 1488950.6250 - val_loss: 529239.6875 - val_mse: 529239.6875\n",
            "Epoch 223/500\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 1480065.3750 - mse: 1480065.3750 - val_loss: 524734.4375 - val_mse: 524734.4375\n",
            "Epoch 224/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1491630.6250 - mse: 1491630.6250 - val_loss: 495473.2500 - val_mse: 495473.2500\n",
            "Epoch 225/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1477437.8750 - mse: 1477437.8750 - val_loss: 516374.8438 - val_mse: 516374.8438\n",
            "Epoch 226/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1461602.6250 - mse: 1461602.6250 - val_loss: 528822.3125 - val_mse: 528822.3125\n",
            "Epoch 227/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1481969.0000 - mse: 1481969.0000 - val_loss: 503738.6250 - val_mse: 503738.6250\n",
            "Epoch 228/500\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 1490941.3750 - mse: 1490941.3750 - val_loss: 516183.4062 - val_mse: 516183.4062\n",
            "Epoch 229/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1494033.3750 - mse: 1494033.3750 - val_loss: 541236.6875 - val_mse: 541236.6875\n",
            "Epoch 230/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1487720.5000 - mse: 1487720.5000 - val_loss: 499087.3438 - val_mse: 499087.3438\n",
            "Epoch 231/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1487949.3750 - mse: 1487949.3750 - val_loss: 524045.2812 - val_mse: 524045.2812\n",
            "Epoch 232/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1480596.2500 - mse: 1480596.2500 - val_loss: 511449.3438 - val_mse: 511449.3438\n",
            "Epoch 233/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1462993.8750 - mse: 1462993.8750 - val_loss: 511372.3750 - val_mse: 511372.3750\n",
            "Epoch 234/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1477596.0000 - mse: 1477596.0000 - val_loss: 511294.2188 - val_mse: 511294.2188\n",
            "Epoch 235/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1462971.6250 - mse: 1462971.6250 - val_loss: 532302.9375 - val_mse: 532302.9375\n",
            "Epoch 236/500\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 1480373.1250 - mse: 1480373.1250 - val_loss: 511181.1562 - val_mse: 511181.1562\n",
            "Epoch 237/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1477654.7500 - mse: 1477654.7500 - val_loss: 502620.5000 - val_mse: 502620.5000\n",
            "Epoch 238/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1473292.5000 - mse: 1473292.5000 - val_loss: 527987.0625 - val_mse: 527987.0625\n",
            "Epoch 239/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1481343.5000 - mse: 1481343.5000 - val_loss: 523496.2500 - val_mse: 523496.2500\n",
            "Epoch 240/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1483497.1250 - mse: 1483497.1250 - val_loss: 506122.2500 - val_mse: 506122.2500\n",
            "Epoch 241/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1489262.3750 - mse: 1489262.3750 - val_loss: 506671.5000 - val_mse: 506671.5000\n",
            "Epoch 242/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1477014.3750 - mse: 1477014.3750 - val_loss: 502567.4062 - val_mse: 502567.4062\n",
            "Epoch 243/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1482884.6250 - mse: 1482884.6250 - val_loss: 498083.5938 - val_mse: 498083.5938\n",
            "Epoch 244/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1487836.5000 - mse: 1487836.5000 - val_loss: 518933.3750 - val_mse: 518933.3750\n",
            "Epoch 245/500\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 1480517.3750 - mse: 1480517.3750 - val_loss: 502376.5938 - val_mse: 502376.5938\n",
            "Epoch 246/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1474460.8750 - mse: 1474460.8750 - val_loss: 527199.6875 - val_mse: 527199.6875\n",
            "Epoch 247/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1492626.2500 - mse: 1492626.2500 - val_loss: 531321.6250 - val_mse: 531321.6250\n",
            "Epoch 248/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1462831.0000 - mse: 1462831.0000 - val_loss: 518747.7188 - val_mse: 518747.7188\n",
            "Epoch 249/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1456608.5000 - mse: 1456608.5000 - val_loss: 502063.0938 - val_mse: 502063.0938\n",
            "Epoch 250/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1473300.6250 - mse: 1473300.6250 - val_loss: 527054.1875 - val_mse: 527054.1875\n",
            "Epoch 251/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1468728.2500 - mse: 1468728.2500 - val_loss: 526884.1875 - val_mse: 526884.1875\n",
            "Epoch 252/500\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 1484760.6250 - mse: 1484760.6250 - val_loss: 501457.9062 - val_mse: 501457.9062\n",
            "Epoch 253/500\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 1488531.7500 - mse: 1488531.7500 - val_loss: 505147.1562 - val_mse: 505147.1562\n",
            "Epoch 254/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1487725.8750 - mse: 1487725.8750 - val_loss: 505695.3750 - val_mse: 505695.3750\n",
            "Epoch 255/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1446159.2500 - mse: 1446159.2500 - val_loss: 514122.7188 - val_mse: 514122.7188\n",
            "Epoch 256/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1489643.6250 - mse: 1489643.6250 - val_loss: 497126.3750 - val_mse: 497126.3750\n",
            "Epoch 257/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1475813.1250 - mse: 1475813.1250 - val_loss: 484527.0938 - val_mse: 484527.0938\n",
            "Epoch 258/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1484252.6250 - mse: 1484252.6250 - val_loss: 526270.7500 - val_mse: 526270.7500\n",
            "Epoch 259/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1475313.2500 - mse: 1475313.2500 - val_loss: 500915.5000 - val_mse: 500915.5000\n",
            "Epoch 260/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1473926.6250 - mse: 1473926.6250 - val_loss: 513635.0312 - val_mse: 513635.0312\n",
            "Epoch 261/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1476494.0000 - mse: 1476494.0000 - val_loss: 496739.6562 - val_mse: 496739.6562\n",
            "Epoch 262/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1478945.2500 - mse: 1478945.2500 - val_loss: 512481.5938 - val_mse: 512481.5938\n",
            "Epoch 263/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1466790.0000 - mse: 1466790.0000 - val_loss: 525951.3125 - val_mse: 525951.3125\n",
            "Epoch 264/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1475267.8750 - mse: 1475267.8750 - val_loss: 488379.5938 - val_mse: 488379.5938\n",
            "Epoch 265/500\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 1475347.1250 - mse: 1475347.1250 - val_loss: 504834.3438 - val_mse: 504834.3438\n",
            "Epoch 266/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1468312.2500 - mse: 1468312.2500 - val_loss: 513157.5312 - val_mse: 513157.5312\n",
            "Epoch 267/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1481572.5000 - mse: 1481572.5000 - val_loss: 500659.2812 - val_mse: 500659.2812\n",
            "Epoch 268/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1480187.8750 - mse: 1480187.8750 - val_loss: 491467.8750 - val_mse: 491467.8750\n",
            "Epoch 269/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1465624.2500 - mse: 1465624.2500 - val_loss: 508584.2188 - val_mse: 508584.2188\n",
            "Epoch 270/500\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 1440562.8750 - mse: 1440562.8750 - val_loss: 537758.1250 - val_mse: 537758.1250\n",
            "Epoch 271/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1485782.8750 - mse: 1485782.8750 - val_loss: 504367.0938 - val_mse: 504367.0938\n",
            "Epoch 272/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1473617.5000 - mse: 1473617.5000 - val_loss: 500270.3438 - val_mse: 500270.3438\n",
            "Epoch 273/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1464731.8750 - mse: 1464731.8750 - val_loss: 516708.9062 - val_mse: 516708.9062\n",
            "Epoch 274/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1460211.7500 - mse: 1460211.7500 - val_loss: 504133.0000 - val_mse: 504133.0000\n",
            "Epoch 275/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1492326.5000 - mse: 1492326.5000 - val_loss: 504011.5312 - val_mse: 504011.5312\n",
            "Epoch 276/500\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 1457833.5000 - mse: 1457833.5000 - val_loss: 495587.7812 - val_mse: 495587.7812\n",
            "Epoch 277/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1466785.0000 - mse: 1466785.0000 - val_loss: 516391.3750 - val_mse: 516391.3750\n",
            "Epoch 278/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1480101.2500 - mse: 1480101.2500 - val_loss: 503821.6250 - val_mse: 503821.6250\n",
            "Epoch 279/500\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 1471076.0000 - mse: 1471076.0000 - val_loss: 520248.1250 - val_mse: 520248.1250\n",
            "Epoch 280/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1463247.0000 - mse: 1463247.0000 - val_loss: 499563.9688 - val_mse: 499563.9688\n",
            "Epoch 281/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1484817.1250 - mse: 1484817.1250 - val_loss: 507646.0938 - val_mse: 507646.0938\n",
            "Epoch 282/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 1486189.7500 - mse: 1486189.7500 - val_loss: 520009.3438 - val_mse: 520009.3438\n",
            "Epoch 283/500\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 1473741.6250 - mse: 1473741.6250 - val_loss: 507447.5312 - val_mse: 507447.5312\n",
            "Epoch 284/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1464889.0000 - mse: 1464889.0000 - val_loss: 507456.2500 - val_mse: 507456.2500\n",
            "Epoch 285/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1446159.5000 - mse: 1446159.5000 - val_loss: 528149.0625 - val_mse: 528149.0625\n",
            "Epoch 286/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1462960.3750 - mse: 1462960.3750 - val_loss: 511620.3750 - val_mse: 511620.3750\n",
            "Epoch 287/500\n",
            "37/37 [==============================] - 6s 141ms/step - loss: 1456678.2500 - mse: 1456678.2500 - val_loss: 523973.3750 - val_mse: 523973.3750\n",
            "Epoch 288/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1477035.6250 - mse: 1477035.6250 - val_loss: 519574.7500 - val_mse: 519574.7500\n",
            "Epoch 289/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1474204.3750 - mse: 1474204.3750 - val_loss: 507023.0312 - val_mse: 507023.0312\n",
            "Epoch 290/500\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 1470479.1250 - mse: 1470479.1250 - val_loss: 536245.5625 - val_mse: 536245.5625\n",
            "Epoch 291/500\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 1458747.6250 - mse: 1458747.6250 - val_loss: 510965.6250 - val_mse: 510965.6250\n",
            "Epoch 292/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1475036.1250 - mse: 1475036.1250 - val_loss: 498419.5938 - val_mse: 498419.5938\n",
            "Epoch 293/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 1480194.0000 - mse: 1480194.0000 - val_loss: 515121.1562 - val_mse: 515121.1562\n",
            "Epoch 294/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1466096.2500 - mse: 1466096.2500 - val_loss: 494166.9062 - val_mse: 494166.9062\n",
            "Epoch 295/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1463315.2500 - mse: 1463315.2500 - val_loss: 490077.9688 - val_mse: 490077.9688\n",
            "Epoch 296/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1456478.6250 - mse: 1456478.6250 - val_loss: 514883.3438 - val_mse: 514883.3438\n",
            "Epoch 297/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1466103.1250 - mse: 1466103.1250 - val_loss: 506397.0000 - val_mse: 506397.0000\n",
            "Epoch 298/500\n",
            "37/37 [==============================] - 9s 214ms/step - loss: 1472037.0000 - mse: 1472037.0000 - val_loss: 518735.8750 - val_mse: 518735.8750\n",
            "Epoch 299/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1476479.6250 - mse: 1476479.6250 - val_loss: 501933.7500 - val_mse: 501933.7500\n",
            "Epoch 300/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1476951.7500 - mse: 1476951.7500 - val_loss: 518660.2188 - val_mse: 518660.2188\n",
            "Epoch 301/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1472150.0000 - mse: 1472150.0000 - val_loss: 510138.0938 - val_mse: 510138.0938\n",
            "Epoch 302/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 1448196.5000 - mse: 1448196.5000 - val_loss: 514406.0938 - val_mse: 514406.0938\n",
            "Epoch 303/500\n",
            "37/37 [==============================] - 6s 140ms/step - loss: 1484990.6250 - mse: 1484990.6250 - val_loss: 514328.4062 - val_mse: 514328.4062\n",
            "Epoch 304/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1465051.7500 - mse: 1465051.7500 - val_loss: 501798.5000 - val_mse: 501798.5000\n",
            "Epoch 305/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1461177.3750 - mse: 1461177.3750 - val_loss: 514128.1562 - val_mse: 514128.1562\n",
            "Epoch 306/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1477453.3750 - mse: 1477453.3750 - val_loss: 505737.5000 - val_mse: 505737.5000\n",
            "Epoch 307/500\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1460825.2500 - mse: 1460825.2500 - val_loss: 517978.7500 - val_mse: 517978.7500\n",
            "Epoch 308/500\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 1463796.1250 - mse: 1463796.1250 - val_loss: 497149.9062 - val_mse: 497149.9062\n",
            "Epoch 309/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1459588.1250 - mse: 1459588.1250 - val_loss: 534644.8125 - val_mse: 534644.8125\n",
            "Epoch 310/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1480512.5000 - mse: 1480512.5000 - val_loss: 504758.8438 - val_mse: 504758.8438\n",
            "Epoch 311/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1453088.8750 - mse: 1453088.8750 - val_loss: 492909.5938 - val_mse: 492909.5938\n",
            "Epoch 312/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1454920.3750 - mse: 1454920.3750 - val_loss: 497170.8438 - val_mse: 497170.8438\n",
            "Epoch 313/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1470137.3750 - mse: 1470137.3750 - val_loss: 513579.5938 - val_mse: 513579.5938\n",
            "Epoch 314/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1470787.3750 - mse: 1470787.3750 - val_loss: 513416.0938 - val_mse: 513416.0938\n",
            "Epoch 315/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1462830.3750 - mse: 1462830.3750 - val_loss: 525810.8125 - val_mse: 525810.8125\n",
            "Epoch 316/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1477194.8750 - mse: 1477194.8750 - val_loss: 525687.1875 - val_mse: 525687.1875\n",
            "Epoch 317/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1448302.8750 - mse: 1448302.8750 - val_loss: 513219.7812 - val_mse: 513219.7812\n",
            "Epoch 318/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1461085.6250 - mse: 1461085.6250 - val_loss: 500755.8750 - val_mse: 500755.8750\n",
            "Epoch 319/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1467292.6250 - mse: 1467292.6250 - val_loss: 500636.6562 - val_mse: 500636.6562\n",
            "Epoch 320/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1458535.5000 - mse: 1458535.5000 - val_loss: 517029.9062 - val_mse: 517029.9062\n",
            "Epoch 321/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1475891.8750 - mse: 1475891.8750 - val_loss: 512861.0000 - val_mse: 512861.0000\n",
            "Epoch 322/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1477759.5000 - mse: 1477759.5000 - val_loss: 512824.2812 - val_mse: 512824.2812\n",
            "Epoch 323/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1462081.1250 - mse: 1462081.1250 - val_loss: 496237.4688 - val_mse: 496237.4688\n",
            "Epoch 324/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1433002.2500 - mse: 1433002.2500 - val_loss: 516713.5938 - val_mse: 516713.5938\n",
            "Epoch 325/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 1461664.5000 - mse: 1461664.5000 - val_loss: 524880.0625 - val_mse: 524880.0625\n",
            "Epoch 326/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1447619.8750 - mse: 1447619.8750 - val_loss: 500094.4688 - val_mse: 500094.4688\n",
            "Epoch 327/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1471322.1250 - mse: 1471322.1250 - val_loss: 504062.2500 - val_mse: 504062.2500\n",
            "Epoch 328/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1463221.5000 - mse: 1463221.5000 - val_loss: 487570.2188 - val_mse: 487570.2188\n",
            "Epoch 329/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1468415.5000 - mse: 1468415.5000 - val_loss: 512189.4688 - val_mse: 512189.4688\n",
            "Epoch 330/500\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 1479776.1250 - mse: 1479776.1250 - val_loss: 503119.0312 - val_mse: 503119.0312\n",
            "Epoch 331/500\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 1458097.8750 - mse: 1458097.8750 - val_loss: 512156.1250 - val_mse: 512156.1250\n",
            "Epoch 332/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1461822.7500 - mse: 1461822.7500 - val_loss: 503633.0938 - val_mse: 503633.0938\n",
            "Epoch 333/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1466285.1250 - mse: 1466285.1250 - val_loss: 516000.6250 - val_mse: 516000.6250\n",
            "Epoch 334/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1433980.3750 - mse: 1433980.3750 - val_loss: 478755.2500 - val_mse: 478755.2500\n",
            "Epoch 335/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 1457099.1250 - mse: 1457099.1250 - val_loss: 511717.6562 - val_mse: 511717.6562\n",
            "Epoch 336/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1455461.0000 - mse: 1455461.0000 - val_loss: 507407.2812 - val_mse: 507407.2812\n",
            "Epoch 337/500\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 1480389.7500 - mse: 1480389.7500 - val_loss: 503286.9062 - val_mse: 503286.9062\n",
            "Epoch 338/500\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 1476671.8750 - mse: 1476671.8750 - val_loss: 486769.7500 - val_mse: 486769.7500\n",
            "Epoch 339/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1460290.7500 - mse: 1460290.7500 - val_loss: 515526.3438 - val_mse: 515526.3438\n",
            "Epoch 340/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1447568.3750 - mse: 1447568.3750 - val_loss: 494701.7188 - val_mse: 494701.7188\n",
            "Epoch 341/500\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 1456888.5000 - mse: 1456888.5000 - val_loss: 515286.3750 - val_mse: 515286.3750\n",
            "Epoch 342/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1465692.7500 - mse: 1465692.7500 - val_loss: 490508.5312 - val_mse: 490508.5312\n",
            "Epoch 343/500\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1466245.8750 - mse: 1466245.8750 - val_loss: 502822.5000 - val_mse: 502822.5000\n",
            "Epoch 344/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1460078.2500 - mse: 1460078.2500 - val_loss: 498662.5312 - val_mse: 498662.5312\n",
            "Epoch 345/500\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 1469946.5000 - mse: 1469946.5000 - val_loss: 494587.3438 - val_mse: 494587.3438\n",
            "Epoch 346/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1467488.5000 - mse: 1467488.5000 - val_loss: 502549.3438 - val_mse: 502549.3438\n",
            "Epoch 347/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1468391.3750 - mse: 1468391.3750 - val_loss: 510854.6562 - val_mse: 510854.6562\n",
            "Epoch 348/500\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 1449109.1250 - mse: 1449109.1250 - val_loss: 510817.1250 - val_mse: 510817.1250\n",
            "Epoch 349/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1448354.5000 - mse: 1448354.5000 - val_loss: 494057.1250 - val_mse: 494057.1250\n",
            "Epoch 350/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1447535.8750 - mse: 1447535.8750 - val_loss: 502239.9062 - val_mse: 502239.9062\n",
            "Epoch 351/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1469189.3750 - mse: 1469189.3750 - val_loss: 522875.6562 - val_mse: 522875.6562\n",
            "Epoch 352/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1476869.8750 - mse: 1476869.8750 - val_loss: 498127.5000 - val_mse: 498127.5000\n",
            "Epoch 353/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1466287.5000 - mse: 1466287.5000 - val_loss: 509756.0938 - val_mse: 509756.0938\n",
            "Epoch 354/500\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 1454109.5000 - mse: 1454109.5000 - val_loss: 497930.7812 - val_mse: 497930.7812\n",
            "Epoch 355/500\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 1451600.2500 - mse: 1451600.2500 - val_loss: 497854.2500 - val_mse: 497854.2500\n",
            "Epoch 356/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1467465.0000 - mse: 1467465.0000 - val_loss: 506107.3438 - val_mse: 506107.3438\n",
            "Epoch 357/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1454493.3750 - mse: 1454493.3750 - val_loss: 489409.7500 - val_mse: 489409.7500\n",
            "Epoch 358/500\n",
            "37/37 [==============================] - 9s 214ms/step - loss: 1447610.6250 - mse: 1447610.6250 - val_loss: 485257.2812 - val_mse: 485257.2812\n",
            "Epoch 359/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1466053.6250 - mse: 1466053.6250 - val_loss: 513985.5938 - val_mse: 513985.5938\n",
            "Epoch 360/500\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 1454857.8750 - mse: 1454857.8750 - val_loss: 489181.5000 - val_mse: 489181.5000\n",
            "Epoch 361/500\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 1464831.1250 - mse: 1464831.1250 - val_loss: 530475.5000 - val_mse: 530475.5000\n",
            "Epoch 362/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1460920.2500 - mse: 1460920.2500 - val_loss: 497311.7500 - val_mse: 497311.7500\n",
            "Epoch 363/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 1452008.8750 - mse: 1452008.8750 - val_loss: 488913.2188 - val_mse: 488913.2188\n",
            "Epoch 364/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1459285.8750 - mse: 1459285.8750 - val_loss: 517794.9688 - val_mse: 517794.9688\n",
            "Epoch 365/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1440487.5000 - mse: 1440487.5000 - val_loss: 509354.7188 - val_mse: 509354.7188\n",
            "Epoch 366/500\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 1473654.6250 - mse: 1473654.6250 - val_loss: 505281.8438 - val_mse: 505281.8438\n",
            "Epoch 367/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1446726.5000 - mse: 1446726.5000 - val_loss: 509237.6562 - val_mse: 509237.6562\n",
            "Epoch 368/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1471255.5000 - mse: 1471255.5000 - val_loss: 488573.4062 - val_mse: 488573.4062\n",
            "Epoch 369/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1462158.1250 - mse: 1462158.1250 - val_loss: 529742.2500 - val_mse: 529742.2500\n",
            "Epoch 370/500\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 1457902.7500 - mse: 1457902.7500 - val_loss: 500768.7500 - val_mse: 500768.7500\n",
            "Epoch 371/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1443964.1250 - mse: 1443964.1250 - val_loss: 500651.3750 - val_mse: 500651.3750\n",
            "Epoch 372/500\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 1466554.2500 - mse: 1466554.2500 - val_loss: 504853.4062 - val_mse: 504853.4062\n",
            "Epoch 373/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1437695.6250 - mse: 1437695.6250 - val_loss: 492432.2500 - val_mse: 492432.2500\n",
            "Epoch 374/500\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 1452529.8750 - mse: 1452529.8750 - val_loss: 500419.8750 - val_mse: 500419.8750\n",
            "Epoch 375/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 1469603.5000 - mse: 1469603.5000 - val_loss: 500302.8438 - val_mse: 500302.8438\n",
            "Epoch 376/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1459344.3750 - mse: 1459344.3750 - val_loss: 529094.1250 - val_mse: 529094.1250\n",
            "Epoch 377/500\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 1473912.2500 - mse: 1473912.2500 - val_loss: 483821.2500 - val_mse: 483821.2500\n",
            "Epoch 378/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1457217.5000 - mse: 1457217.5000 - val_loss: 496079.0938 - val_mse: 496079.0938\n",
            "Epoch 379/500\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 1430746.1250 - mse: 1430746.1250 - val_loss: 496002.5938 - val_mse: 496002.5938\n",
            "Epoch 380/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1450246.8750 - mse: 1450246.8750 - val_loss: 516517.3438 - val_mse: 516517.3438\n",
            "Epoch 381/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1467656.1250 - mse: 1467656.1250 - val_loss: 491818.8438 - val_mse: 491818.8438\n",
            "Epoch 382/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1437764.8750 - mse: 1437764.8750 - val_loss: 491742.5312 - val_mse: 491742.5312\n",
            "Epoch 383/500\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 1469515.0000 - mse: 1469515.0000 - val_loss: 516279.4688 - val_mse: 516279.4688\n",
            "Epoch 384/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1470022.8750 - mse: 1470022.8750 - val_loss: 516159.6562 - val_mse: 516159.6562\n",
            "Epoch 385/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1453711.1250 - mse: 1453711.1250 - val_loss: 499530.1562 - val_mse: 499530.1562\n",
            "Epoch 386/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1454610.8750 - mse: 1454610.8750 - val_loss: 503679.6250 - val_mse: 503679.6250\n",
            "Epoch 387/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1456777.7500 - mse: 1456777.7500 - val_loss: 503444.2812 - val_mse: 503444.2812\n",
            "Epoch 388/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1463070.8750 - mse: 1463070.8750 - val_loss: 507629.1562 - val_mse: 507629.1562\n",
            "Epoch 389/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1457871.2500 - mse: 1457871.2500 - val_loss: 503523.7500 - val_mse: 503523.7500\n",
            "Epoch 390/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1454857.1250 - mse: 1454857.1250 - val_loss: 503171.9062 - val_mse: 503171.9062\n",
            "Epoch 391/500\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 1457926.5000 - mse: 1457926.5000 - val_loss: 482767.0000 - val_mse: 482767.0000\n",
            "Epoch 392/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1450543.8750 - mse: 1450543.8750 - val_loss: 515638.6562 - val_mse: 515638.6562\n",
            "Epoch 393/500\n",
            "37/37 [==============================] - 10s 242ms/step - loss: 1450133.5000 - mse: 1450133.5000 - val_loss: 494965.5000 - val_mse: 494965.5000\n",
            "Epoch 394/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1459206.7500 - mse: 1459206.7500 - val_loss: 511183.0312 - val_mse: 511183.0312\n",
            "Epoch 395/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1462044.2500 - mse: 1462044.2500 - val_loss: 507041.1562 - val_mse: 507041.1562\n",
            "Epoch 396/500\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 1455155.6250 - mse: 1455155.6250 - val_loss: 498682.2188 - val_mse: 498682.2188\n",
            "Epoch 397/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1439035.2500 - mse: 1439035.2500 - val_loss: 498644.5938 - val_mse: 498644.5938\n",
            "Epoch 398/500\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 1454451.1250 - mse: 1454451.1250 - val_loss: 515007.8438 - val_mse: 515007.8438\n",
            "Epoch 399/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1467413.8750 - mse: 1467413.8750 - val_loss: 498488.9062 - val_mse: 498488.9062\n",
            "Epoch 400/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1439145.5000 - mse: 1439145.5000 - val_loss: 510712.5938 - val_mse: 510712.5938\n",
            "Epoch 401/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1435253.1250 - mse: 1435253.1250 - val_loss: 490063.0000 - val_mse: 490063.0000\n",
            "Epoch 402/500\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 1473303.5000 - mse: 1473303.5000 - val_loss: 514727.5938 - val_mse: 514727.5938\n",
            "Epoch 403/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1450680.7500 - mse: 1450680.7500 - val_loss: 514649.2188 - val_mse: 514649.2188\n",
            "Epoch 404/500\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 1448835.6250 - mse: 1448835.6250 - val_loss: 514531.3750 - val_mse: 514531.3750\n",
            "Epoch 405/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1453909.5000 - mse: 1453909.5000 - val_loss: 502274.5000 - val_mse: 502274.5000\n",
            "Epoch 406/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1465139.8750 - mse: 1465139.8750 - val_loss: 506141.8438 - val_mse: 506141.8438\n",
            "Epoch 407/500\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 1456699.0000 - mse: 1456699.0000 - val_loss: 489646.7500 - val_mse: 489646.7500\n",
            "Epoch 408/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1472639.2500 - mse: 1472639.2500 - val_loss: 502040.4688 - val_mse: 502040.4688\n",
            "Epoch 409/500\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 1463582.8750 - mse: 1463582.8750 - val_loss: 489454.5938 - val_mse: 489454.5938\n",
            "Epoch 410/500\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 1444786.3750 - mse: 1444786.3750 - val_loss: 518151.7500 - val_mse: 518151.7500\n",
            "Epoch 411/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1462811.0000 - mse: 1462811.0000 - val_loss: 497566.5000 - val_mse: 497566.5000\n",
            "Epoch 412/500\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 1456261.8750 - mse: 1456261.8750 - val_loss: 505749.0000 - val_mse: 505749.0000\n",
            "Epoch 413/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1460289.3750 - mse: 1460289.3750 - val_loss: 505633.4062 - val_mse: 505633.4062\n",
            "Epoch 414/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1459733.5000 - mse: 1459733.5000 - val_loss: 493276.4062 - val_mse: 493276.4062\n",
            "Epoch 415/500\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 1443509.0000 - mse: 1443509.0000 - val_loss: 505438.1250 - val_mse: 505438.1250\n",
            "Epoch 416/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1442066.7500 - mse: 1442066.7500 - val_loss: 504736.3438 - val_mse: 504736.3438\n",
            "Epoch 417/500\n",
            "37/37 [==============================] - 4s 110ms/step - loss: 1453973.5000 - mse: 1453973.5000 - val_loss: 497068.2812 - val_mse: 497068.2812\n",
            "Epoch 418/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1458611.0000 - mse: 1458611.0000 - val_loss: 475880.7188 - val_mse: 475880.7188\n",
            "Epoch 419/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1449520.8750 - mse: 1449520.8750 - val_loss: 496952.5312 - val_mse: 496952.5312\n",
            "Epoch 420/500\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 1450000.0000 - mse: 1450000.0000 - val_loss: 509143.3438 - val_mse: 509143.3438\n",
            "Epoch 421/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 1460284.3750 - mse: 1460284.3750 - val_loss: 505085.1562 - val_mse: 505085.1562\n",
            "Epoch 422/500\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1439387.5000 - mse: 1439387.5000 - val_loss: 484458.1250 - val_mse: 484458.1250\n",
            "Epoch 423/500\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 1454405.8750 - mse: 1454405.8750 - val_loss: 525289.4375 - val_mse: 525289.4375\n",
            "Epoch 424/500\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 1457500.7500 - mse: 1457500.7500 - val_loss: 492553.1250 - val_mse: 492553.1250\n",
            "Epoch 425/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1460925.5000 - mse: 1460925.5000 - val_loss: 504735.8750 - val_mse: 504735.8750\n",
            "Epoch 426/500\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 1447578.7500 - mse: 1447578.7500 - val_loss: 512862.4062 - val_mse: 512862.4062\n",
            "Epoch 427/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1453489.6250 - mse: 1453489.6250 - val_loss: 500564.6250 - val_mse: 500564.6250\n",
            "Epoch 428/500\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1457768.7500 - mse: 1457768.7500 - val_loss: 512742.8438 - val_mse: 512742.8438\n",
            "Epoch 429/500\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1450092.7500 - mse: 1450092.7500 - val_loss: 471716.0312 - val_mse: 471716.0312\n",
            "Epoch 430/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1463042.8750 - mse: 1463042.8750 - val_loss: 500295.3750 - val_mse: 500295.3750\n",
            "Epoch 431/500\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 1432164.1250 - mse: 1432164.1250 - val_loss: 504269.5312 - val_mse: 504269.5312\n",
            "Epoch 432/500\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 1432483.3750 - mse: 1432483.3750 - val_loss: 504190.7812 - val_mse: 504190.7812\n",
            "Epoch 433/500\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 1418152.8750 - mse: 1418152.8750 - val_loss: 487815.1562 - val_mse: 487815.1562\n",
            "Epoch 434/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1460038.1250 - mse: 1460038.1250 - val_loss: 504074.3438 - val_mse: 504074.3438\n",
            "Epoch 435/500\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 1463920.8750 - mse: 1463920.8750 - val_loss: 499982.6562 - val_mse: 499982.6562\n",
            "Epoch 436/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1449876.3750 - mse: 1449876.3750 - val_loss: 491471.7500 - val_mse: 491471.7500\n",
            "Epoch 437/500\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 1456699.8750 - mse: 1456699.8750 - val_loss: 495573.7812 - val_mse: 495573.7812\n",
            "Epoch 438/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1444291.0000 - mse: 1444291.0000 - val_loss: 491484.5938 - val_mse: 491484.5938\n",
            "Epoch 439/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1454157.5000 - mse: 1454157.5000 - val_loss: 515848.1562 - val_mse: 515848.1562\n",
            "Epoch 440/500\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 1440070.5000 - mse: 1440070.5000 - val_loss: 483106.3438 - val_mse: 483106.3438\n",
            "Epoch 441/500\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 1454452.7500 - mse: 1454452.7500 - val_loss: 503491.8438 - val_mse: 503491.8438\n",
            "Epoch 442/500\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 1444614.1250 - mse: 1444614.1250 - val_loss: 482993.6562 - val_mse: 482993.6562\n",
            "Epoch 443/500\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 1450223.1250 - mse: 1450223.1250 - val_loss: 499163.6562 - val_mse: 499163.6562\n",
            "Epoch 444/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1434929.5000 - mse: 1434929.5000 - val_loss: 503220.2188 - val_mse: 503220.2188\n",
            "Epoch 445/500\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 1459059.5000 - mse: 1459059.5000 - val_loss: 503178.5938 - val_mse: 503178.5938\n",
            "Epoch 446/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1457218.3750 - mse: 1457218.3750 - val_loss: 503100.4062 - val_mse: 503100.4062\n",
            "Epoch 447/500\n",
            "37/37 [==============================] - 6s 138ms/step - loss: 1451975.5000 - mse: 1451975.5000 - val_loss: 478607.0312 - val_mse: 478607.0312\n",
            "Epoch 448/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1427902.6250 - mse: 1427902.6250 - val_loss: 498777.5312 - val_mse: 498777.5312\n",
            "Epoch 449/500\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 1444207.8750 - mse: 1444207.8750 - val_loss: 493992.6562 - val_mse: 493992.6562\n",
            "Epoch 450/500\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 1452294.2500 - mse: 1452294.2500 - val_loss: 490568.7188 - val_mse: 490568.7188\n",
            "Epoch 451/500\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 1456965.0000 - mse: 1456965.0000 - val_loss: 494537.8438 - val_mse: 494537.8438\n",
            "Epoch 452/500\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 1459420.5000 - mse: 1459420.5000 - val_loss: 510770.0000 - val_mse: 510770.0000\n",
            "Epoch 453/500\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 1457253.1250 - mse: 1457253.1250 - val_loss: 490338.7188 - val_mse: 490338.7188\n",
            "Epoch 454/500\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 1449025.3750 - mse: 1449025.3750 - val_loss: 477386.7812 - val_mse: 477386.7812\n",
            "Epoch 455/500\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 1451328.5000 - mse: 1451328.5000 - val_loss: 498355.2188 - val_mse: 498355.2188\n",
            "Epoch 456/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1443035.1250 - mse: 1443035.1250 - val_loss: 494116.9062 - val_mse: 494116.9062\n",
            "Epoch 457/500\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 1460367.0000 - mse: 1460367.0000 - val_loss: 514417.2812 - val_mse: 514417.2812\n",
            "Epoch 458/500\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 1448587.5000 - mse: 1448587.5000 - val_loss: 493963.8750 - val_mse: 493963.8750\n",
            "Epoch 459/500\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 1425032.3750 - mse: 1425032.3750 - val_loss: 498045.0000 - val_mse: 498045.0000\n",
            "Epoch 460/500\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 1456197.0000 - mse: 1456197.0000 - val_loss: 506052.9062 - val_mse: 506052.9062\n",
            "Epoch 461/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1449927.7500 - mse: 1449927.7500 - val_loss: 477373.9688 - val_mse: 477373.9688\n",
            "Epoch 462/500\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 1437339.2500 - mse: 1437339.2500 - val_loss: 501818.5000 - val_mse: 501818.5000\n",
            "Epoch 463/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1430679.3750 - mse: 1430679.3750 - val_loss: 505782.3750 - val_mse: 505782.3750\n",
            "Epoch 464/500\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 1428761.2500 - mse: 1428761.2500 - val_loss: 472488.2500 - val_mse: 472488.2500\n",
            "Epoch 465/500\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 1450033.1250 - mse: 1450033.1250 - val_loss: 473075.7500 - val_mse: 473075.7500\n",
            "Epoch 466/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1433449.5000 - mse: 1433449.5000 - val_loss: 485345.7188 - val_mse: 485345.7188\n",
            "Epoch 467/500\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 1429981.7500 - mse: 1429981.7500 - val_loss: 497465.0938 - val_mse: 497465.0938\n",
            "Epoch 468/500\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 1448572.0000 - mse: 1448572.0000 - val_loss: 497386.5938 - val_mse: 497386.5938\n",
            "Epoch 469/500\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 1433217.7500 - mse: 1433217.7500 - val_loss: 485118.5312 - val_mse: 485118.5312\n",
            "Epoch 470/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1458515.0000 - mse: 1458515.0000 - val_loss: 501200.1250 - val_mse: 501200.1250\n",
            "Epoch 471/500\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 1426477.0000 - mse: 1426477.0000 - val_loss: 501122.2812 - val_mse: 501122.2812\n",
            "Epoch 472/500\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 1433396.2500 - mse: 1433396.2500 - val_loss: 497006.7500 - val_mse: 497006.7500\n",
            "Epoch 473/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1439914.7500 - mse: 1439914.7500 - val_loss: 476668.3750 - val_mse: 476668.3750\n",
            "Epoch 474/500\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 1444797.2500 - mse: 1444797.2500 - val_loss: 492745.6562 - val_mse: 492745.6562\n",
            "Epoch 475/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1448580.0000 - mse: 1448580.0000 - val_loss: 495975.3750 - val_mse: 495975.3750\n",
            "Epoch 476/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1444745.7500 - mse: 1444745.7500 - val_loss: 488555.0938 - val_mse: 488555.0938\n",
            "Epoch 477/500\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 1438385.7500 - mse: 1438385.7500 - val_loss: 520978.4062 - val_mse: 520978.4062\n",
            "Epoch 478/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1438382.5000 - mse: 1438382.5000 - val_loss: 476294.4062 - val_mse: 476294.4062\n",
            "Epoch 479/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1450908.5000 - mse: 1450908.5000 - val_loss: 496536.6250 - val_mse: 496536.6250\n",
            "Epoch 480/500\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 1429824.7500 - mse: 1429824.7500 - val_loss: 508598.3438 - val_mse: 508598.3438\n",
            "Epoch 481/500\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 1441079.7500 - mse: 1441079.7500 - val_loss: 488175.5312 - val_mse: 488175.5312\n",
            "Epoch 482/500\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 1428162.6250 - mse: 1428162.6250 - val_loss: 488170.1562 - val_mse: 488170.1562\n",
            "Epoch 483/500\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 1429422.1250 - mse: 1429422.1250 - val_loss: 475890.0000 - val_mse: 475890.0000\n",
            "Epoch 484/500\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 1447957.2500 - mse: 1447957.2500 - val_loss: 500151.6562 - val_mse: 500151.6562\n",
            "Epoch 485/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1448444.8750 - mse: 1448444.8750 - val_loss: 491906.5000 - val_mse: 491906.5000\n",
            "Epoch 486/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1419745.1250 - mse: 1419745.1250 - val_loss: 483697.6250 - val_mse: 483697.6250\n",
            "Epoch 487/500\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 1455238.6250 - mse: 1455238.6250 - val_loss: 479624.8750 - val_mse: 479624.8750\n",
            "Epoch 488/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1421649.0000 - mse: 1421649.0000 - val_loss: 479550.5312 - val_mse: 479550.5312\n",
            "Epoch 489/500\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 1450989.7500 - mse: 1450989.7500 - val_loss: 483440.2500 - val_mse: 483440.2500\n",
            "Epoch 490/500\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 1419957.0000 - mse: 1419957.0000 - val_loss: 479403.1250 - val_mse: 479403.1250\n",
            "Epoch 491/500\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1426019.3750 - mse: 1426019.3750 - val_loss: 487454.7188 - val_mse: 487454.7188\n",
            "Epoch 492/500\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 1436525.5000 - mse: 1436525.5000 - val_loss: 487380.0000 - val_mse: 487380.0000\n",
            "Epoch 493/500\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 1437573.8750 - mse: 1437573.8750 - val_loss: 499457.4062 - val_mse: 499457.4062\n",
            "Epoch 494/500\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 1459628.3750 - mse: 1459628.3750 - val_loss: 503376.1562 - val_mse: 503376.1562\n",
            "Epoch 495/500\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 1457979.8750 - mse: 1457979.8750 - val_loss: 462883.6562 - val_mse: 462883.6562\n",
            "Epoch 496/500\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 1438364.8750 - mse: 1438364.8750 - val_loss: 491104.6562 - val_mse: 491104.6562\n",
            "Epoch 497/500\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 1444454.0000 - mse: 1444454.0000 - val_loss: 519412.7812 - val_mse: 519412.7812\n",
            "Epoch 498/500\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 1444005.7500 - mse: 1444005.7500 - val_loss: 507254.0938 - val_mse: 507254.0938\n",
            "Epoch 499/500\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 1443961.1250 - mse: 1443961.1250 - val_loss: 494961.5312 - val_mse: 494961.5312\n",
            "Epoch 500/500\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 1448676.0000 - mse: 1448676.0000 - val_loss: 490765.3438 - val_mse: 490765.3438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history "
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "outputId": "1e024ced-d50b-4fa1-9ec5-9b944ec7bd44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [1516518.75,\n",
              "  1506363.125,\n",
              "  1507930.0,\n",
              "  1508656.625,\n",
              "  1505641.5,\n",
              "  1492480.375,\n",
              "  1514817.125,\n",
              "  1508224.5,\n",
              "  1496289.625,\n",
              "  1516412.875,\n",
              "  1521321.125,\n",
              "  1526450.75,\n",
              "  1521686.0,\n",
              "  1504394.875,\n",
              "  1501763.5,\n",
              "  1503003.875,\n",
              "  1491755.0,\n",
              "  1489553.125,\n",
              "  1512119.125,\n",
              "  1494952.75,\n",
              "  1504440.25,\n",
              "  1500903.25,\n",
              "  1501924.5,\n",
              "  1498644.0,\n",
              "  1499875.75,\n",
              "  1518455.25,\n",
              "  1510430.375,\n",
              "  1515438.125,\n",
              "  1498455.75,\n",
              "  1499913.375,\n",
              "  1504160.875,\n",
              "  1508111.125,\n",
              "  1506441.375,\n",
              "  1505892.625,\n",
              "  1526953.625,\n",
              "  1521998.25,\n",
              "  1529106.375,\n",
              "  1509687.625,\n",
              "  1498651.75,\n",
              "  1513464.625,\n",
              "  1501841.625,\n",
              "  1514421.25,\n",
              "  1511862.5,\n",
              "  1494262.875,\n",
              "  1514574.0,\n",
              "  1501229.75,\n",
              "  1518473.375,\n",
              "  1510203.875,\n",
              "  1489474.0,\n",
              "  1489945.125,\n",
              "  1512915.125,\n",
              "  1483195.75,\n",
              "  1499369.375,\n",
              "  1514341.375,\n",
              "  1496078.25,\n",
              "  1493722.75,\n",
              "  1503530.0,\n",
              "  1500965.25,\n",
              "  1503888.75,\n",
              "  1496408.25,\n",
              "  1495677.5,\n",
              "  1503660.125,\n",
              "  1482232.0,\n",
              "  1502316.375,\n",
              "  1520752.125,\n",
              "  1520212.0,\n",
              "  1500745.875,\n",
              "  1490273.125,\n",
              "  1502591.875,\n",
              "  1483654.125,\n",
              "  1508383.5,\n",
              "  1499282.5,\n",
              "  1491994.375,\n",
              "  1506359.125,\n",
              "  1503887.25,\n",
              "  1501965.125,\n",
              "  1506014.875,\n",
              "  1514910.875,\n",
              "  1504018.75,\n",
              "  1468060.25,\n",
              "  1500011.5,\n",
              "  1507869.125,\n",
              "  1496374.125,\n",
              "  1504766.625,\n",
              "  1485917.625,\n",
              "  1473575.875,\n",
              "  1486936.25,\n",
              "  1496843.625,\n",
              "  1492259.5,\n",
              "  1501368.5,\n",
              "  1488260.0,\n",
              "  1497936.5,\n",
              "  1503674.0,\n",
              "  1498933.625,\n",
              "  1487127.375,\n",
              "  1506876.75,\n",
              "  1495722.75,\n",
              "  1479400.5,\n",
              "  1499250.75,\n",
              "  1467238.25,\n",
              "  1519997.25,\n",
              "  1487971.875,\n",
              "  1511044.5,\n",
              "  1503621.625,\n",
              "  1496572.875,\n",
              "  1477199.375,\n",
              "  1504871.0,\n",
              "  1481985.375,\n",
              "  1484818.75,\n",
              "  1505331.375,\n",
              "  1489390.75,\n",
              "  1487005.5,\n",
              "  1502418.625,\n",
              "  1503317.75,\n",
              "  1493833.875,\n",
              "  1495257.375,\n",
              "  1489792.125,\n",
              "  1498479.125,\n",
              "  1484321.0,\n",
              "  1513805.625,\n",
              "  1494163.25,\n",
              "  1501193.5,\n",
              "  1483598.625,\n",
              "  1507464.625,\n",
              "  1481781.75,\n",
              "  1498002.625,\n",
              "  1506425.125,\n",
              "  1474123.75,\n",
              "  1504510.625,\n",
              "  1504499.5,\n",
              "  1501263.75,\n",
              "  1496033.25,\n",
              "  1497339.875,\n",
              "  1494796.25,\n",
              "  1484730.75,\n",
              "  1511896.375,\n",
              "  1470312.375,\n",
              "  1484227.625,\n",
              "  1478168.5,\n",
              "  1492062.0,\n",
              "  1484031.5,\n",
              "  1506140.125,\n",
              "  1491647.125,\n",
              "  1471848.75,\n",
              "  1479649.375,\n",
              "  1479439.375,\n",
              "  1495394.75,\n",
              "  1469196.375,\n",
              "  1492060.875,\n",
              "  1501797.75,\n",
              "  1491230.75,\n",
              "  1487680.625,\n",
              "  1496848.125,\n",
              "  1483839.5,\n",
              "  1492969.0,\n",
              "  1456627.375,\n",
              "  1487008.5,\n",
              "  1491890.125,\n",
              "  1497044.75,\n",
              "  1497930.0,\n",
              "  1497295.625,\n",
              "  1479832.875,\n",
              "  1476368.75,\n",
              "  1485433.875,\n",
              "  1472559.25,\n",
              "  1490059.25,\n",
              "  1482861.0,\n",
              "  1497962.5,\n",
              "  1492519.75,\n",
              "  1492599.375,\n",
              "  1482512.0,\n",
              "  1478388.5,\n",
              "  1493616.125,\n",
              "  1488450.0,\n",
              "  1482475.25,\n",
              "  1463809.375,\n",
              "  1482926.625,\n",
              "  1499718.625,\n",
              "  1497092.125,\n",
              "  1488690.25,\n",
              "  1481407.0,\n",
              "  1487622.625,\n",
              "  1481105.875,\n",
              "  1471481.0,\n",
              "  1488059.25,\n",
              "  1495652.125,\n",
              "  1485886.875,\n",
              "  1502375.375,\n",
              "  1477885.125,\n",
              "  1487396.25,\n",
              "  1474520.75,\n",
              "  1493876.25,\n",
              "  1476568.75,\n",
              "  1485193.875,\n",
              "  1486338.875,\n",
              "  1500524.375,\n",
              "  1489019.625,\n",
              "  1476191.625,\n",
              "  1474556.375,\n",
              "  1479547.5,\n",
              "  1450085.0,\n",
              "  1495337.25,\n",
              "  1490369.75,\n",
              "  1485237.75,\n",
              "  1485919.875,\n",
              "  1480749.75,\n",
              "  1483843.625,\n",
              "  1459975.875,\n",
              "  1484836.25,\n",
              "  1483460.375,\n",
              "  1469287.875,\n",
              "  1501247.25,\n",
              "  1466388.625,\n",
              "  1481379.625,\n",
              "  1478357.125,\n",
              "  1491691.25,\n",
              "  1474337.125,\n",
              "  1454816.625,\n",
              "  1470378.5,\n",
              "  1465462.875,\n",
              "  1471956.5,\n",
              "  1488950.625,\n",
              "  1480065.375,\n",
              "  1491630.625,\n",
              "  1477437.875,\n",
              "  1461602.625,\n",
              "  1481969.0,\n",
              "  1490941.375,\n",
              "  1494033.375,\n",
              "  1487720.5,\n",
              "  1487949.375,\n",
              "  1480596.25,\n",
              "  1462993.875,\n",
              "  1477596.0,\n",
              "  1462971.625,\n",
              "  1480373.125,\n",
              "  1477654.75,\n",
              "  1473292.5,\n",
              "  1481343.5,\n",
              "  1483497.125,\n",
              "  1489262.375,\n",
              "  1477014.375,\n",
              "  1482884.625,\n",
              "  1487836.5,\n",
              "  1480517.375,\n",
              "  1474460.875,\n",
              "  1492626.25,\n",
              "  1462831.0,\n",
              "  1456608.5,\n",
              "  1473300.625,\n",
              "  1468728.25,\n",
              "  1484760.625,\n",
              "  1488531.75,\n",
              "  1487725.875,\n",
              "  1446159.25,\n",
              "  1489643.625,\n",
              "  1475813.125,\n",
              "  1484252.625,\n",
              "  1475313.25,\n",
              "  1473926.625,\n",
              "  1476494.0,\n",
              "  1478945.25,\n",
              "  1466790.0,\n",
              "  1475267.875,\n",
              "  1475347.125,\n",
              "  1468312.25,\n",
              "  1481572.5,\n",
              "  1480187.875,\n",
              "  1465624.25,\n",
              "  1440562.875,\n",
              "  1485782.875,\n",
              "  1473617.5,\n",
              "  1464731.875,\n",
              "  1460211.75,\n",
              "  1492326.5,\n",
              "  1457833.5,\n",
              "  1466785.0,\n",
              "  1480101.25,\n",
              "  1471076.0,\n",
              "  1463247.0,\n",
              "  1484817.125,\n",
              "  1486189.75,\n",
              "  1473741.625,\n",
              "  1464889.0,\n",
              "  1446159.5,\n",
              "  1462960.375,\n",
              "  1456678.25,\n",
              "  1477035.625,\n",
              "  1474204.375,\n",
              "  1470479.125,\n",
              "  1458747.625,\n",
              "  1475036.125,\n",
              "  1480194.0,\n",
              "  1466096.25,\n",
              "  1463315.25,\n",
              "  1456478.625,\n",
              "  1466103.125,\n",
              "  1472037.0,\n",
              "  1476479.625,\n",
              "  1476951.75,\n",
              "  1472150.0,\n",
              "  1448196.5,\n",
              "  1484990.625,\n",
              "  1465051.75,\n",
              "  1461177.375,\n",
              "  1477453.375,\n",
              "  1460825.25,\n",
              "  1463796.125,\n",
              "  1459588.125,\n",
              "  1480512.5,\n",
              "  1453088.875,\n",
              "  1454920.375,\n",
              "  1470137.375,\n",
              "  1470787.375,\n",
              "  1462830.375,\n",
              "  1477194.875,\n",
              "  1448302.875,\n",
              "  1461085.625,\n",
              "  1467292.625,\n",
              "  1458535.5,\n",
              "  1475891.875,\n",
              "  1477759.5,\n",
              "  1462081.125,\n",
              "  1433002.25,\n",
              "  1461664.5,\n",
              "  1447619.875,\n",
              "  1471322.125,\n",
              "  1463221.5,\n",
              "  1468415.5,\n",
              "  1479776.125,\n",
              "  1458097.875,\n",
              "  1461822.75,\n",
              "  1466285.125,\n",
              "  1433980.375,\n",
              "  1457099.125,\n",
              "  1455461.0,\n",
              "  1480389.75,\n",
              "  1476671.875,\n",
              "  1460290.75,\n",
              "  1447568.375,\n",
              "  1456888.5,\n",
              "  1465692.75,\n",
              "  1466245.875,\n",
              "  1460078.25,\n",
              "  1469946.5,\n",
              "  1467488.5,\n",
              "  1468391.375,\n",
              "  1449109.125,\n",
              "  1448354.5,\n",
              "  1447535.875,\n",
              "  1469189.375,\n",
              "  1476869.875,\n",
              "  1466287.5,\n",
              "  1454109.5,\n",
              "  1451600.25,\n",
              "  1467465.0,\n",
              "  1454493.375,\n",
              "  1447610.625,\n",
              "  1466053.625,\n",
              "  1454857.875,\n",
              "  1464831.125,\n",
              "  1460920.25,\n",
              "  1452008.875,\n",
              "  1459285.875,\n",
              "  1440487.5,\n",
              "  1473654.625,\n",
              "  1446726.5,\n",
              "  1471255.5,\n",
              "  1462158.125,\n",
              "  1457902.75,\n",
              "  1443964.125,\n",
              "  1466554.25,\n",
              "  1437695.625,\n",
              "  1452529.875,\n",
              "  1469603.5,\n",
              "  1459344.375,\n",
              "  1473912.25,\n",
              "  1457217.5,\n",
              "  1430746.125,\n",
              "  1450246.875,\n",
              "  1467656.125,\n",
              "  1437764.875,\n",
              "  1469515.0,\n",
              "  1470022.875,\n",
              "  1453711.125,\n",
              "  1454610.875,\n",
              "  1456777.75,\n",
              "  1463070.875,\n",
              "  1457871.25,\n",
              "  1454857.125,\n",
              "  1457926.5,\n",
              "  1450543.875,\n",
              "  1450133.5,\n",
              "  1459206.75,\n",
              "  1462044.25,\n",
              "  1455155.625,\n",
              "  1439035.25,\n",
              "  1454451.125,\n",
              "  1467413.875,\n",
              "  1439145.5,\n",
              "  1435253.125,\n",
              "  1473303.5,\n",
              "  1450680.75,\n",
              "  1448835.625,\n",
              "  1453909.5,\n",
              "  1465139.875,\n",
              "  1456699.0,\n",
              "  1472639.25,\n",
              "  1463582.875,\n",
              "  1444786.375,\n",
              "  1462811.0,\n",
              "  1456261.875,\n",
              "  1460289.375,\n",
              "  1459733.5,\n",
              "  1443509.0,\n",
              "  1442066.75,\n",
              "  1453973.5,\n",
              "  1458611.0,\n",
              "  1449520.875,\n",
              "  1450000.0,\n",
              "  1460284.375,\n",
              "  1439387.5,\n",
              "  1454405.875,\n",
              "  1457500.75,\n",
              "  1460925.5,\n",
              "  1447578.75,\n",
              "  1453489.625,\n",
              "  1457768.75,\n",
              "  1450092.75,\n",
              "  1463042.875,\n",
              "  1432164.125,\n",
              "  1432483.375,\n",
              "  1418152.875,\n",
              "  1460038.125,\n",
              "  1463920.875,\n",
              "  1449876.375,\n",
              "  1456699.875,\n",
              "  1444291.0,\n",
              "  1454157.5,\n",
              "  1440070.5,\n",
              "  1454452.75,\n",
              "  1444614.125,\n",
              "  1450223.125,\n",
              "  1434929.5,\n",
              "  1459059.5,\n",
              "  1457218.375,\n",
              "  1451975.5,\n",
              "  1427902.625,\n",
              "  1444207.875,\n",
              "  1452294.25,\n",
              "  1456965.0,\n",
              "  1459420.5,\n",
              "  1457253.125,\n",
              "  1449025.375,\n",
              "  1451328.5,\n",
              "  1443035.125,\n",
              "  1460367.0,\n",
              "  1448587.5,\n",
              "  1425032.375,\n",
              "  1456197.0,\n",
              "  1449927.75,\n",
              "  1437339.25,\n",
              "  1430679.375,\n",
              "  1428761.25,\n",
              "  1450033.125,\n",
              "  1433449.5,\n",
              "  1429981.75,\n",
              "  1448572.0,\n",
              "  1433217.75,\n",
              "  1458515.0,\n",
              "  1426477.0,\n",
              "  1433396.25,\n",
              "  1439914.75,\n",
              "  1444797.25,\n",
              "  1448580.0,\n",
              "  1444745.75,\n",
              "  1438385.75,\n",
              "  1438382.5,\n",
              "  1450908.5,\n",
              "  1429824.75,\n",
              "  1441079.75,\n",
              "  1428162.625,\n",
              "  1429422.125,\n",
              "  1447957.25,\n",
              "  1448444.875,\n",
              "  1419745.125,\n",
              "  1455238.625,\n",
              "  1421649.0,\n",
              "  1450989.75,\n",
              "  1419957.0,\n",
              "  1426019.375,\n",
              "  1436525.5,\n",
              "  1437573.875,\n",
              "  1459628.375,\n",
              "  1457979.875,\n",
              "  1438364.875,\n",
              "  1444454.0,\n",
              "  1444005.75,\n",
              "  1443961.125,\n",
              "  1448676.0],\n",
              " 'mse': [1516518.75,\n",
              "  1506363.125,\n",
              "  1507930.0,\n",
              "  1508656.625,\n",
              "  1505641.5,\n",
              "  1492480.375,\n",
              "  1514817.125,\n",
              "  1508224.5,\n",
              "  1496289.625,\n",
              "  1516412.875,\n",
              "  1521321.125,\n",
              "  1526450.75,\n",
              "  1521686.0,\n",
              "  1504394.875,\n",
              "  1501763.5,\n",
              "  1503003.875,\n",
              "  1491755.0,\n",
              "  1489553.125,\n",
              "  1512119.125,\n",
              "  1494952.75,\n",
              "  1504440.25,\n",
              "  1500903.25,\n",
              "  1501924.5,\n",
              "  1498644.0,\n",
              "  1499875.75,\n",
              "  1518455.25,\n",
              "  1510430.375,\n",
              "  1515438.125,\n",
              "  1498455.75,\n",
              "  1499913.375,\n",
              "  1504160.875,\n",
              "  1508111.125,\n",
              "  1506441.375,\n",
              "  1505892.625,\n",
              "  1526953.625,\n",
              "  1521998.25,\n",
              "  1529106.375,\n",
              "  1509687.625,\n",
              "  1498651.75,\n",
              "  1513464.625,\n",
              "  1501841.625,\n",
              "  1514421.25,\n",
              "  1511862.5,\n",
              "  1494262.875,\n",
              "  1514574.0,\n",
              "  1501229.75,\n",
              "  1518473.375,\n",
              "  1510203.875,\n",
              "  1489474.0,\n",
              "  1489945.125,\n",
              "  1512915.125,\n",
              "  1483195.75,\n",
              "  1499369.375,\n",
              "  1514341.375,\n",
              "  1496078.25,\n",
              "  1493722.75,\n",
              "  1503530.0,\n",
              "  1500965.25,\n",
              "  1503888.75,\n",
              "  1496408.25,\n",
              "  1495677.5,\n",
              "  1503660.125,\n",
              "  1482232.0,\n",
              "  1502316.375,\n",
              "  1520752.125,\n",
              "  1520212.0,\n",
              "  1500745.875,\n",
              "  1490273.125,\n",
              "  1502591.875,\n",
              "  1483654.125,\n",
              "  1508383.5,\n",
              "  1499282.5,\n",
              "  1491994.375,\n",
              "  1506359.125,\n",
              "  1503887.25,\n",
              "  1501965.125,\n",
              "  1506014.875,\n",
              "  1514910.875,\n",
              "  1504018.75,\n",
              "  1468060.25,\n",
              "  1500011.5,\n",
              "  1507869.125,\n",
              "  1496374.125,\n",
              "  1504766.625,\n",
              "  1485917.625,\n",
              "  1473575.875,\n",
              "  1486936.25,\n",
              "  1496843.625,\n",
              "  1492259.5,\n",
              "  1501368.5,\n",
              "  1488260.0,\n",
              "  1497936.5,\n",
              "  1503674.0,\n",
              "  1498933.625,\n",
              "  1487127.375,\n",
              "  1506876.75,\n",
              "  1495722.75,\n",
              "  1479400.5,\n",
              "  1499250.75,\n",
              "  1467238.25,\n",
              "  1519997.25,\n",
              "  1487971.875,\n",
              "  1511044.5,\n",
              "  1503621.625,\n",
              "  1496572.875,\n",
              "  1477199.375,\n",
              "  1504871.0,\n",
              "  1481985.375,\n",
              "  1484818.75,\n",
              "  1505331.375,\n",
              "  1489390.75,\n",
              "  1487005.5,\n",
              "  1502418.625,\n",
              "  1503317.75,\n",
              "  1493833.875,\n",
              "  1495257.375,\n",
              "  1489792.125,\n",
              "  1498479.125,\n",
              "  1484321.0,\n",
              "  1513805.625,\n",
              "  1494163.25,\n",
              "  1501193.5,\n",
              "  1483598.625,\n",
              "  1507464.625,\n",
              "  1481781.75,\n",
              "  1498002.625,\n",
              "  1506425.125,\n",
              "  1474123.75,\n",
              "  1504510.625,\n",
              "  1504499.5,\n",
              "  1501263.75,\n",
              "  1496033.25,\n",
              "  1497339.875,\n",
              "  1494796.25,\n",
              "  1484730.75,\n",
              "  1511896.375,\n",
              "  1470312.375,\n",
              "  1484227.625,\n",
              "  1478168.5,\n",
              "  1492062.0,\n",
              "  1484031.5,\n",
              "  1506140.125,\n",
              "  1491647.125,\n",
              "  1471848.75,\n",
              "  1479649.375,\n",
              "  1479439.375,\n",
              "  1495394.75,\n",
              "  1469196.375,\n",
              "  1492060.875,\n",
              "  1501797.75,\n",
              "  1491230.75,\n",
              "  1487680.625,\n",
              "  1496848.125,\n",
              "  1483839.5,\n",
              "  1492969.0,\n",
              "  1456627.375,\n",
              "  1487008.5,\n",
              "  1491890.125,\n",
              "  1497044.75,\n",
              "  1497930.0,\n",
              "  1497295.625,\n",
              "  1479832.875,\n",
              "  1476368.75,\n",
              "  1485433.875,\n",
              "  1472559.25,\n",
              "  1490059.25,\n",
              "  1482861.0,\n",
              "  1497962.5,\n",
              "  1492519.75,\n",
              "  1492599.375,\n",
              "  1482512.0,\n",
              "  1478388.5,\n",
              "  1493616.125,\n",
              "  1488450.0,\n",
              "  1482475.25,\n",
              "  1463809.375,\n",
              "  1482926.625,\n",
              "  1499718.625,\n",
              "  1497092.125,\n",
              "  1488690.25,\n",
              "  1481407.0,\n",
              "  1487622.625,\n",
              "  1481105.875,\n",
              "  1471481.0,\n",
              "  1488059.25,\n",
              "  1495652.125,\n",
              "  1485886.875,\n",
              "  1502375.375,\n",
              "  1477885.125,\n",
              "  1487396.25,\n",
              "  1474520.75,\n",
              "  1493876.25,\n",
              "  1476568.75,\n",
              "  1485193.875,\n",
              "  1486338.875,\n",
              "  1500524.375,\n",
              "  1489019.625,\n",
              "  1476191.625,\n",
              "  1474556.375,\n",
              "  1479547.5,\n",
              "  1450085.0,\n",
              "  1495337.25,\n",
              "  1490369.75,\n",
              "  1485237.75,\n",
              "  1485919.875,\n",
              "  1480749.75,\n",
              "  1483843.625,\n",
              "  1459975.875,\n",
              "  1484836.25,\n",
              "  1483460.375,\n",
              "  1469287.875,\n",
              "  1501247.25,\n",
              "  1466388.625,\n",
              "  1481379.625,\n",
              "  1478357.125,\n",
              "  1491691.25,\n",
              "  1474337.125,\n",
              "  1454816.625,\n",
              "  1470378.5,\n",
              "  1465462.875,\n",
              "  1471956.5,\n",
              "  1488950.625,\n",
              "  1480065.375,\n",
              "  1491630.625,\n",
              "  1477437.875,\n",
              "  1461602.625,\n",
              "  1481969.0,\n",
              "  1490941.375,\n",
              "  1494033.375,\n",
              "  1487720.5,\n",
              "  1487949.375,\n",
              "  1480596.25,\n",
              "  1462993.875,\n",
              "  1477596.0,\n",
              "  1462971.625,\n",
              "  1480373.125,\n",
              "  1477654.75,\n",
              "  1473292.5,\n",
              "  1481343.5,\n",
              "  1483497.125,\n",
              "  1489262.375,\n",
              "  1477014.375,\n",
              "  1482884.625,\n",
              "  1487836.5,\n",
              "  1480517.375,\n",
              "  1474460.875,\n",
              "  1492626.25,\n",
              "  1462831.0,\n",
              "  1456608.5,\n",
              "  1473300.625,\n",
              "  1468728.25,\n",
              "  1484760.625,\n",
              "  1488531.75,\n",
              "  1487725.875,\n",
              "  1446159.25,\n",
              "  1489643.625,\n",
              "  1475813.125,\n",
              "  1484252.625,\n",
              "  1475313.25,\n",
              "  1473926.625,\n",
              "  1476494.0,\n",
              "  1478945.25,\n",
              "  1466790.0,\n",
              "  1475267.875,\n",
              "  1475347.125,\n",
              "  1468312.25,\n",
              "  1481572.5,\n",
              "  1480187.875,\n",
              "  1465624.25,\n",
              "  1440562.875,\n",
              "  1485782.875,\n",
              "  1473617.5,\n",
              "  1464731.875,\n",
              "  1460211.75,\n",
              "  1492326.5,\n",
              "  1457833.5,\n",
              "  1466785.0,\n",
              "  1480101.25,\n",
              "  1471076.0,\n",
              "  1463247.0,\n",
              "  1484817.125,\n",
              "  1486189.75,\n",
              "  1473741.625,\n",
              "  1464889.0,\n",
              "  1446159.5,\n",
              "  1462960.375,\n",
              "  1456678.25,\n",
              "  1477035.625,\n",
              "  1474204.375,\n",
              "  1470479.125,\n",
              "  1458747.625,\n",
              "  1475036.125,\n",
              "  1480194.0,\n",
              "  1466096.25,\n",
              "  1463315.25,\n",
              "  1456478.625,\n",
              "  1466103.125,\n",
              "  1472037.0,\n",
              "  1476479.625,\n",
              "  1476951.75,\n",
              "  1472150.0,\n",
              "  1448196.5,\n",
              "  1484990.625,\n",
              "  1465051.75,\n",
              "  1461177.375,\n",
              "  1477453.375,\n",
              "  1460825.25,\n",
              "  1463796.125,\n",
              "  1459588.125,\n",
              "  1480512.5,\n",
              "  1453088.875,\n",
              "  1454920.375,\n",
              "  1470137.375,\n",
              "  1470787.375,\n",
              "  1462830.375,\n",
              "  1477194.875,\n",
              "  1448302.875,\n",
              "  1461085.625,\n",
              "  1467292.625,\n",
              "  1458535.5,\n",
              "  1475891.875,\n",
              "  1477759.5,\n",
              "  1462081.125,\n",
              "  1433002.25,\n",
              "  1461664.5,\n",
              "  1447619.875,\n",
              "  1471322.125,\n",
              "  1463221.5,\n",
              "  1468415.5,\n",
              "  1479776.125,\n",
              "  1458097.875,\n",
              "  1461822.75,\n",
              "  1466285.125,\n",
              "  1433980.375,\n",
              "  1457099.125,\n",
              "  1455461.0,\n",
              "  1480389.75,\n",
              "  1476671.875,\n",
              "  1460290.75,\n",
              "  1447568.375,\n",
              "  1456888.5,\n",
              "  1465692.75,\n",
              "  1466245.875,\n",
              "  1460078.25,\n",
              "  1469946.5,\n",
              "  1467488.5,\n",
              "  1468391.375,\n",
              "  1449109.125,\n",
              "  1448354.5,\n",
              "  1447535.875,\n",
              "  1469189.375,\n",
              "  1476869.875,\n",
              "  1466287.5,\n",
              "  1454109.5,\n",
              "  1451600.25,\n",
              "  1467465.0,\n",
              "  1454493.375,\n",
              "  1447610.625,\n",
              "  1466053.625,\n",
              "  1454857.875,\n",
              "  1464831.125,\n",
              "  1460920.25,\n",
              "  1452008.875,\n",
              "  1459285.875,\n",
              "  1440487.5,\n",
              "  1473654.625,\n",
              "  1446726.5,\n",
              "  1471255.5,\n",
              "  1462158.125,\n",
              "  1457902.75,\n",
              "  1443964.125,\n",
              "  1466554.25,\n",
              "  1437695.625,\n",
              "  1452529.875,\n",
              "  1469603.5,\n",
              "  1459344.375,\n",
              "  1473912.25,\n",
              "  1457217.5,\n",
              "  1430746.125,\n",
              "  1450246.875,\n",
              "  1467656.125,\n",
              "  1437764.875,\n",
              "  1469515.0,\n",
              "  1470022.875,\n",
              "  1453711.125,\n",
              "  1454610.875,\n",
              "  1456777.75,\n",
              "  1463070.875,\n",
              "  1457871.25,\n",
              "  1454857.125,\n",
              "  1457926.5,\n",
              "  1450543.875,\n",
              "  1450133.5,\n",
              "  1459206.75,\n",
              "  1462044.25,\n",
              "  1455155.625,\n",
              "  1439035.25,\n",
              "  1454451.125,\n",
              "  1467413.875,\n",
              "  1439145.5,\n",
              "  1435253.125,\n",
              "  1473303.5,\n",
              "  1450680.75,\n",
              "  1448835.625,\n",
              "  1453909.5,\n",
              "  1465139.875,\n",
              "  1456699.0,\n",
              "  1472639.25,\n",
              "  1463582.875,\n",
              "  1444786.375,\n",
              "  1462811.0,\n",
              "  1456261.875,\n",
              "  1460289.375,\n",
              "  1459733.5,\n",
              "  1443509.0,\n",
              "  1442066.75,\n",
              "  1453973.5,\n",
              "  1458611.0,\n",
              "  1449520.875,\n",
              "  1450000.0,\n",
              "  1460284.375,\n",
              "  1439387.5,\n",
              "  1454405.875,\n",
              "  1457500.75,\n",
              "  1460925.5,\n",
              "  1447578.75,\n",
              "  1453489.625,\n",
              "  1457768.75,\n",
              "  1450092.75,\n",
              "  1463042.875,\n",
              "  1432164.125,\n",
              "  1432483.375,\n",
              "  1418152.875,\n",
              "  1460038.125,\n",
              "  1463920.875,\n",
              "  1449876.375,\n",
              "  1456699.875,\n",
              "  1444291.0,\n",
              "  1454157.5,\n",
              "  1440070.5,\n",
              "  1454452.75,\n",
              "  1444614.125,\n",
              "  1450223.125,\n",
              "  1434929.5,\n",
              "  1459059.5,\n",
              "  1457218.375,\n",
              "  1451975.5,\n",
              "  1427902.625,\n",
              "  1444207.875,\n",
              "  1452294.25,\n",
              "  1456965.0,\n",
              "  1459420.5,\n",
              "  1457253.125,\n",
              "  1449025.375,\n",
              "  1451328.5,\n",
              "  1443035.125,\n",
              "  1460367.0,\n",
              "  1448587.5,\n",
              "  1425032.375,\n",
              "  1456197.0,\n",
              "  1449927.75,\n",
              "  1437339.25,\n",
              "  1430679.375,\n",
              "  1428761.25,\n",
              "  1450033.125,\n",
              "  1433449.5,\n",
              "  1429981.75,\n",
              "  1448572.0,\n",
              "  1433217.75,\n",
              "  1458515.0,\n",
              "  1426477.0,\n",
              "  1433396.25,\n",
              "  1439914.75,\n",
              "  1444797.25,\n",
              "  1448580.0,\n",
              "  1444745.75,\n",
              "  1438385.75,\n",
              "  1438382.5,\n",
              "  1450908.5,\n",
              "  1429824.75,\n",
              "  1441079.75,\n",
              "  1428162.625,\n",
              "  1429422.125,\n",
              "  1447957.25,\n",
              "  1448444.875,\n",
              "  1419745.125,\n",
              "  1455238.625,\n",
              "  1421649.0,\n",
              "  1450989.75,\n",
              "  1419957.0,\n",
              "  1426019.375,\n",
              "  1436525.5,\n",
              "  1437573.875,\n",
              "  1459628.375,\n",
              "  1457979.875,\n",
              "  1438364.875,\n",
              "  1444454.0,\n",
              "  1444005.75,\n",
              "  1443961.125,\n",
              "  1448676.0],\n",
              " 'val_loss': [500053.40625,\n",
              "  521621.40625,\n",
              "  547272.375,\n",
              "  538520.4375,\n",
              "  538382.4375,\n",
              "  547021.9375,\n",
              "  508306.46875,\n",
              "  550995.0625,\n",
              "  542301.6875,\n",
              "  520485.125,\n",
              "  537996.4375,\n",
              "  520378.59375,\n",
              "  516159.15625,\n",
              "  546458.75,\n",
              "  524655.25,\n",
              "  528765.9375,\n",
              "  528740.6875,\n",
              "  528604.3125,\n",
              "  537227.5625,\n",
              "  520223.125,\n",
              "  541254.6875,\n",
              "  536980.8125,\n",
              "  511174.03125,\n",
              "  523983.25,\n",
              "  515151.46875,\n",
              "  545402.375,\n",
              "  545317.5625,\n",
              "  514964.875,\n",
              "  523577.96875,\n",
              "  549252.9375,\n",
              "  536346.5625,\n",
              "  527522.1875,\n",
              "  527387.8125,\n",
              "  519042.09375,\n",
              "  540093.9375,\n",
              "  522956.625,\n",
              "  539137.0,\n",
              "  531164.4375,\n",
              "  518218.25,\n",
              "  534760.8125,\n",
              "  531284.25,\n",
              "  539515.25,\n",
              "  509169.96875,\n",
              "  539351.0625,\n",
              "  530541.5,\n",
              "  535057.3125,\n",
              "  526199.625,\n",
              "  543562.6875,\n",
              "  534811.0,\n",
              "  526062.4375,\n",
              "  546704.6875,\n",
              "  534511.5625,\n",
              "  521692.71875,\n",
              "  530275.9375,\n",
              "  508641.59375,\n",
              "  530112.3125,\n",
              "  529924.4375,\n",
              "  521240.53125,\n",
              "  516575.59375,\n",
              "  533856.1875,\n",
              "  542480.3125,\n",
              "  529621.3125,\n",
              "  533663.3125,\n",
              "  524931.9375,\n",
              "  520728.78125,\n",
              "  533364.0625,\n",
              "  536775.1875,\n",
              "  541898.0625,\n",
              "  520302.84375,\n",
              "  520327.0,\n",
              "  554463.0,\n",
              "  532823.4375,\n",
              "  524102.71875,\n",
              "  528643.5625,\n",
              "  511238.90625,\n",
              "  541130.5625,\n",
              "  523884.375,\n",
              "  523699.71875,\n",
              "  523671.59375,\n",
              "  514961.34375,\n",
              "  506647.65625,\n",
              "  506125.875,\n",
              "  531926.3125,\n",
              "  540572.1875,\n",
              "  540540.3125,\n",
              "  510371.5,\n",
              "  531074.0625,\n",
              "  510162.34375,\n",
              "  531539.3125,\n",
              "  531459.125,\n",
              "  518594.875,\n",
              "  531295.6875,\n",
              "  522548.0,\n",
              "  543859.4375,\n",
              "  522388.75,\n",
              "  522256.40625,\n",
              "  522226.78125,\n",
              "  530805.875,\n",
              "  539281.0,\n",
              "  552016.3125,\n",
              "  543226.9375,\n",
              "  504948.40625,\n",
              "  517583.78125,\n",
              "  517554.59375,\n",
              "  512986.25,\n",
              "  521454.65625,\n",
              "  512098.84375,\n",
              "  499993.90625,\n",
              "  508512.59375,\n",
              "  491619.34375,\n",
              "  516947.84375,\n",
              "  529615.25,\n",
              "  525424.8125,\n",
              "  512227.65625,\n",
              "  499873.5,\n",
              "  503853.84375,\n",
              "  516467.0,\n",
              "  533234.5625,\n",
              "  529095.4375,\n",
              "  516276.875,\n",
              "  528883.125,\n",
              "  524695.5,\n",
              "  541453.0,\n",
              "  520063.03125,\n",
              "  528608.6875,\n",
              "  515434.84375,\n",
              "  519872.96875,\n",
              "  541040.4375,\n",
              "  528233.4375,\n",
              "  532256.9375,\n",
              "  524065.90625,\n",
              "  523528.0,\n",
              "  531333.8125,\n",
              "  515212.40625,\n",
              "  493808.09375,\n",
              "  540377.4375,\n",
              "  540247.4375,\n",
              "  523047.46875,\n",
              "  502107.84375,\n",
              "  527343.8125,\n",
              "  497802.25,\n",
              "  518676.46875,\n",
              "  526372.0625,\n",
              "  548269.3125,\n",
              "  526260.5625,\n",
              "  518262.0,\n",
              "  539522.125,\n",
              "  501010.46875,\n",
              "  500255.65625,\n",
              "  517362.84375,\n",
              "  513418.34375,\n",
              "  539062.0,\n",
              "  534833.875,\n",
              "  530309.6875,\n",
              "  521987.84375,\n",
              "  534635.5625,\n",
              "  492124.5,\n",
              "  513259.75,\n",
              "  496064.375,\n",
              "  525725.0625,\n",
              "  513068.0,\n",
              "  534189.6875,\n",
              "  512910.28125,\n",
              "  525499.9375,\n",
              "  520891.875,\n",
              "  516717.25,\n",
              "  525162.0,\n",
              "  525127.9375,\n",
              "  520906.59375,\n",
              "  533485.0,\n",
              "  516318.34375,\n",
              "  516287.34375,\n",
              "  520297.75,\n",
              "  524642.6875,\n",
              "  532979.75,\n",
              "  520297.40625,\n",
              "  524354.75,\n",
              "  536919.3125,\n",
              "  515683.40625,\n",
              "  524112.15625,\n",
              "  519659.75,\n",
              "  519955.46875,\n",
              "  528004.6875,\n",
              "  544927.5625,\n",
              "  511073.59375,\n",
              "  515127.75,\n",
              "  523594.25,\n",
              "  523466.34375,\n",
              "  510757.59375,\n",
              "  518944.40625,\n",
              "  523179.46875,\n",
              "  523193.03125,\n",
              "  518980.40625,\n",
              "  544096.1875,\n",
              "  505253.53125,\n",
              "  510299.375,\n",
              "  535361.1875,\n",
              "  514179.125,\n",
              "  509969.96875,\n",
              "  509936.84375,\n",
              "  513989.71875,\n",
              "  522388.53125,\n",
              "  522262.34375,\n",
              "  526263.1875,\n",
              "  517973.875,\n",
              "  513547.15625,\n",
              "  521987.53125,\n",
              "  530332.0625,\n",
              "  525861.3125,\n",
              "  521746.25,\n",
              "  509070.875,\n",
              "  496445.5,\n",
              "  512993.21875,\n",
              "  533971.625,\n",
              "  500246.03125,\n",
              "  500214.03125,\n",
              "  512003.84375,\n",
              "  521059.5,\n",
              "  495860.34375,\n",
              "  511814.15625,\n",
              "  524941.6875,\n",
              "  529239.6875,\n",
              "  524734.4375,\n",
              "  495473.25,\n",
              "  516374.84375,\n",
              "  528822.3125,\n",
              "  503738.625,\n",
              "  516183.40625,\n",
              "  541236.6875,\n",
              "  499087.34375,\n",
              "  524045.28125,\n",
              "  511449.34375,\n",
              "  511372.375,\n",
              "  511294.21875,\n",
              "  532302.9375,\n",
              "  511181.15625,\n",
              "  502620.5,\n",
              "  527987.0625,\n",
              "  523496.25,\n",
              "  506122.25,\n",
              "  506671.5,\n",
              "  502567.40625,\n",
              "  498083.59375,\n",
              "  518933.375,\n",
              "  502376.59375,\n",
              "  527199.6875,\n",
              "  531321.625,\n",
              "  518747.71875,\n",
              "  502063.09375,\n",
              "  527054.1875,\n",
              "  526884.1875,\n",
              "  501457.90625,\n",
              "  505147.15625,\n",
              "  505695.375,\n",
              "  514122.71875,\n",
              "  497126.375,\n",
              "  484527.09375,\n",
              "  526270.75,\n",
              "  500915.5,\n",
              "  513635.03125,\n",
              "  496739.65625,\n",
              "  512481.59375,\n",
              "  525951.3125,\n",
              "  488379.59375,\n",
              "  504834.34375,\n",
              "  513157.53125,\n",
              "  500659.28125,\n",
              "  491467.875,\n",
              "  508584.21875,\n",
              "  537758.125,\n",
              "  504367.09375,\n",
              "  500270.34375,\n",
              "  516708.90625,\n",
              "  504133.0,\n",
              "  504011.53125,\n",
              "  495587.78125,\n",
              "  516391.375,\n",
              "  503821.625,\n",
              "  520248.125,\n",
              "  499563.96875,\n",
              "  507646.09375,\n",
              "  520009.34375,\n",
              "  507447.53125,\n",
              "  507456.25,\n",
              "  528149.0625,\n",
              "  511620.375,\n",
              "  523973.375,\n",
              "  519574.75,\n",
              "  507023.03125,\n",
              "  536245.5625,\n",
              "  510965.625,\n",
              "  498419.59375,\n",
              "  515121.15625,\n",
              "  494166.90625,\n",
              "  490077.96875,\n",
              "  514883.34375,\n",
              "  506397.0,\n",
              "  518735.875,\n",
              "  501933.75,\n",
              "  518660.21875,\n",
              "  510138.09375,\n",
              "  514406.09375,\n",
              "  514328.40625,\n",
              "  501798.5,\n",
              "  514128.15625,\n",
              "  505737.5,\n",
              "  517978.75,\n",
              "  497149.90625,\n",
              "  534644.8125,\n",
              "  504758.84375,\n",
              "  492909.59375,\n",
              "  497170.84375,\n",
              "  513579.59375,\n",
              "  513416.09375,\n",
              "  525810.8125,\n",
              "  525687.1875,\n",
              "  513219.78125,\n",
              "  500755.875,\n",
              "  500636.65625,\n",
              "  517029.90625,\n",
              "  512861.0,\n",
              "  512824.28125,\n",
              "  496237.46875,\n",
              "  516713.59375,\n",
              "  524880.0625,\n",
              "  500094.46875,\n",
              "  504062.25,\n",
              "  487570.21875,\n",
              "  512189.46875,\n",
              "  503119.03125,\n",
              "  512156.125,\n",
              "  503633.09375,\n",
              "  516000.625,\n",
              "  478755.25,\n",
              "  511717.65625,\n",
              "  507407.28125,\n",
              "  503286.90625,\n",
              "  486769.75,\n",
              "  515526.34375,\n",
              "  494701.71875,\n",
              "  515286.375,\n",
              "  490508.53125,\n",
              "  502822.5,\n",
              "  498662.53125,\n",
              "  494587.34375,\n",
              "  502549.34375,\n",
              "  510854.65625,\n",
              "  510817.125,\n",
              "  494057.125,\n",
              "  502239.90625,\n",
              "  522875.65625,\n",
              "  498127.5,\n",
              "  509756.09375,\n",
              "  497930.78125,\n",
              "  497854.25,\n",
              "  506107.34375,\n",
              "  489409.75,\n",
              "  485257.28125,\n",
              "  513985.59375,\n",
              "  489181.5,\n",
              "  530475.5,\n",
              "  497311.75,\n",
              "  488913.21875,\n",
              "  517794.96875,\n",
              "  509354.71875,\n",
              "  505281.84375,\n",
              "  509237.65625,\n",
              "  488573.40625,\n",
              "  529742.25,\n",
              "  500768.75,\n",
              "  500651.375,\n",
              "  504853.40625,\n",
              "  492432.25,\n",
              "  500419.875,\n",
              "  500302.84375,\n",
              "  529094.125,\n",
              "  483821.25,\n",
              "  496079.09375,\n",
              "  496002.59375,\n",
              "  516517.34375,\n",
              "  491818.84375,\n",
              "  491742.53125,\n",
              "  516279.46875,\n",
              "  516159.65625,\n",
              "  499530.15625,\n",
              "  503679.625,\n",
              "  503444.28125,\n",
              "  507629.15625,\n",
              "  503523.75,\n",
              "  503171.90625,\n",
              "  482767.0,\n",
              "  515638.65625,\n",
              "  494965.5,\n",
              "  511183.03125,\n",
              "  507041.15625,\n",
              "  498682.21875,\n",
              "  498644.59375,\n",
              "  515007.84375,\n",
              "  498488.90625,\n",
              "  510712.59375,\n",
              "  490063.0,\n",
              "  514727.59375,\n",
              "  514649.21875,\n",
              "  514531.375,\n",
              "  502274.5,\n",
              "  506141.84375,\n",
              "  489646.75,\n",
              "  502040.46875,\n",
              "  489454.59375,\n",
              "  518151.75,\n",
              "  497566.5,\n",
              "  505749.0,\n",
              "  505633.40625,\n",
              "  493276.40625,\n",
              "  505438.125,\n",
              "  504736.34375,\n",
              "  497068.28125,\n",
              "  475880.71875,\n",
              "  496952.53125,\n",
              "  509143.34375,\n",
              "  505085.15625,\n",
              "  484458.125,\n",
              "  525289.4375,\n",
              "  492553.125,\n",
              "  504735.875,\n",
              "  512862.40625,\n",
              "  500564.625,\n",
              "  512742.84375,\n",
              "  471716.03125,\n",
              "  500295.375,\n",
              "  504269.53125,\n",
              "  504190.78125,\n",
              "  487815.15625,\n",
              "  504074.34375,\n",
              "  499982.65625,\n",
              "  491471.75,\n",
              "  495573.78125,\n",
              "  491484.59375,\n",
              "  515848.15625,\n",
              "  483106.34375,\n",
              "  503491.84375,\n",
              "  482993.65625,\n",
              "  499163.65625,\n",
              "  503220.21875,\n",
              "  503178.59375,\n",
              "  503100.40625,\n",
              "  478607.03125,\n",
              "  498777.53125,\n",
              "  493992.65625,\n",
              "  490568.71875,\n",
              "  494537.84375,\n",
              "  510770.0,\n",
              "  490338.71875,\n",
              "  477386.78125,\n",
              "  498355.21875,\n",
              "  494116.90625,\n",
              "  514417.28125,\n",
              "  493963.875,\n",
              "  498045.0,\n",
              "  506052.90625,\n",
              "  477373.96875,\n",
              "  501818.5,\n",
              "  505782.375,\n",
              "  472488.25,\n",
              "  473075.75,\n",
              "  485345.71875,\n",
              "  497465.09375,\n",
              "  497386.59375,\n",
              "  485118.53125,\n",
              "  501200.125,\n",
              "  501122.28125,\n",
              "  497006.75,\n",
              "  476668.375,\n",
              "  492745.65625,\n",
              "  495975.375,\n",
              "  488555.09375,\n",
              "  520978.40625,\n",
              "  476294.40625,\n",
              "  496536.625,\n",
              "  508598.34375,\n",
              "  488175.53125,\n",
              "  488170.15625,\n",
              "  475890.0,\n",
              "  500151.65625,\n",
              "  491906.5,\n",
              "  483697.625,\n",
              "  479624.875,\n",
              "  479550.53125,\n",
              "  483440.25,\n",
              "  479403.125,\n",
              "  487454.71875,\n",
              "  487380.0,\n",
              "  499457.40625,\n",
              "  503376.15625,\n",
              "  462883.65625,\n",
              "  491104.65625,\n",
              "  519412.78125,\n",
              "  507254.09375,\n",
              "  494961.53125,\n",
              "  490765.34375],\n",
              " 'val_mse': [500053.40625,\n",
              "  521621.40625,\n",
              "  547272.375,\n",
              "  538520.4375,\n",
              "  538382.4375,\n",
              "  547021.9375,\n",
              "  508306.46875,\n",
              "  550995.0625,\n",
              "  542301.6875,\n",
              "  520485.125,\n",
              "  537996.4375,\n",
              "  520378.59375,\n",
              "  516159.15625,\n",
              "  546458.75,\n",
              "  524655.25,\n",
              "  528765.9375,\n",
              "  528740.6875,\n",
              "  528604.3125,\n",
              "  537227.5625,\n",
              "  520223.125,\n",
              "  541254.6875,\n",
              "  536980.8125,\n",
              "  511174.03125,\n",
              "  523983.25,\n",
              "  515151.46875,\n",
              "  545402.375,\n",
              "  545317.5625,\n",
              "  514964.875,\n",
              "  523577.96875,\n",
              "  549252.9375,\n",
              "  536346.5625,\n",
              "  527522.1875,\n",
              "  527387.8125,\n",
              "  519042.09375,\n",
              "  540093.9375,\n",
              "  522956.625,\n",
              "  539137.0,\n",
              "  531164.4375,\n",
              "  518218.25,\n",
              "  534760.8125,\n",
              "  531284.25,\n",
              "  539515.25,\n",
              "  509169.96875,\n",
              "  539351.0625,\n",
              "  530541.5,\n",
              "  535057.3125,\n",
              "  526199.625,\n",
              "  543562.6875,\n",
              "  534811.0,\n",
              "  526062.4375,\n",
              "  546704.6875,\n",
              "  534511.5625,\n",
              "  521692.71875,\n",
              "  530275.9375,\n",
              "  508641.59375,\n",
              "  530112.3125,\n",
              "  529924.4375,\n",
              "  521240.53125,\n",
              "  516575.59375,\n",
              "  533856.1875,\n",
              "  542480.3125,\n",
              "  529621.3125,\n",
              "  533663.3125,\n",
              "  524931.9375,\n",
              "  520728.78125,\n",
              "  533364.0625,\n",
              "  536775.1875,\n",
              "  541898.0625,\n",
              "  520302.84375,\n",
              "  520327.0,\n",
              "  554463.0,\n",
              "  532823.4375,\n",
              "  524102.71875,\n",
              "  528643.5625,\n",
              "  511238.90625,\n",
              "  541130.5625,\n",
              "  523884.375,\n",
              "  523699.71875,\n",
              "  523671.59375,\n",
              "  514961.34375,\n",
              "  506647.65625,\n",
              "  506125.875,\n",
              "  531926.3125,\n",
              "  540572.1875,\n",
              "  540540.3125,\n",
              "  510371.5,\n",
              "  531074.0625,\n",
              "  510162.34375,\n",
              "  531539.3125,\n",
              "  531459.125,\n",
              "  518594.875,\n",
              "  531295.6875,\n",
              "  522548.0,\n",
              "  543859.4375,\n",
              "  522388.75,\n",
              "  522256.40625,\n",
              "  522226.78125,\n",
              "  530805.875,\n",
              "  539281.0,\n",
              "  552016.3125,\n",
              "  543226.9375,\n",
              "  504948.40625,\n",
              "  517583.78125,\n",
              "  517554.59375,\n",
              "  512986.25,\n",
              "  521454.65625,\n",
              "  512098.84375,\n",
              "  499993.90625,\n",
              "  508512.59375,\n",
              "  491619.34375,\n",
              "  516947.84375,\n",
              "  529615.25,\n",
              "  525424.8125,\n",
              "  512227.65625,\n",
              "  499873.5,\n",
              "  503853.84375,\n",
              "  516467.0,\n",
              "  533234.5625,\n",
              "  529095.4375,\n",
              "  516276.875,\n",
              "  528883.125,\n",
              "  524695.5,\n",
              "  541453.0,\n",
              "  520063.03125,\n",
              "  528608.6875,\n",
              "  515434.84375,\n",
              "  519872.96875,\n",
              "  541040.4375,\n",
              "  528233.4375,\n",
              "  532256.9375,\n",
              "  524065.90625,\n",
              "  523528.0,\n",
              "  531333.8125,\n",
              "  515212.40625,\n",
              "  493808.09375,\n",
              "  540377.4375,\n",
              "  540247.4375,\n",
              "  523047.46875,\n",
              "  502107.84375,\n",
              "  527343.8125,\n",
              "  497802.25,\n",
              "  518676.46875,\n",
              "  526372.0625,\n",
              "  548269.3125,\n",
              "  526260.5625,\n",
              "  518262.0,\n",
              "  539522.125,\n",
              "  501010.46875,\n",
              "  500255.65625,\n",
              "  517362.84375,\n",
              "  513418.34375,\n",
              "  539062.0,\n",
              "  534833.875,\n",
              "  530309.6875,\n",
              "  521987.84375,\n",
              "  534635.5625,\n",
              "  492124.5,\n",
              "  513259.75,\n",
              "  496064.375,\n",
              "  525725.0625,\n",
              "  513068.0,\n",
              "  534189.6875,\n",
              "  512910.28125,\n",
              "  525499.9375,\n",
              "  520891.875,\n",
              "  516717.25,\n",
              "  525162.0,\n",
              "  525127.9375,\n",
              "  520906.59375,\n",
              "  533485.0,\n",
              "  516318.34375,\n",
              "  516287.34375,\n",
              "  520297.75,\n",
              "  524642.6875,\n",
              "  532979.75,\n",
              "  520297.40625,\n",
              "  524354.75,\n",
              "  536919.3125,\n",
              "  515683.40625,\n",
              "  524112.15625,\n",
              "  519659.75,\n",
              "  519955.46875,\n",
              "  528004.6875,\n",
              "  544927.5625,\n",
              "  511073.59375,\n",
              "  515127.75,\n",
              "  523594.25,\n",
              "  523466.34375,\n",
              "  510757.59375,\n",
              "  518944.40625,\n",
              "  523179.46875,\n",
              "  523193.03125,\n",
              "  518980.40625,\n",
              "  544096.1875,\n",
              "  505253.53125,\n",
              "  510299.375,\n",
              "  535361.1875,\n",
              "  514179.125,\n",
              "  509969.96875,\n",
              "  509936.84375,\n",
              "  513989.71875,\n",
              "  522388.53125,\n",
              "  522262.34375,\n",
              "  526263.1875,\n",
              "  517973.875,\n",
              "  513547.15625,\n",
              "  521987.53125,\n",
              "  530332.0625,\n",
              "  525861.3125,\n",
              "  521746.25,\n",
              "  509070.875,\n",
              "  496445.5,\n",
              "  512993.21875,\n",
              "  533971.625,\n",
              "  500246.03125,\n",
              "  500214.03125,\n",
              "  512003.84375,\n",
              "  521059.5,\n",
              "  495860.34375,\n",
              "  511814.15625,\n",
              "  524941.6875,\n",
              "  529239.6875,\n",
              "  524734.4375,\n",
              "  495473.25,\n",
              "  516374.84375,\n",
              "  528822.3125,\n",
              "  503738.625,\n",
              "  516183.40625,\n",
              "  541236.6875,\n",
              "  499087.34375,\n",
              "  524045.28125,\n",
              "  511449.34375,\n",
              "  511372.375,\n",
              "  511294.21875,\n",
              "  532302.9375,\n",
              "  511181.15625,\n",
              "  502620.5,\n",
              "  527987.0625,\n",
              "  523496.25,\n",
              "  506122.25,\n",
              "  506671.5,\n",
              "  502567.40625,\n",
              "  498083.59375,\n",
              "  518933.375,\n",
              "  502376.59375,\n",
              "  527199.6875,\n",
              "  531321.625,\n",
              "  518747.71875,\n",
              "  502063.09375,\n",
              "  527054.1875,\n",
              "  526884.1875,\n",
              "  501457.90625,\n",
              "  505147.15625,\n",
              "  505695.375,\n",
              "  514122.71875,\n",
              "  497126.375,\n",
              "  484527.09375,\n",
              "  526270.75,\n",
              "  500915.5,\n",
              "  513635.03125,\n",
              "  496739.65625,\n",
              "  512481.59375,\n",
              "  525951.3125,\n",
              "  488379.59375,\n",
              "  504834.34375,\n",
              "  513157.53125,\n",
              "  500659.28125,\n",
              "  491467.875,\n",
              "  508584.21875,\n",
              "  537758.125,\n",
              "  504367.09375,\n",
              "  500270.34375,\n",
              "  516708.90625,\n",
              "  504133.0,\n",
              "  504011.53125,\n",
              "  495587.78125,\n",
              "  516391.375,\n",
              "  503821.625,\n",
              "  520248.125,\n",
              "  499563.96875,\n",
              "  507646.09375,\n",
              "  520009.34375,\n",
              "  507447.53125,\n",
              "  507456.25,\n",
              "  528149.0625,\n",
              "  511620.375,\n",
              "  523973.375,\n",
              "  519574.75,\n",
              "  507023.03125,\n",
              "  536245.5625,\n",
              "  510965.625,\n",
              "  498419.59375,\n",
              "  515121.15625,\n",
              "  494166.90625,\n",
              "  490077.96875,\n",
              "  514883.34375,\n",
              "  506397.0,\n",
              "  518735.875,\n",
              "  501933.75,\n",
              "  518660.21875,\n",
              "  510138.09375,\n",
              "  514406.09375,\n",
              "  514328.40625,\n",
              "  501798.5,\n",
              "  514128.15625,\n",
              "  505737.5,\n",
              "  517978.75,\n",
              "  497149.90625,\n",
              "  534644.8125,\n",
              "  504758.84375,\n",
              "  492909.59375,\n",
              "  497170.84375,\n",
              "  513579.59375,\n",
              "  513416.09375,\n",
              "  525810.8125,\n",
              "  525687.1875,\n",
              "  513219.78125,\n",
              "  500755.875,\n",
              "  500636.65625,\n",
              "  517029.90625,\n",
              "  512861.0,\n",
              "  512824.28125,\n",
              "  496237.46875,\n",
              "  516713.59375,\n",
              "  524880.0625,\n",
              "  500094.46875,\n",
              "  504062.25,\n",
              "  487570.21875,\n",
              "  512189.46875,\n",
              "  503119.03125,\n",
              "  512156.125,\n",
              "  503633.09375,\n",
              "  516000.625,\n",
              "  478755.25,\n",
              "  511717.65625,\n",
              "  507407.28125,\n",
              "  503286.90625,\n",
              "  486769.75,\n",
              "  515526.34375,\n",
              "  494701.71875,\n",
              "  515286.375,\n",
              "  490508.53125,\n",
              "  502822.5,\n",
              "  498662.53125,\n",
              "  494587.34375,\n",
              "  502549.34375,\n",
              "  510854.65625,\n",
              "  510817.125,\n",
              "  494057.125,\n",
              "  502239.90625,\n",
              "  522875.65625,\n",
              "  498127.5,\n",
              "  509756.09375,\n",
              "  497930.78125,\n",
              "  497854.25,\n",
              "  506107.34375,\n",
              "  489409.75,\n",
              "  485257.28125,\n",
              "  513985.59375,\n",
              "  489181.5,\n",
              "  530475.5,\n",
              "  497311.75,\n",
              "  488913.21875,\n",
              "  517794.96875,\n",
              "  509354.71875,\n",
              "  505281.84375,\n",
              "  509237.65625,\n",
              "  488573.40625,\n",
              "  529742.25,\n",
              "  500768.75,\n",
              "  500651.375,\n",
              "  504853.40625,\n",
              "  492432.25,\n",
              "  500419.875,\n",
              "  500302.84375,\n",
              "  529094.125,\n",
              "  483821.25,\n",
              "  496079.09375,\n",
              "  496002.59375,\n",
              "  516517.34375,\n",
              "  491818.84375,\n",
              "  491742.53125,\n",
              "  516279.46875,\n",
              "  516159.65625,\n",
              "  499530.15625,\n",
              "  503679.625,\n",
              "  503444.28125,\n",
              "  507629.15625,\n",
              "  503523.75,\n",
              "  503171.90625,\n",
              "  482767.0,\n",
              "  515638.65625,\n",
              "  494965.5,\n",
              "  511183.03125,\n",
              "  507041.15625,\n",
              "  498682.21875,\n",
              "  498644.59375,\n",
              "  515007.84375,\n",
              "  498488.90625,\n",
              "  510712.59375,\n",
              "  490063.0,\n",
              "  514727.59375,\n",
              "  514649.21875,\n",
              "  514531.375,\n",
              "  502274.5,\n",
              "  506141.84375,\n",
              "  489646.75,\n",
              "  502040.46875,\n",
              "  489454.59375,\n",
              "  518151.75,\n",
              "  497566.5,\n",
              "  505749.0,\n",
              "  505633.40625,\n",
              "  493276.40625,\n",
              "  505438.125,\n",
              "  504736.34375,\n",
              "  497068.28125,\n",
              "  475880.71875,\n",
              "  496952.53125,\n",
              "  509143.34375,\n",
              "  505085.15625,\n",
              "  484458.125,\n",
              "  525289.4375,\n",
              "  492553.125,\n",
              "  504735.875,\n",
              "  512862.40625,\n",
              "  500564.625,\n",
              "  512742.84375,\n",
              "  471716.03125,\n",
              "  500295.375,\n",
              "  504269.53125,\n",
              "  504190.78125,\n",
              "  487815.15625,\n",
              "  504074.34375,\n",
              "  499982.65625,\n",
              "  491471.75,\n",
              "  495573.78125,\n",
              "  491484.59375,\n",
              "  515848.15625,\n",
              "  483106.34375,\n",
              "  503491.84375,\n",
              "  482993.65625,\n",
              "  499163.65625,\n",
              "  503220.21875,\n",
              "  503178.59375,\n",
              "  503100.40625,\n",
              "  478607.03125,\n",
              "  498777.53125,\n",
              "  493992.65625,\n",
              "  490568.71875,\n",
              "  494537.84375,\n",
              "  510770.0,\n",
              "  490338.71875,\n",
              "  477386.78125,\n",
              "  498355.21875,\n",
              "  494116.90625,\n",
              "  514417.28125,\n",
              "  493963.875,\n",
              "  498045.0,\n",
              "  506052.90625,\n",
              "  477373.96875,\n",
              "  501818.5,\n",
              "  505782.375,\n",
              "  472488.25,\n",
              "  473075.75,\n",
              "  485345.71875,\n",
              "  497465.09375,\n",
              "  497386.59375,\n",
              "  485118.53125,\n",
              "  501200.125,\n",
              "  501122.28125,\n",
              "  497006.75,\n",
              "  476668.375,\n",
              "  492745.65625,\n",
              "  495975.375,\n",
              "  488555.09375,\n",
              "  520978.40625,\n",
              "  476294.40625,\n",
              "  496536.625,\n",
              "  508598.34375,\n",
              "  488175.53125,\n",
              "  488170.15625,\n",
              "  475890.0,\n",
              "  500151.65625,\n",
              "  491906.5,\n",
              "  483697.625,\n",
              "  479624.875,\n",
              "  479550.53125,\n",
              "  483440.25,\n",
              "  479403.125,\n",
              "  487454.71875,\n",
              "  487380.0,\n",
              "  499457.40625,\n",
              "  503376.15625,\n",
              "  462883.65625,\n",
              "  491104.65625,\n",
              "  519412.78125,\n",
              "  507254.09375,\n",
              "  494961.53125,\n",
              "  490765.34375]}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "mse = history.history['mse']\n",
        "val_mse = history.history['val_mse']\n",
        "\n",
        "epochs_x = range(len(loss))\n",
        "\n",
        "plt.plot(epochs_x, mse, 'co', label='Training MAE')\n",
        "plt.plot(epochs_x, val_mse, 'k', label='Validation MAE')\n",
        "plt.title('Training and Mean squared error')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "outputId": "54e5b0e2-5121-48ef-e03c-d699b2aa16c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE model** "
      ],
      "metadata": {
        "id": "okE8lIwPkoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "# model.save('./content/drive/My Drive/new/2Class_regression_freeze_500.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# files.download(\"./content/drive/My Drive/new/2Class_regression_freeze_500.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}