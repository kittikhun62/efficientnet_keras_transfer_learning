{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNTg07LncZ1rlDozTZIgRCs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittikhun62/efficientnet_keras_transfer_learning/blob/master/UNfreeze2Class_h5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import from drive"
      ],
      "metadata": {
        "id": "bB2gfuYWczY-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpXyZ9k5YIJu",
        "outputId": "900f72ad-d27b-4dbb-8e2e-4bbeae77add1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'/content/drive/My Drive/data - 2 class.csv')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "7mqo2OLAc2pG",
        "outputId": "52957fd8-d4fa-45a4-d8bd-4bd6d64e0e70"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      No                      Name_file  \\\n",
              "0      1                        pore-sb   \n",
              "1      2                        pore-sb   \n",
              "2      3                        pore-sb   \n",
              "3      4                        pore-sb   \n",
              "4      5                        pore-sb   \n",
              "..   ...                            ...   \n",
              "795  796  1-s2.0-S2095268622000210-main   \n",
              "796  797  1-s2.0-S2095268622000210-main   \n",
              "797  798  1-s2.0-S2095268622000210-main   \n",
              "798  799  1-s2.0-S2095268622000210-main   \n",
              "799  800  1-s2.0-S2095268622000210-main   \n",
              "\n",
              "                                            Name_Paper  \\\n",
              "0    Preparation and electrochemical behaviour of b...   \n",
              "1    Preparation and electrochemical behaviour of b...   \n",
              "2    Preparation and electrochemical behaviour of b...   \n",
              "3    Preparation and electrochemical behaviour of b...   \n",
              "4    Preparation and electrochemical behaviour of b...   \n",
              "..                                                 ...   \n",
              "795  Integration of preparation of K, Na-embedded a...   \n",
              "796  Integration of preparation of K, Na-embedded a...   \n",
              "797  Integration of preparation of K, Na-embedded a...   \n",
              "798  Integration of preparation of K, Na-embedded a...   \n",
              "799  Integration of preparation of K, Na-embedded a...   \n",
              "\n",
              "                                               journal  \\\n",
              "0                                  Korean J. Chem. Eng   \n",
              "1                                  Korean J. Chem. Eng   \n",
              "2                                  Korean J. Chem. Eng   \n",
              "3                                  Korean J. Chem. Eng   \n",
              "4                                  Korean J. Chem. Eng   \n",
              "..                                                 ...   \n",
              "795  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "796  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "797  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "798  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "799  Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...   \n",
              "\n",
              "                                          path_Picture  detail  Class     BET  \\\n",
              "0    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom1  0-800  135.06   \n",
              "1    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom2  0-800  135.06   \n",
              "2    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom3  0-800  135.06   \n",
              "3    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom4  0-800  135.06   \n",
              "4    /content/drive/My Drive/new train/pore-sb/PCC(...   zoom5  0-800  135.06   \n",
              "..                                                 ...     ...    ...     ...   \n",
              "795  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom21  0-800  301.70   \n",
              "796  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom22  0-800  301.70   \n",
              "797  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom23  0-800  301.70   \n",
              "798  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom24  0-800  301.70   \n",
              "799  /content/drive/My Drive/new train/1-s2.0-S2095...  zoom25  0-800  301.70   \n",
              "\n",
              "     Size(mico)  \n",
              "0            10  \n",
              "1            10  \n",
              "2            10  \n",
              "3            10  \n",
              "4            10  \n",
              "..          ...  \n",
              "795          10  \n",
              "796          10  \n",
              "797          10  \n",
              "798          10  \n",
              "799          10  \n",
              "\n",
              "[800 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-90f20847-e47a-48de-b326-a25ad1d6125a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>Name_file</th>\n",
              "      <th>Name_Paper</th>\n",
              "      <th>journal</th>\n",
              "      <th>path_Picture</th>\n",
              "      <th>detail</th>\n",
              "      <th>Class</th>\n",
              "      <th>BET</th>\n",
              "      <th>Size(mico)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom1</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom2</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom3</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom4</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>pore-sb</td>\n",
              "      <td>Preparation and electrochemical behaviour of b...</td>\n",
              "      <td>Korean J. Chem. Eng</td>\n",
              "      <td>/content/drive/My Drive/new train/pore-sb/PCC(...</td>\n",
              "      <td>zoom5</td>\n",
              "      <td>0-800</td>\n",
              "      <td>135.06</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>796</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom21</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>797</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom22</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>798</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom23</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>799</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom24</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>800</td>\n",
              "      <td>1-s2.0-S2095268622000210-main</td>\n",
              "      <td>Integration of preparation of K, Na-embedded a...</td>\n",
              "      <td>Dingzheng Wang,Deqing Zhu,Jian Pan, Zhengqi Gu...</td>\n",
              "      <td>/content/drive/My Drive/new train/1-s2.0-S2095...</td>\n",
              "      <td>zoom25</td>\n",
              "      <td>0-800</td>\n",
              "      <td>301.70</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90f20847-e47a-48de-b326-a25ad1d6125a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-90f20847-e47a-48de-b326-a25ad1d6125a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-90f20847-e47a-48de-b326-a25ad1d6125a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone efficientnet repo"
      ],
      "metadata": {
        "id": "DSSZIFGhUyQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดึงข้อมูลใน Github มาใช้\n",
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        " !git clone https://github.com/Wanita-8943/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "metadata": {
        "id": "nFvKcuBDTj1N",
        "outputId": "a9b4fa4c-fc3e-4796-f4bd-225014e389a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'efficientnet_keras_transfer_learning'...\n",
            "remote: Enumerating objects: 837, done.\u001b[K\n",
            "remote: Counting objects: 100% (359/359), done.\u001b[K\n",
            "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
            "remote: Total 837 (delta 255), reused 328 (delta 235), pack-reused 478\u001b[K\n",
            "Receiving objects: 100% (837/837), 13.82 MiB | 19.54 MiB/s, done.\n",
            "Resolving deltas: 100% (495/495), done.\n",
            "/content/efficientnet_keras_transfer_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# การเเบ่งข้อมูล train/validation/test sets"
      ],
      "metadata": {
        "id": "JDJCDzEDWnVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/My Drive/new project'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Directories for our training,\n",
        "# validation and test splits\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.makedirs(test_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "R7L0rJNRU2MY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_1_dir = os.path.join(train_dir, '0-800')\n",
        "os.makedirs(train_1_dir, exist_ok=True)\n",
        "\n",
        "train_2_dir = os.path.join(train_dir, '801-3200')\n",
        "os.makedirs(train_2_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "validation_1_dir = os.path.join(validation_dir, '0-800')\n",
        "os.makedirs(validation_1_dir, exist_ok=True)\n",
        "\n",
        "validation_2_dir = os.path.join(validation_dir, '801-3200')\n",
        "os.makedirs(validation_2_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "test_1_dir = os.path.join(test_dir, '0-800')\n",
        "os.makedirs(test_1_dir, exist_ok=True)\n",
        "\n",
        "test_2_dir = os.path.join(test_dir, '801-3200')\n",
        "os.makedirs(test_2_dir, exist_ok=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "jtyFMXrWU2KD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = df[df['No'].between(599,699)]\n",
        "train = df[df['No'].between(1,598)]\n",
        "test = df[df['No'].between(700,800)] \n",
        "\n",
        "#Path Train\n",
        "T1_train = train[train['Class']=='0-800']\n",
        "T1_path_train = T1_train['path_Picture'].tolist() \n",
        "T2_train = train[train['Class']=='801-3200']\n",
        "T2_path_train = T2_train['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "#Path Validation\n",
        "T1_val = val[val['Class']=='0-800']\n",
        "T1_path_val = T1_val['path_Picture'].tolist() \n",
        "T2_val = val[val['Class']=='801-3200']\n",
        "T2_path_val = T2_val['path_Picture'].tolist() \n",
        "\n",
        "\n",
        "\n",
        "#Path Test\n",
        "T1_test = test[test['Class']=='0-800']\n",
        "T1_path_test = T1_test['path_Picture'].tolist() \n",
        "T2_test = test[test['Class']=='801-3200']\n",
        "T2_path_test = T2_test['path_Picture'].tolist() \n"
      ],
      "metadata": {
        "id": "mlmd_kyhW2LZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "ZkfPduNQW43l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_train\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(train_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "9jMgUluKU2Hp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "Mj3sViKJaLSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_test\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(validation_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "    \n"
      ],
      "metadata": {
        "id": "WvK0Y2FIYat1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation"
      ],
      "metadata": {
        "id": "rc4HwwbPaD6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = T1_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_1_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n",
        "\n",
        "fnames = T2_path_val\n",
        "for fname in fnames:\n",
        "    dst = os.path.join(test_2_dir, os.path.basename(fname))\n",
        "    shutil.copyfile(fname, dst)\n"
      ],
      "metadata": {
        "id": "XxtmCbyUYarp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('total training 1 images:', len(os.listdir(train_1_dir))) \n",
        "print('total training 2 images:', len(os.listdir(train_2_dir)),'\\n')\n",
        "\n",
        "print('total validation 1 images:', len(os.listdir(validation_1_dir)))\n",
        "print('total validation 2 images:', len(os.listdir(validation_2_dir)),'\\n')\n",
        "\n",
        "print('total test 1 images:', len(os.listdir(test_1_dir)))\n",
        "print('total test 2 images:', len(os.listdir(test_2_dir)),'\\n')\n"
      ],
      "metadata": {
        "id": "Tvk53f-WYapl",
        "outputId": "3122bffd-c0e1-47f6-c9f1-740b2536e2b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training 1 images: 301\n",
            "total training 2 images: 297 \n",
            "\n",
            "total validation 1 images: 50\n",
            "total validation 2 images: 51 \n",
            "\n",
            "total test 1 images: 50\n",
            "total test 2 images: 51 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports library"
      ],
      "metadata": {
        "id": "VkYa-4LeTbir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "myAsBcVhTW7j"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyper parameter\n"
      ],
      "metadata": {
        "id": "T529UM_tVV-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "width = 150\n",
        "height = 150\n",
        "\n",
        "epochs = 1000 #จำนวนรอบในการ Train\n",
        "\n",
        "NUM_TRAIN = 598  # จำนวนภาพ Train\n",
        "NUM_TEST = 101 #จำนวนภาพ Test\n",
        "\n",
        "dropout_rate = 0.3\n",
        "input_shape = (height, width, 3) #ขนาด image enter"
      ],
      "metadata": {
        "id": "zXpmkJ2GVZ1l"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import efficientnet and load the conv base model"
      ],
      "metadata": {
        "id": "j3EgSOiSZHaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "metadata": {
        "id": "iI3DQ18HU2TF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pretrained conv base model\n",
        "# โหลดโมเดล มาโดยตัด output ของโมเดลออก เเต่ยังใช้ input อันเดิม\n",
        "# เเละโหลด weight ของโมเดล มาด้วยที่ชื่อว่า imagenet\n",
        "#EfficientNet สร้างขึ้นสำหรับการจำแนกประเภท ImageNet ประกอบด้วยป้ายกำกับคลาส 1,000 รายการ เรามีเพียง 2 เลเยอร์เท่านั้น ซึ่งหมายความว่าเลเยอร์สองสามเลเยอร์สุดท้ายสำหรับการจำแนกไม่มีประโยชน์สำหรับเรา สามารถยกเว้นได้ขณะโหลดโมเดลโดยระบุอาร์กิวเมนต์ include_top เป็น False และนำไปใช้กับโมเดล ImageNet อื่นๆ ที่มีอยู่ในแอปพลิเคชัน Keras เช่นกัน\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "UmBDAgF_VAjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b4e8039-4733-4205-ebdf-476cdafe5c6c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\n",
            "16717576/16717576 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary() #ดู Summary"
      ],
      "metadata": {
        "id": "rRXoFFzfU2Oi",
        "outputId": "b520e3b8-1dd2-4cb5-c95b-fc98e0184115",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet-b0\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 75, 75, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 75, 75, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " swish (Swish)                  (None, 75, 75, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 75, 75, 32)  288         ['swish[0][0]']                  \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 75, 75, 32)  128         ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_1 (Swish)                (None, 75, 75, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 1, 32)     0           ['swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 1, 1, 8)      264         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " swish_2 (Swish)                (None, 1, 1, 8)      0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 1, 1, 32)     288         ['swish_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1, 1, 32)     0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 75, 75, 32)   0           ['activation[0][0]',             \n",
            "                                                                  'swish_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 75, 75, 16)   512         ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 75, 75, 16)  64          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 75, 75, 96)   1536        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 75, 75, 96)  384         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_3 (Swish)                (None, 75, 75, 96)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 38, 38, 96)  864         ['swish_3[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 38, 96)  384         ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_4 (Swish)                (None, 38, 38, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 1, 96)     0           ['swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 1, 1, 4)      388         ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " swish_5 (Swish)                (None, 1, 1, 4)      0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 1, 1, 96)     480         ['swish_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 96)     0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 38, 38, 96)   0           ['activation_1[0][0]',           \n",
            "                                                                  'swish_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 38, 38, 24)   2304        ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 38, 38, 144)  3456        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_6 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 38, 38, 144)  1296       ['swish_6[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 38, 38, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_7 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1, 144)    0           ['swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 1, 1, 6)      870         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " swish_8 (Swish)                (None, 1, 1, 6)      0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_8[0][0]']                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 1, 1, 144)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 38, 38, 144)  0           ['activation_2[0][0]',           \n",
            "                                                                  'swish_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 38, 38, 24)   3456        ['multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 38, 38, 24)  96          ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " drop_connect (DropConnect)     (None, 38, 38, 24)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 38, 38, 24)   0           ['drop_connect[0][0]',           \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 38, 38, 144)  3456        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 38, 38, 144)  576        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " swish_9 (Swish)                (None, 38, 38, 144)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " depthwise_conv2d_3 (DepthwiseC  (None, 19, 19, 144)  3600       ['swish_9[0][0]']                \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 19, 19, 144)  576        ['depthwise_conv2d_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_10 (Swish)               (None, 19, 19, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 1, 1, 144)    0           ['swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 6)      870         ['lambda_3[0][0]']               \n",
            "                                                                                                  \n",
            " swish_11 (Swish)               (None, 1, 1, 6)      0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 1, 1, 144)    1008        ['swish_11[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 1, 1, 144)    0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 19, 19, 144)  0           ['activation_3[0][0]',           \n",
            "                                                                  'swish_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 19, 19, 40)   5760        ['multiply_3[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 19, 19, 40)  160         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 19, 19, 240)  9600        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 19, 19, 240)  960        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_12 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_4 (DepthwiseC  (None, 19, 19, 240)  6000       ['swish_12[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 19, 19, 240)  960        ['depthwise_conv2d_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_13 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 1, 1, 240)    0           ['swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            " swish_14 (Swish)               (None, 1, 1, 10)     0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_14[0][0]']               \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 1, 240)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)          (None, 19, 19, 240)  0           ['activation_4[0][0]',           \n",
            "                                                                  'swish_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 19, 19, 40)   9600        ['multiply_4[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 19, 19, 40)  160         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_1 (DropConnect)   (None, 19, 19, 40)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 19, 19, 40)   0           ['drop_connect_1[0][0]',         \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 19, 19, 240)  9600        ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 19, 19, 240)  960        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_15 (Swish)               (None, 19, 19, 240)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_5 (DepthwiseC  (None, 10, 10, 240)  2160       ['swish_15[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 10, 10, 240)  960        ['depthwise_conv2d_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_16 (Swish)               (None, 10, 10, 240)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)              (None, 1, 1, 240)    0           ['swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 10)     2410        ['lambda_5[0][0]']               \n",
            "                                                                                                  \n",
            " swish_17 (Swish)               (None, 1, 1, 10)     0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 1, 1, 240)    2640        ['swish_17[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 1, 240)    0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)          (None, 10, 10, 240)  0           ['activation_5[0][0]',           \n",
            "                                                                  'swish_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 10, 10, 80)   19200       ['multiply_5[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 10, 10, 80)  320         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 10, 10, 480)  38400       ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_18 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_6 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_18[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_19 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_6 (Lambda)              (None, 1, 1, 480)    0           ['swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_6[0][0]']               \n",
            "                                                                                                  \n",
            " swish_20 (Swish)               (None, 1, 1, 20)     0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_20[0][0]']               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 1, 1, 480)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 10, 10, 480)  0           ['activation_6[0][0]',           \n",
            "                                                                  'swish_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 10, 10, 80)  320         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_2 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_2[0][0]',         \n",
            "                                                                  'batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 10, 10, 480)  38400       ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_21 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_7 (DepthwiseC  (None, 10, 10, 480)  4320       ['swish_21[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_22 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_7 (Lambda)              (None, 1, 1, 480)    0           ['swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_7[0][0]']               \n",
            "                                                                                                  \n",
            " swish_23 (Swish)               (None, 1, 1, 20)     0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_23[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1, 1, 480)    0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 10, 10, 480)  0           ['activation_7[0][0]',           \n",
            "                                                                  'swish_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 10, 10, 80)   38400       ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 10, 10, 80)  320         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_3 (DropConnect)   (None, 10, 10, 80)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 10, 10, 80)   0           ['drop_connect_3[0][0]',         \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 10, 10, 480)  38400       ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 10, 10, 480)  1920       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_24 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_8 (DepthwiseC  (None, 10, 10, 480)  12000      ['swish_24[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 10, 10, 480)  1920       ['depthwise_conv2d_8[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_25 (Swish)               (None, 10, 10, 480)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_8 (Lambda)              (None, 1, 1, 480)    0           ['swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 1, 1, 20)     9620        ['lambda_8[0][0]']               \n",
            "                                                                                                  \n",
            " swish_26 (Swish)               (None, 1, 1, 20)     0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 1, 1, 480)    10080       ['swish_26[0][0]']               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1, 1, 480)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)          (None, 10, 10, 480)  0           ['activation_8[0][0]',           \n",
            "                                                                  'swish_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 10, 10, 112)  53760       ['multiply_8[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 10, 10, 112)  448        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 10, 10, 672)  75264       ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_27 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_9 (DepthwiseC  (None, 10, 10, 672)  16800      ['swish_27[0][0]']               \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_9[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_28 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 1, 1, 672)    0           ['swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            " swish_29 (Swish)               (None, 1, 1, 28)     0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_29[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1, 1, 672)    0           ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 10, 10, 672)  0           ['activation_9[0][0]',           \n",
            "                                                                  'swish_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 10, 10, 112)  448        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_4 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_4[0][0]',         \n",
            "                                                                  'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 10, 10, 672)  75264       ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_30 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_10 (Depthwise  (None, 10, 10, 672)  16800      ['swish_30[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 10, 10, 672)  2688       ['depthwise_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_31 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_10 (Lambda)             (None, 1, 1, 672)    0           ['swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_10[0][0]']              \n",
            "                                                                                                  \n",
            " swish_32 (Swish)               (None, 1, 1, 28)     0           ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_32[0][0]']               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1, 1, 672)    0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 10, 10, 672)  0           ['activation_10[0][0]',          \n",
            "                                                                  'swish_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 10, 10, 112)  75264       ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 10, 10, 112)  448        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_5 (DropConnect)   (None, 10, 10, 112)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 10, 10, 112)  0           ['drop_connect_5[0][0]',         \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 10, 10, 672)  75264       ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 10, 10, 672)  2688       ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_33 (Swish)               (None, 10, 10, 672)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_11 (Depthwise  (None, 5, 5, 672)   16800       ['swish_33[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 5, 5, 672)   2688        ['depthwise_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_34 (Swish)               (None, 5, 5, 672)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 1, 1, 672)    0           ['swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 1, 1, 28)     18844       ['lambda_11[0][0]']              \n",
            "                                                                                                  \n",
            " swish_35 (Swish)               (None, 1, 1, 28)     0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 1, 1, 672)    19488       ['swish_35[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1, 1, 672)    0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 5, 5, 672)    0           ['activation_11[0][0]',          \n",
            "                                                                  'swish_34[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 5, 5, 192)    129024      ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 5, 5, 192)   768         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 5, 5, 1152)   221184      ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_36 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_12 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_36[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_37 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 1, 1, 1152)   0           ['swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " swish_38 (Swish)               (None, 1, 1, 48)     0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_38[0][0]']               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 5, 5, 1152)   0           ['activation_12[0][0]',          \n",
            "                                                                  'swish_37[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_12[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 5, 5, 192)   768         ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_6 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_6[0][0]',         \n",
            "                                                                  'batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_39 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_13 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_39[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_40 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 1, 1, 1152)   0           ['swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " swish_41 (Swish)               (None, 1, 1, 48)     0           ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_41[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 5, 5, 1152)   0           ['activation_13[0][0]',          \n",
            "                                                                  'swish_40[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 5, 5, 192)   768         ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_7 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_7[0][0]',         \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_42 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_14 (Depthwise  (None, 5, 5, 1152)  28800       ['swish_42[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_43 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 1, 1, 1152)   0           ['swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " swish_44 (Swish)               (None, 1, 1, 48)     0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_44[0][0]']               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 5, 5, 1152)   0           ['activation_14[0][0]',          \n",
            "                                                                  'swish_43[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 5, 192)    221184      ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 5, 5, 192)   768         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " drop_connect_8 (DropConnect)   (None, 5, 5, 192)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 5, 5, 192)    0           ['drop_connect_8[0][0]',         \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 5, 5, 1152)   221184      ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 5, 5, 1152)  4608        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_45 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " depthwise_conv2d_15 (Depthwise  (None, 5, 5, 1152)  10368       ['swish_45[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 5, 5, 1152)  4608        ['depthwise_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_46 (Swish)               (None, 5, 5, 1152)   0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 1, 1, 1152)   0           ['swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 1, 1, 48)     55344       ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " swish_47 (Swish)               (None, 1, 1, 48)     0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 1, 1, 1152)   56448       ['swish_47[0][0]']               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1, 1, 1152)   0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 5, 5, 1152)   0           ['activation_15[0][0]',          \n",
            "                                                                  'swish_46[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 5, 5, 320)    368640      ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 5, 5, 320)   1280        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 5, 5, 1280)   409600      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 5, 5, 1280)  5120        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " swish_48 (Swish)               (None, 5, 5, 1280)   0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,049,564\n",
            "Trainable params: 4,007,548\n",
            "Non-trainable params: 42,016\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show architecture model"
      ],
      "metadata": {
        "id": "W1JJhCEWZjiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ดัดแปลง GlobalMaxPooling2D เพื่อแปลง 4D the (batch_size, rows, cols,channels) tensor เป็น 2D tensor with shape (batch_size,channels)\n",
        "#GlobalMaxPooling2D ส่งผลให้มีจำนวนฟีเจอร์น้อยกว่ามากเมื่อเทียบกับเลเยอร์ Flatten ซึ่งช่วยลดจำนวนพารามิเตอร์ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(2, activation='softmax', name=\"fc_out\"))"
      ],
      "metadata": {
        "id": "-9EQ5AdjZT9s"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wmBUcgsMZVkW",
        "outputId": "3ab3d4dd-0879-48c2-b5a2-43750ddd1c04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b0 (Functional  (None, 5, 5, 1280)       4049564   \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 1280)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 2)                 2562      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,052,126\n",
            "Trainable params: 4,010,110\n",
            "Non-trainable params: 42,016\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "id": "xRyPafCIZXzU",
        "outputId": "7f94551c-7dd9-4183-cd02-57ac539cf97e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers before freezing the conv base: 213\n",
            "This is the number of trainable layers after freezing the conv base: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting data augmentation"
      ],
      "metadata": {
        "id": "X9Xfp10TY9xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ด้วย ImageDataGenerator ของ Keras ซึ่งจะเพิ่มข้อมูลเสริมระหว่างการฝึกเพื่อลดโอกาสเกิด overfitting\n",
        "#overfitting เกิดจากข้อมูลที่ซับซ้อนกันเกินไป\n",
        "#Image Augmentation \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255, # image input 0-255 --> 0-1 เปลี่ยนค่าสี\n",
        "      rotation_range=40,# หมุนภาพในองศา\n",
        "      width_shift_range=0.2, #เปลี่ยนความกว้าง\n",
        "      height_shift_range=0.2, #ปลี่ยนความสูง\n",
        "      shear_range=0.2, #ทำให้ภาพเบี้ยว\n",
        "      zoom_range=0.2, # Randomly zoom image\n",
        "      horizontal_flip=True, \n",
        "      #โดย Default เมื่อมีการเลื่อนภาพ บิดภาพ หมุนภาพ จะเกิดพื้นที่ว่างที่มุม \n",
        "      #ซึ่งจะมีการเติมภาพให้เต็มโดยใช้เทคนิคแบบ Nearest neighbor ซึ่งเป็นการดึงสีบริเวณใหล้าเคียงมาระบายให้เต็ม แต่เราก็ยังสามารถกำหนดวิธีการ Fill ภาพด้วยเทคนิคอื่นได้จาก Parameter fill_mode\n",
        "      fill_mode='nearest')\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory #ไดเรกเป้าหมาย\n",
        "        train_dir,\n",
        "        # รูปภาพทั้งหมดจะถูกปรับขนาดตามความสูงและความกว้างของเป้าหมาย\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        # Since we use categorical_crossentropy loss, we need categorical labels\n",
        "        #เนื่องจากเราใช้ categorical_crossentropy loss เราจึงต้องมีป้ายกำกับตามหมวดหมู่\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory( #การดึงภาพจาก Directory มาเข้าโมเดล \n",
        "        validation_dir,\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')"
      ],
      "metadata": {
        "id": "GC-vPos9Y9HD",
        "outputId": "6b6365dd-7f9d-4f9b-b0cf-27de1df4d533",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 598 images belonging to 2 classes.\n",
            "Found 101 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, to_file='conv_base.png', show_shapes=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "NYCPRqN2c2gF",
        "outputId": "44d720a8-d2a6-4942-e513-c8a115288b06"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(filename='/content/unfreeze.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "enZMbj9Nc2iX",
        "outputId": "19cb5822-80fb-46f5-f8f7-42b06df00709"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# unfreeze"
      ],
      "metadata": {
        "id": "wcg1SJsalDvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set 'multiply_16' and following layers trainable (Unfreeze --> multiply_16 ) ให้เป็น layers ที่ train ชุดข้อมูลใหม่\n",
        "conv_base.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "    if layer.name == 'multiply_15':\n",
        "        set_trainable = True\n",
        "    if set_trainable:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__Oz62foc2dq",
        "outputId": "a3334e66-cf80-4185-9a9b-6d22cd791c73"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable layers after freezing the conv base: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8S60IpOWcB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "metadata": {
        "id": "Od8zqlOwb9Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50e86cf6-cd60-4925-c7c4-9f0f733fa495"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n",
            "<ipython-input-21-bbda3a575f01>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "37/37 [==============================] - 19s 120ms/step - loss: 2.2832 - acc: 0.5670 - val_loss: 0.8469 - val_acc: 0.6250\n",
            "Epoch 2/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 1.7157 - acc: 0.6392 - val_loss: 2.9448 - val_acc: 0.5000\n",
            "Epoch 3/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 1.4155 - acc: 0.6639 - val_loss: 1.6020 - val_acc: 0.4167\n",
            "Epoch 4/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 1.3589 - acc: 0.6787 - val_loss: 1.1227 - val_acc: 0.7292\n",
            "Epoch 5/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 1.3528 - acc: 0.6598 - val_loss: 1.3793 - val_acc: 0.6771\n",
            "Epoch 6/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 1.1714 - acc: 0.6684 - val_loss: 1.2219 - val_acc: 0.5625\n",
            "Epoch 7/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.9729 - acc: 0.6942 - val_loss: 2.0476 - val_acc: 0.6146\n",
            "Epoch 8/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 1.0244 - acc: 0.6821 - val_loss: 1.7154 - val_acc: 0.6458\n",
            "Epoch 9/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.7862 - acc: 0.7371 - val_loss: 1.2729 - val_acc: 0.5938\n",
            "Epoch 10/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.7760 - acc: 0.7285 - val_loss: 2.8911 - val_acc: 0.5729\n",
            "Epoch 11/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.7411 - acc: 0.7196 - val_loss: 2.0621 - val_acc: 0.6354\n",
            "Epoch 12/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.9321 - acc: 0.6907 - val_loss: 1.2622 - val_acc: 0.6875\n",
            "Epoch 13/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.6871 - acc: 0.7543 - val_loss: 1.0422 - val_acc: 0.6667\n",
            "Epoch 14/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.6943 - acc: 0.7509 - val_loss: 1.6308 - val_acc: 0.6458\n",
            "Epoch 15/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.5876 - acc: 0.7852 - val_loss: 0.9945 - val_acc: 0.6562\n",
            "Epoch 16/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.6043 - acc: 0.7577 - val_loss: 1.3812 - val_acc: 0.6562\n",
            "Epoch 17/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.6374 - acc: 0.7388 - val_loss: 0.8123 - val_acc: 0.5833\n",
            "Epoch 18/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.6162 - acc: 0.7251 - val_loss: 0.6931 - val_acc: 0.7396\n",
            "Epoch 19/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.4831 - acc: 0.7766 - val_loss: 0.6941 - val_acc: 0.7396\n",
            "Epoch 20/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.5327 - acc: 0.7818 - val_loss: 0.5069 - val_acc: 0.7708\n",
            "Epoch 21/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.4956 - acc: 0.8058 - val_loss: 0.9331 - val_acc: 0.5417\n",
            "Epoch 22/1000\n",
            "37/37 [==============================] - 3s 73ms/step - loss: 0.4561 - acc: 0.7973 - val_loss: 1.2549 - val_acc: 0.6250\n",
            "Epoch 23/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.4407 - acc: 0.8024 - val_loss: 0.6408 - val_acc: 0.7292\n",
            "Epoch 24/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.5424 - acc: 0.7784 - val_loss: 2.1636 - val_acc: 0.5000\n",
            "Epoch 25/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.5274 - acc: 0.7663 - val_loss: 0.8399 - val_acc: 0.7188\n",
            "Epoch 26/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.4095 - acc: 0.8179 - val_loss: 1.0022 - val_acc: 0.6979\n",
            "Epoch 27/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.4866 - acc: 0.7921 - val_loss: 1.1306 - val_acc: 0.6458\n",
            "Epoch 28/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.4065 - acc: 0.8144 - val_loss: 1.0660 - val_acc: 0.6354\n",
            "Epoch 29/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.4472 - acc: 0.8024 - val_loss: 0.7395 - val_acc: 0.7188\n",
            "Epoch 30/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.4370 - acc: 0.8196 - val_loss: 1.0181 - val_acc: 0.6771\n",
            "Epoch 31/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.4466 - acc: 0.7973 - val_loss: 0.9532 - val_acc: 0.6146\n",
            "Epoch 32/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.4207 - acc: 0.8247 - val_loss: 0.7614 - val_acc: 0.7188\n",
            "Epoch 33/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.4091 - acc: 0.8127 - val_loss: 1.1500 - val_acc: 0.6562\n",
            "Epoch 34/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.3835 - acc: 0.8230 - val_loss: 1.0082 - val_acc: 0.6250\n",
            "Epoch 35/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.3490 - acc: 0.8694 - val_loss: 0.7101 - val_acc: 0.7604\n",
            "Epoch 36/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.3157 - acc: 0.8608 - val_loss: 0.8331 - val_acc: 0.6667\n",
            "Epoch 37/1000\n",
            "37/37 [==============================] - 4s 76ms/step - loss: 0.3415 - acc: 0.8557 - val_loss: 0.7667 - val_acc: 0.7604\n",
            "Epoch 38/1000\n",
            "37/37 [==============================] - 3s 85ms/step - loss: 0.3342 - acc: 0.8660 - val_loss: 1.4334 - val_acc: 0.5729\n",
            "Epoch 39/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.3850 - acc: 0.8368 - val_loss: 0.8526 - val_acc: 0.7083\n",
            "Epoch 40/1000\n",
            "37/37 [==============================] - 3s 77ms/step - loss: 0.3745 - acc: 0.8368 - val_loss: 1.2558 - val_acc: 0.6354\n",
            "Epoch 41/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.3505 - acc: 0.8608 - val_loss: 1.3421 - val_acc: 0.6562\n",
            "Epoch 42/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.3340 - acc: 0.8574 - val_loss: 0.9323 - val_acc: 0.7292\n",
            "Epoch 43/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.2913 - acc: 0.8660 - val_loss: 1.2220 - val_acc: 0.6875\n",
            "Epoch 44/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.3573 - acc: 0.8471 - val_loss: 1.1874 - val_acc: 0.6458\n",
            "Epoch 45/1000\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 0.3734 - acc: 0.8522 - val_loss: 1.0882 - val_acc: 0.6875\n",
            "Epoch 46/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.3302 - acc: 0.8557 - val_loss: 0.9128 - val_acc: 0.6771\n",
            "Epoch 47/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2865 - acc: 0.8814 - val_loss: 1.1627 - val_acc: 0.6875\n",
            "Epoch 48/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.3394 - acc: 0.8625 - val_loss: 1.5227 - val_acc: 0.6458\n",
            "Epoch 49/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.3007 - acc: 0.8780 - val_loss: 0.8094 - val_acc: 0.7188\n",
            "Epoch 50/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.3120 - acc: 0.8797 - val_loss: 0.8947 - val_acc: 0.6562\n",
            "Epoch 51/1000\n",
            "37/37 [==============================] - 8s 204ms/step - loss: 0.3030 - acc: 0.8660 - val_loss: 0.8034 - val_acc: 0.7083\n",
            "Epoch 52/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.3050 - acc: 0.8694 - val_loss: 1.0660 - val_acc: 0.6562\n",
            "Epoch 53/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.3134 - acc: 0.8729 - val_loss: 0.8418 - val_acc: 0.7604\n",
            "Epoch 54/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.2377 - acc: 0.9072 - val_loss: 0.8124 - val_acc: 0.7396\n",
            "Epoch 55/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.3221 - acc: 0.8694 - val_loss: 1.0492 - val_acc: 0.6979\n",
            "Epoch 56/1000\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.2766 - acc: 0.8866 - val_loss: 0.9287 - val_acc: 0.7708\n",
            "Epoch 57/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.2513 - acc: 0.9038 - val_loss: 0.8632 - val_acc: 0.6771\n",
            "Epoch 58/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.3145 - acc: 0.8574 - val_loss: 1.0054 - val_acc: 0.6979\n",
            "Epoch 59/1000\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.2778 - acc: 0.8677 - val_loss: 1.0551 - val_acc: 0.6667\n",
            "Epoch 60/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.3182 - acc: 0.8694 - val_loss: 0.9098 - val_acc: 0.6979\n",
            "Epoch 61/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.2941 - acc: 0.8763 - val_loss: 1.3028 - val_acc: 0.6667\n",
            "Epoch 62/1000\n",
            "37/37 [==============================] - 3s 85ms/step - loss: 0.2640 - acc: 0.8900 - val_loss: 1.4247 - val_acc: 0.6458\n",
            "Epoch 63/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2163 - acc: 0.9072 - val_loss: 1.0451 - val_acc: 0.6979\n",
            "Epoch 64/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.2551 - acc: 0.8970 - val_loss: 1.4064 - val_acc: 0.6562\n",
            "Epoch 65/1000\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 0.3517 - acc: 0.8711 - val_loss: 0.8290 - val_acc: 0.7292\n",
            "Epoch 66/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.2458 - acc: 0.8918 - val_loss: 0.9858 - val_acc: 0.6875\n",
            "Epoch 67/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2237 - acc: 0.9072 - val_loss: 1.0528 - val_acc: 0.6458\n",
            "Epoch 68/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.2876 - acc: 0.8918 - val_loss: 0.9560 - val_acc: 0.7188\n",
            "Epoch 69/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2634 - acc: 0.9089 - val_loss: 1.1560 - val_acc: 0.6667\n",
            "Epoch 70/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2813 - acc: 0.8866 - val_loss: 1.1468 - val_acc: 0.6979\n",
            "Epoch 71/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2176 - acc: 0.9021 - val_loss: 0.8537 - val_acc: 0.7188\n",
            "Epoch 72/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.2157 - acc: 0.9124 - val_loss: 1.0598 - val_acc: 0.6562\n",
            "Epoch 73/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.2467 - acc: 0.8935 - val_loss: 0.8073 - val_acc: 0.7708\n",
            "Epoch 74/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.2954 - acc: 0.8918 - val_loss: 1.0444 - val_acc: 0.6875\n",
            "Epoch 75/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.2401 - acc: 0.8969 - val_loss: 1.7843 - val_acc: 0.6042\n",
            "Epoch 76/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2608 - acc: 0.8986 - val_loss: 1.0995 - val_acc: 0.6875\n",
            "Epoch 77/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.2547 - acc: 0.8797 - val_loss: 1.2681 - val_acc: 0.6458\n",
            "Epoch 78/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.2085 - acc: 0.9141 - val_loss: 1.1566 - val_acc: 0.7083\n",
            "Epoch 79/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.2640 - acc: 0.8952 - val_loss: 1.1485 - val_acc: 0.6979\n",
            "Epoch 80/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2796 - acc: 0.8918 - val_loss: 0.8630 - val_acc: 0.7292\n",
            "Epoch 81/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2592 - acc: 0.9038 - val_loss: 0.9470 - val_acc: 0.7083\n",
            "Epoch 82/1000\n",
            "37/37 [==============================] - 4s 77ms/step - loss: 0.2457 - acc: 0.8918 - val_loss: 0.7605 - val_acc: 0.7500\n",
            "Epoch 83/1000\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 0.2464 - acc: 0.9021 - val_loss: 0.8051 - val_acc: 0.6771\n",
            "Epoch 84/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2437 - acc: 0.9107 - val_loss: 0.7327 - val_acc: 0.7500\n",
            "Epoch 85/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2606 - acc: 0.8952 - val_loss: 1.0033 - val_acc: 0.7083\n",
            "Epoch 86/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.2308 - acc: 0.9021 - val_loss: 1.3310 - val_acc: 0.6458\n",
            "Epoch 87/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.2493 - acc: 0.8936 - val_loss: 0.9787 - val_acc: 0.6979\n",
            "Epoch 88/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2758 - acc: 0.8969 - val_loss: 1.1927 - val_acc: 0.6771\n",
            "Epoch 89/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2501 - acc: 0.8866 - val_loss: 1.7502 - val_acc: 0.6250\n",
            "Epoch 90/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.2153 - acc: 0.9210 - val_loss: 1.3326 - val_acc: 0.6771\n",
            "Epoch 91/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2167 - acc: 0.9189 - val_loss: 1.2233 - val_acc: 0.6979\n",
            "Epoch 92/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2043 - acc: 0.9175 - val_loss: 1.3121 - val_acc: 0.6979\n",
            "Epoch 93/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2546 - acc: 0.9072 - val_loss: 1.4563 - val_acc: 0.6979\n",
            "Epoch 94/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2383 - acc: 0.8986 - val_loss: 1.4454 - val_acc: 0.6771\n",
            "Epoch 95/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.2124 - acc: 0.9296 - val_loss: 1.2078 - val_acc: 0.6667\n",
            "Epoch 96/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.2369 - acc: 0.9124 - val_loss: 1.3577 - val_acc: 0.6354\n",
            "Epoch 97/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.1845 - acc: 0.9278 - val_loss: 0.9607 - val_acc: 0.7083\n",
            "Epoch 98/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2569 - acc: 0.9141 - val_loss: 0.9651 - val_acc: 0.7500\n",
            "Epoch 99/1000\n",
            "37/37 [==============================] - 3s 74ms/step - loss: 0.2561 - acc: 0.8918 - val_loss: 1.2190 - val_acc: 0.5417\n",
            "Epoch 100/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.2396 - acc: 0.9124 - val_loss: 1.3961 - val_acc: 0.6146\n",
            "Epoch 101/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.2581 - acc: 0.8935 - val_loss: 1.3221 - val_acc: 0.5833\n",
            "Epoch 102/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.2568 - acc: 0.8883 - val_loss: 1.5679 - val_acc: 0.6146\n",
            "Epoch 103/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.2101 - acc: 0.9313 - val_loss: 1.8247 - val_acc: 0.6250\n",
            "Epoch 104/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.2257 - acc: 0.9192 - val_loss: 1.9654 - val_acc: 0.6250\n",
            "Epoch 105/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.1941 - acc: 0.9244 - val_loss: 1.6070 - val_acc: 0.6771\n",
            "Epoch 106/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.2639 - acc: 0.9107 - val_loss: 1.5611 - val_acc: 0.6354\n",
            "Epoch 107/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2195 - acc: 0.9089 - val_loss: 1.4067 - val_acc: 0.5625\n",
            "Epoch 108/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.2053 - acc: 0.9124 - val_loss: 1.3068 - val_acc: 0.5729\n",
            "Epoch 109/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.2467 - acc: 0.9089 - val_loss: 1.2156 - val_acc: 0.5729\n",
            "Epoch 110/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.2241 - acc: 0.9158 - val_loss: 1.5680 - val_acc: 0.4896\n",
            "Epoch 111/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.2578 - acc: 0.9003 - val_loss: 1.3411 - val_acc: 0.6354\n",
            "Epoch 112/1000\n",
            "37/37 [==============================] - 5s 108ms/step - loss: 0.2297 - acc: 0.9124 - val_loss: 1.3887 - val_acc: 0.5729\n",
            "Epoch 113/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.2329 - acc: 0.9141 - val_loss: 1.3747 - val_acc: 0.5938\n",
            "Epoch 114/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.2166 - acc: 0.9055 - val_loss: 1.2368 - val_acc: 0.6250\n",
            "Epoch 115/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.2012 - acc: 0.9278 - val_loss: 1.6976 - val_acc: 0.6354\n",
            "Epoch 116/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.2599 - acc: 0.9139 - val_loss: 1.5519 - val_acc: 0.6146\n",
            "Epoch 117/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2463 - acc: 0.8935 - val_loss: 1.2820 - val_acc: 0.6667\n",
            "Epoch 118/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.1974 - acc: 0.9192 - val_loss: 1.5886 - val_acc: 0.6667\n",
            "Epoch 119/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.2503 - acc: 0.9038 - val_loss: 1.2715 - val_acc: 0.6458\n",
            "Epoch 120/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2150 - acc: 0.9038 - val_loss: 1.5353 - val_acc: 0.6562\n",
            "Epoch 121/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.2123 - acc: 0.9227 - val_loss: 1.6695 - val_acc: 0.6250\n",
            "Epoch 122/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2094 - acc: 0.9192 - val_loss: 1.3089 - val_acc: 0.6771\n",
            "Epoch 123/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2011 - acc: 0.9089 - val_loss: 1.4397 - val_acc: 0.6146\n",
            "Epoch 124/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.2466 - acc: 0.9072 - val_loss: 1.4372 - val_acc: 0.6354\n",
            "Epoch 125/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.2420 - acc: 0.9089 - val_loss: 1.3708 - val_acc: 0.6354\n",
            "Epoch 126/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 0.2404 - acc: 0.9192 - val_loss: 1.5930 - val_acc: 0.6667\n",
            "Epoch 127/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.2136 - acc: 0.9244 - val_loss: 1.3292 - val_acc: 0.6979\n",
            "Epoch 128/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.2040 - acc: 0.9107 - val_loss: 1.4398 - val_acc: 0.6771\n",
            "Epoch 129/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.2689 - acc: 0.8832 - val_loss: 1.4233 - val_acc: 0.6979\n",
            "Epoch 130/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.1854 - acc: 0.9261 - val_loss: 1.2647 - val_acc: 0.6771\n",
            "Epoch 131/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.1787 - acc: 0.9261 - val_loss: 1.7508 - val_acc: 0.5938\n",
            "Epoch 132/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.2204 - acc: 0.9175 - val_loss: 1.7912 - val_acc: 0.6354\n",
            "Epoch 133/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.1373 - acc: 0.9485 - val_loss: 1.3695 - val_acc: 0.6667\n",
            "Epoch 134/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2134 - acc: 0.9158 - val_loss: 1.6953 - val_acc: 0.6458\n",
            "Epoch 135/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1710 - acc: 0.9399 - val_loss: 1.6287 - val_acc: 0.5417\n",
            "Epoch 136/1000\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.1920 - acc: 0.9244 - val_loss: 1.5112 - val_acc: 0.6042\n",
            "Epoch 137/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.2432 - acc: 0.9072 - val_loss: 1.5904 - val_acc: 0.6875\n",
            "Epoch 138/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2407 - acc: 0.9107 - val_loss: 2.0194 - val_acc: 0.6562\n",
            "Epoch 139/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.2111 - acc: 0.9210 - val_loss: 1.4572 - val_acc: 0.5938\n",
            "Epoch 140/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2095 - acc: 0.9192 - val_loss: 1.5247 - val_acc: 0.5104\n",
            "Epoch 141/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.1993 - acc: 0.9296 - val_loss: 1.4535 - val_acc: 0.6667\n",
            "Epoch 142/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2879 - acc: 0.8952 - val_loss: 1.2671 - val_acc: 0.6771\n",
            "Epoch 143/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 0.1677 - acc: 0.9296 - val_loss: 1.2813 - val_acc: 0.6875\n",
            "Epoch 144/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 0.1725 - acc: 0.9381 - val_loss: 1.2335 - val_acc: 0.6979\n",
            "Epoch 145/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.2448 - acc: 0.8935 - val_loss: 1.2854 - val_acc: 0.6354\n",
            "Epoch 146/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.1976 - acc: 0.9296 - val_loss: 1.1409 - val_acc: 0.6146\n",
            "Epoch 147/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.1770 - acc: 0.9210 - val_loss: 1.3252 - val_acc: 0.6354\n",
            "Epoch 148/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1876 - acc: 0.9261 - val_loss: 1.6823 - val_acc: 0.6562\n",
            "Epoch 149/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1787 - acc: 0.9313 - val_loss: 1.4185 - val_acc: 0.6771\n",
            "Epoch 150/1000\n",
            "37/37 [==============================] - 3s 76ms/step - loss: 0.1794 - acc: 0.9381 - val_loss: 1.0600 - val_acc: 0.6354\n",
            "Epoch 151/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1931 - acc: 0.9175 - val_loss: 1.0918 - val_acc: 0.6250\n",
            "Epoch 152/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2168 - acc: 0.9227 - val_loss: 1.3588 - val_acc: 0.6458\n",
            "Epoch 153/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1698 - acc: 0.9296 - val_loss: 1.1706 - val_acc: 0.6979\n",
            "Epoch 154/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1622 - acc: 0.9347 - val_loss: 1.5200 - val_acc: 0.6875\n",
            "Epoch 155/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.1967 - acc: 0.9313 - val_loss: 1.1564 - val_acc: 0.6250\n",
            "Epoch 156/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.2009 - acc: 0.9192 - val_loss: 1.3790 - val_acc: 0.6562\n",
            "Epoch 157/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.2114 - acc: 0.9313 - val_loss: 1.4366 - val_acc: 0.7083\n",
            "Epoch 158/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1775 - acc: 0.9433 - val_loss: 1.8870 - val_acc: 0.6771\n",
            "Epoch 159/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.1805 - acc: 0.9158 - val_loss: 1.3493 - val_acc: 0.7292\n",
            "Epoch 160/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2099 - acc: 0.9089 - val_loss: 1.6322 - val_acc: 0.7083\n",
            "Epoch 161/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.2110 - acc: 0.9261 - val_loss: 1.7283 - val_acc: 0.6875\n",
            "Epoch 162/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.1918 - acc: 0.9244 - val_loss: 1.4007 - val_acc: 0.6771\n",
            "Epoch 163/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1677 - acc: 0.9244 - val_loss: 1.1885 - val_acc: 0.6979\n",
            "Epoch 164/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.1662 - acc: 0.9313 - val_loss: 1.0525 - val_acc: 0.7083\n",
            "Epoch 165/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1956 - acc: 0.9261 - val_loss: 1.5836 - val_acc: 0.6667\n",
            "Epoch 166/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.1608 - acc: 0.9399 - val_loss: 1.9316 - val_acc: 0.6250\n",
            "Epoch 167/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.1937 - acc: 0.9307 - val_loss: 2.1113 - val_acc: 0.6250\n",
            "Epoch 168/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.2519 - acc: 0.9192 - val_loss: 1.7317 - val_acc: 0.6667\n",
            "Epoch 169/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1766 - acc: 0.9261 - val_loss: 1.6603 - val_acc: 0.6771\n",
            "Epoch 170/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.1794 - acc: 0.9330 - val_loss: 1.9795 - val_acc: 0.7188\n",
            "Epoch 171/1000\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 0.2207 - acc: 0.9278 - val_loss: 1.6273 - val_acc: 0.7083\n",
            "Epoch 172/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1793 - acc: 0.9296 - val_loss: 1.5748 - val_acc: 0.6458\n",
            "Epoch 173/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.1735 - acc: 0.9261 - val_loss: 1.6340 - val_acc: 0.6667\n",
            "Epoch 174/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.1661 - acc: 0.9227 - val_loss: 1.6771 - val_acc: 0.6979\n",
            "Epoch 175/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.1204 - acc: 0.9519 - val_loss: 1.5017 - val_acc: 0.7188\n",
            "Epoch 176/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2549 - acc: 0.9107 - val_loss: 1.2152 - val_acc: 0.6771\n",
            "Epoch 177/1000\n",
            "37/37 [==============================] - 4s 78ms/step - loss: 0.1933 - acc: 0.9381 - val_loss: 1.1976 - val_acc: 0.6771\n",
            "Epoch 178/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1833 - acc: 0.9278 - val_loss: 1.3684 - val_acc: 0.6771\n",
            "Epoch 179/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2513 - acc: 0.9072 - val_loss: 1.1709 - val_acc: 0.6042\n",
            "Epoch 180/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1793 - acc: 0.9244 - val_loss: 1.3218 - val_acc: 0.6771\n",
            "Epoch 181/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.1841 - acc: 0.9364 - val_loss: 1.4048 - val_acc: 0.6458\n",
            "Epoch 182/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1643 - acc: 0.9296 - val_loss: 1.2406 - val_acc: 0.7188\n",
            "Epoch 183/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1850 - acc: 0.9313 - val_loss: 1.4857 - val_acc: 0.6667\n",
            "Epoch 184/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.2007 - acc: 0.9227 - val_loss: 2.0049 - val_acc: 0.6771\n",
            "Epoch 185/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.1833 - acc: 0.9399 - val_loss: 1.7219 - val_acc: 0.7292\n",
            "Epoch 186/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.2405 - acc: 0.9192 - val_loss: 2.1154 - val_acc: 0.7083\n",
            "Epoch 187/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1820 - acc: 0.9347 - val_loss: 1.9402 - val_acc: 0.7083\n",
            "Epoch 188/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1278 - acc: 0.9588 - val_loss: 2.3146 - val_acc: 0.6354\n",
            "Epoch 189/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.1818 - acc: 0.9364 - val_loss: 2.0869 - val_acc: 0.6979\n",
            "Epoch 190/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.2007 - acc: 0.9244 - val_loss: 2.2804 - val_acc: 0.6562\n",
            "Epoch 191/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.1761 - acc: 0.9347 - val_loss: 2.0024 - val_acc: 0.6667\n",
            "Epoch 192/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.2066 - acc: 0.9313 - val_loss: 1.4585 - val_acc: 0.7188\n",
            "Epoch 193/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1566 - acc: 0.9450 - val_loss: 1.5317 - val_acc: 0.7188\n",
            "Epoch 194/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2195 - acc: 0.9261 - val_loss: 1.6037 - val_acc: 0.7188\n",
            "Epoch 195/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.1969 - acc: 0.9278 - val_loss: 1.7053 - val_acc: 0.7083\n",
            "Epoch 196/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.1727 - acc: 0.9364 - val_loss: 1.9167 - val_acc: 0.6979\n",
            "Epoch 197/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1756 - acc: 0.9278 - val_loss: 1.7605 - val_acc: 0.6667\n",
            "Epoch 198/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.1316 - acc: 0.9536 - val_loss: 2.2782 - val_acc: 0.6354\n",
            "Epoch 199/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1476 - acc: 0.9416 - val_loss: 1.8202 - val_acc: 0.6979\n",
            "Epoch 200/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.1899 - acc: 0.9330 - val_loss: 1.7600 - val_acc: 0.7708\n",
            "Epoch 201/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1972 - acc: 0.9381 - val_loss: 2.0166 - val_acc: 0.7188\n",
            "Epoch 202/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1712 - acc: 0.9381 - val_loss: 1.6124 - val_acc: 0.7083\n",
            "Epoch 203/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.2203 - acc: 0.9244 - val_loss: 1.9234 - val_acc: 0.6875\n",
            "Epoch 204/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2166 - acc: 0.9210 - val_loss: 1.6830 - val_acc: 0.6562\n",
            "Epoch 205/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1998 - acc: 0.9347 - val_loss: 1.3438 - val_acc: 0.6771\n",
            "Epoch 206/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.2084 - acc: 0.9192 - val_loss: 1.6476 - val_acc: 0.6875\n",
            "Epoch 207/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2062 - acc: 0.9296 - val_loss: 1.7942 - val_acc: 0.6146\n",
            "Epoch 208/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1805 - acc: 0.9381 - val_loss: 1.2244 - val_acc: 0.6667\n",
            "Epoch 209/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1657 - acc: 0.9399 - val_loss: 1.6516 - val_acc: 0.5938\n",
            "Epoch 210/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.1639 - acc: 0.9381 - val_loss: 1.5369 - val_acc: 0.6771\n",
            "Epoch 211/1000\n",
            "37/37 [==============================] - 9s 211ms/step - loss: 0.2029 - acc: 0.9347 - val_loss: 1.8033 - val_acc: 0.6042\n",
            "Epoch 212/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2138 - acc: 0.9330 - val_loss: 1.5366 - val_acc: 0.6667\n",
            "Epoch 213/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2091 - acc: 0.9313 - val_loss: 1.8416 - val_acc: 0.6458\n",
            "Epoch 214/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.2220 - acc: 0.9313 - val_loss: 1.8596 - val_acc: 0.6771\n",
            "Epoch 215/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.1385 - acc: 0.9502 - val_loss: 1.5257 - val_acc: 0.6875\n",
            "Epoch 216/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1603 - acc: 0.9261 - val_loss: 2.0603 - val_acc: 0.6458\n",
            "Epoch 217/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2571 - acc: 0.9141 - val_loss: 1.7481 - val_acc: 0.6562\n",
            "Epoch 218/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1697 - acc: 0.9433 - val_loss: 1.8212 - val_acc: 0.6667\n",
            "Epoch 219/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.2091 - acc: 0.9330 - val_loss: 1.8586 - val_acc: 0.6458\n",
            "Epoch 220/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 0.2059 - acc: 0.9244 - val_loss: 1.5494 - val_acc: 0.6562\n",
            "Epoch 221/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.1718 - acc: 0.9416 - val_loss: 1.0744 - val_acc: 0.7396\n",
            "Epoch 222/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2053 - acc: 0.9313 - val_loss: 1.6546 - val_acc: 0.6458\n",
            "Epoch 223/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1832 - acc: 0.9399 - val_loss: 1.0755 - val_acc: 0.6771\n",
            "Epoch 224/1000\n",
            "37/37 [==============================] - 3s 75ms/step - loss: 0.1943 - acc: 0.9244 - val_loss: 1.2074 - val_acc: 0.6667\n",
            "Epoch 225/1000\n",
            "37/37 [==============================] - 6s 141ms/step - loss: 0.1948 - acc: 0.9244 - val_loss: 1.5344 - val_acc: 0.6458\n",
            "Epoch 226/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.1874 - acc: 0.9296 - val_loss: 1.7401 - val_acc: 0.5208\n",
            "Epoch 227/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1984 - acc: 0.9278 - val_loss: 1.4783 - val_acc: 0.6146\n",
            "Epoch 228/1000\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 0.1789 - acc: 0.9278 - val_loss: 1.7602 - val_acc: 0.5938\n",
            "Epoch 229/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.1625 - acc: 0.9364 - val_loss: 1.6414 - val_acc: 0.5938\n",
            "Epoch 230/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.1585 - acc: 0.9330 - val_loss: 2.1223 - val_acc: 0.6250\n",
            "Epoch 231/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.1559 - acc: 0.9347 - val_loss: 1.4755 - val_acc: 0.6562\n",
            "Epoch 232/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2213 - acc: 0.9381 - val_loss: 1.5694 - val_acc: 0.6250\n",
            "Epoch 233/1000\n",
            "37/37 [==============================] - 3s 84ms/step - loss: 0.1771 - acc: 0.9364 - val_loss: 1.3285 - val_acc: 0.6562\n",
            "Epoch 234/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.2026 - acc: 0.9381 - val_loss: 2.0220 - val_acc: 0.6250\n",
            "Epoch 235/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.2028 - acc: 0.9257 - val_loss: 1.7357 - val_acc: 0.6562\n",
            "Epoch 236/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 0.1888 - acc: 0.9296 - val_loss: 1.6878 - val_acc: 0.5729\n",
            "Epoch 237/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1725 - acc: 0.9416 - val_loss: 1.2664 - val_acc: 0.7083\n",
            "Epoch 238/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1305 - acc: 0.9485 - val_loss: 1.4816 - val_acc: 0.6562\n",
            "Epoch 239/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.1819 - acc: 0.9347 - val_loss: 1.3980 - val_acc: 0.6979\n",
            "Epoch 240/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1879 - acc: 0.9296 - val_loss: 1.4891 - val_acc: 0.6562\n",
            "Epoch 241/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.2448 - acc: 0.9158 - val_loss: 1.7508 - val_acc: 0.6250\n",
            "Epoch 242/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.1274 - acc: 0.9416 - val_loss: 1.7006 - val_acc: 0.5729\n",
            "Epoch 243/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.2001 - acc: 0.9381 - val_loss: 2.1987 - val_acc: 0.5625\n",
            "Epoch 244/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.1850 - acc: 0.9381 - val_loss: 2.1128 - val_acc: 0.5833\n",
            "Epoch 245/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1832 - acc: 0.9381 - val_loss: 2.1265 - val_acc: 0.6667\n",
            "Epoch 246/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2321 - acc: 0.9210 - val_loss: 2.6161 - val_acc: 0.6562\n",
            "Epoch 247/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.1949 - acc: 0.9347 - val_loss: 1.9651 - val_acc: 0.6771\n",
            "Epoch 248/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1992 - acc: 0.9278 - val_loss: 2.1238 - val_acc: 0.6562\n",
            "Epoch 249/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.1889 - acc: 0.9278 - val_loss: 1.5382 - val_acc: 0.7083\n",
            "Epoch 250/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1843 - acc: 0.9485 - val_loss: 1.2796 - val_acc: 0.7188\n",
            "Epoch 251/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1971 - acc: 0.9261 - val_loss: 1.5473 - val_acc: 0.6667\n",
            "Epoch 252/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1359 - acc: 0.9467 - val_loss: 1.6679 - val_acc: 0.6562\n",
            "Epoch 253/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1261 - acc: 0.9502 - val_loss: 2.8008 - val_acc: 0.6250\n",
            "Epoch 254/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.1757 - acc: 0.9347 - val_loss: 1.6398 - val_acc: 0.6979\n",
            "Epoch 255/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1621 - acc: 0.9416 - val_loss: 1.6188 - val_acc: 0.6667\n",
            "Epoch 256/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.2390 - acc: 0.9347 - val_loss: 1.4083 - val_acc: 0.6979\n",
            "Epoch 257/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.1636 - acc: 0.9416 - val_loss: 1.2875 - val_acc: 0.7083\n",
            "Epoch 258/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1512 - acc: 0.9433 - val_loss: 1.5361 - val_acc: 0.7188\n",
            "Epoch 259/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.1788 - acc: 0.9399 - val_loss: 1.2352 - val_acc: 0.6354\n",
            "Epoch 260/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.1679 - acc: 0.9433 - val_loss: 1.8234 - val_acc: 0.6146\n",
            "Epoch 261/1000\n",
            "37/37 [==============================] - 9s 216ms/step - loss: 0.1666 - acc: 0.9536 - val_loss: 2.3201 - val_acc: 0.5417\n",
            "Epoch 262/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1530 - acc: 0.9296 - val_loss: 1.9734 - val_acc: 0.6146\n",
            "Epoch 263/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1195 - acc: 0.9493 - val_loss: 1.6577 - val_acc: 0.6354\n",
            "Epoch 264/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.1646 - acc: 0.9467 - val_loss: 1.2017 - val_acc: 0.6354\n",
            "Epoch 265/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.1497 - acc: 0.9433 - val_loss: 1.6332 - val_acc: 0.6562\n",
            "Epoch 266/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.1590 - acc: 0.9485 - val_loss: 1.7170 - val_acc: 0.6354\n",
            "Epoch 267/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1940 - acc: 0.9381 - val_loss: 1.9689 - val_acc: 0.6979\n",
            "Epoch 268/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.1721 - acc: 0.9502 - val_loss: 1.8943 - val_acc: 0.6354\n",
            "Epoch 269/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.1889 - acc: 0.9296 - val_loss: 1.5849 - val_acc: 0.7083\n",
            "Epoch 270/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.2456 - acc: 0.9124 - val_loss: 1.7214 - val_acc: 0.6667\n",
            "Epoch 271/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.2045 - acc: 0.9296 - val_loss: 1.5201 - val_acc: 0.7083\n",
            "Epoch 272/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.1344 - acc: 0.9467 - val_loss: 1.6122 - val_acc: 0.6979\n",
            "Epoch 273/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2038 - acc: 0.9399 - val_loss: 2.1752 - val_acc: 0.6250\n",
            "Epoch 274/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.1786 - acc: 0.9433 - val_loss: 2.6182 - val_acc: 0.6458\n",
            "Epoch 275/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.2122 - acc: 0.9347 - val_loss: 2.2737 - val_acc: 0.6875\n",
            "Epoch 276/1000\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 0.1540 - acc: 0.9364 - val_loss: 2.5874 - val_acc: 0.6458\n",
            "Epoch 277/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2265 - acc: 0.9158 - val_loss: 2.3957 - val_acc: 0.6771\n",
            "Epoch 278/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1517 - acc: 0.9433 - val_loss: 2.8102 - val_acc: 0.6354\n",
            "Epoch 279/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1568 - acc: 0.9364 - val_loss: 1.9864 - val_acc: 0.6458\n",
            "Epoch 280/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1859 - acc: 0.9381 - val_loss: 2.1457 - val_acc: 0.6667\n",
            "Epoch 281/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.1533 - acc: 0.9553 - val_loss: 2.1622 - val_acc: 0.5833\n",
            "Epoch 282/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2013 - acc: 0.9330 - val_loss: 2.2901 - val_acc: 0.6250\n",
            "Epoch 283/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 0.1474 - acc: 0.9553 - val_loss: 2.2876 - val_acc: 0.6354\n",
            "Epoch 284/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1718 - acc: 0.9399 - val_loss: 2.1520 - val_acc: 0.6771\n",
            "Epoch 285/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.1508 - acc: 0.9467 - val_loss: 2.3571 - val_acc: 0.6250\n",
            "Epoch 286/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.1894 - acc: 0.9433 - val_loss: 2.3236 - val_acc: 0.6146\n",
            "Epoch 287/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1345 - acc: 0.9485 - val_loss: 2.2855 - val_acc: 0.6354\n",
            "Epoch 288/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1433 - acc: 0.9381 - val_loss: 2.1986 - val_acc: 0.6667\n",
            "Epoch 289/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.1635 - acc: 0.9364 - val_loss: 1.8370 - val_acc: 0.6875\n",
            "Epoch 290/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.1086 - acc: 0.9553 - val_loss: 2.2956 - val_acc: 0.6667\n",
            "Epoch 291/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.1644 - acc: 0.9485 - val_loss: 2.3536 - val_acc: 0.6562\n",
            "Epoch 292/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1682 - acc: 0.9347 - val_loss: 2.4955 - val_acc: 0.6771\n",
            "Epoch 293/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.1215 - acc: 0.9519 - val_loss: 2.3905 - val_acc: 0.6771\n",
            "Epoch 294/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.2078 - acc: 0.9364 - val_loss: 2.0139 - val_acc: 0.5625\n",
            "Epoch 295/1000\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.1708 - acc: 0.9450 - val_loss: 1.7474 - val_acc: 0.6354\n",
            "Epoch 296/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.2281 - acc: 0.9192 - val_loss: 1.7488 - val_acc: 0.6771\n",
            "Epoch 297/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.1485 - acc: 0.9467 - val_loss: 2.0626 - val_acc: 0.6354\n",
            "Epoch 298/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.1411 - acc: 0.9536 - val_loss: 2.0086 - val_acc: 0.6562\n",
            "Epoch 299/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.2364 - acc: 0.9381 - val_loss: 1.8524 - val_acc: 0.6562\n",
            "Epoch 300/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.2169 - acc: 0.9364 - val_loss: 1.5483 - val_acc: 0.6562\n",
            "Epoch 301/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1084 - acc: 0.9605 - val_loss: 1.9231 - val_acc: 0.6354\n",
            "Epoch 302/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1687 - acc: 0.9399 - val_loss: 2.1444 - val_acc: 0.5625\n",
            "Epoch 303/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1706 - acc: 0.9364 - val_loss: 1.5150 - val_acc: 0.6875\n",
            "Epoch 304/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1409 - acc: 0.9450 - val_loss: 1.7435 - val_acc: 0.6458\n",
            "Epoch 305/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.1966 - acc: 0.9330 - val_loss: 1.6529 - val_acc: 0.6667\n",
            "Epoch 306/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.1616 - acc: 0.9450 - val_loss: 2.1809 - val_acc: 0.6250\n",
            "Epoch 307/1000\n",
            "37/37 [==============================] - 6s 139ms/step - loss: 0.1887 - acc: 0.9364 - val_loss: 1.9780 - val_acc: 0.5938\n",
            "Epoch 308/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.2366 - acc: 0.9381 - val_loss: 2.0021 - val_acc: 0.6458\n",
            "Epoch 309/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1811 - acc: 0.9502 - val_loss: 1.5509 - val_acc: 0.6771\n",
            "Epoch 310/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2159 - acc: 0.9210 - val_loss: 1.5767 - val_acc: 0.6042\n",
            "Epoch 311/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.1593 - acc: 0.9330 - val_loss: 1.7175 - val_acc: 0.6562\n",
            "Epoch 312/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.0935 - acc: 0.9536 - val_loss: 1.5188 - val_acc: 0.6667\n",
            "Epoch 313/1000\n",
            "37/37 [==============================] - 3s 78ms/step - loss: 0.1657 - acc: 0.9399 - val_loss: 1.3618 - val_acc: 0.6354\n",
            "Epoch 314/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.2085 - acc: 0.9244 - val_loss: 1.5498 - val_acc: 0.6354\n",
            "Epoch 315/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1608 - acc: 0.9485 - val_loss: 1.7364 - val_acc: 0.5833\n",
            "Epoch 316/1000\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.1774 - acc: 0.9278 - val_loss: 1.9673 - val_acc: 0.6354\n",
            "Epoch 317/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1600 - acc: 0.9485 - val_loss: 1.9358 - val_acc: 0.6354\n",
            "Epoch 318/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.1204 - acc: 0.9485 - val_loss: 1.7036 - val_acc: 0.6250\n",
            "Epoch 319/1000\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.1376 - acc: 0.9588 - val_loss: 1.8115 - val_acc: 0.6354\n",
            "Epoch 320/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.2973 - acc: 0.9296 - val_loss: 1.8420 - val_acc: 0.6979\n",
            "Epoch 321/1000\n",
            "37/37 [==============================] - 4s 111ms/step - loss: 0.1484 - acc: 0.9519 - val_loss: 1.8582 - val_acc: 0.6771\n",
            "Epoch 322/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1424 - acc: 0.9553 - val_loss: 1.7020 - val_acc: 0.7083\n",
            "Epoch 323/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 0.1324 - acc: 0.9485 - val_loss: 1.4201 - val_acc: 0.6979\n",
            "Epoch 324/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1663 - acc: 0.9364 - val_loss: 1.7939 - val_acc: 0.7188\n",
            "Epoch 325/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1552 - acc: 0.9519 - val_loss: 2.0782 - val_acc: 0.6771\n",
            "Epoch 326/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.1367 - acc: 0.9485 - val_loss: 2.1963 - val_acc: 0.6771\n",
            "Epoch 327/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2455 - acc: 0.9347 - val_loss: 1.7541 - val_acc: 0.6458\n",
            "Epoch 328/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2001 - acc: 0.9381 - val_loss: 1.5476 - val_acc: 0.6771\n",
            "Epoch 329/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.1747 - acc: 0.9206 - val_loss: 1.4654 - val_acc: 0.6771\n",
            "Epoch 330/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.2227 - acc: 0.9244 - val_loss: 1.3653 - val_acc: 0.6771\n",
            "Epoch 331/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.1988 - acc: 0.9330 - val_loss: 1.5326 - val_acc: 0.6458\n",
            "Epoch 332/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1962 - acc: 0.9072 - val_loss: 1.6717 - val_acc: 0.6250\n",
            "Epoch 333/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.2067 - acc: 0.9261 - val_loss: 1.7406 - val_acc: 0.6042\n",
            "Epoch 334/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.1506 - acc: 0.9416 - val_loss: 1.3485 - val_acc: 0.6042\n",
            "Epoch 335/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1284 - acc: 0.9588 - val_loss: 1.5976 - val_acc: 0.6354\n",
            "Epoch 336/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.1769 - acc: 0.9485 - val_loss: 1.5419 - val_acc: 0.6667\n",
            "Epoch 337/1000\n",
            "37/37 [==============================] - 10s 235ms/step - loss: 0.1721 - acc: 0.9364 - val_loss: 1.2464 - val_acc: 0.6771\n",
            "Epoch 338/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1434 - acc: 0.9399 - val_loss: 1.0963 - val_acc: 0.6875\n",
            "Epoch 339/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.1761 - acc: 0.9416 - val_loss: 1.4099 - val_acc: 0.5938\n",
            "Epoch 340/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.2364 - acc: 0.9347 - val_loss: 1.6023 - val_acc: 0.5938\n",
            "Epoch 341/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.2607 - acc: 0.9227 - val_loss: 1.7701 - val_acc: 0.5521\n",
            "Epoch 342/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.2001 - acc: 0.9399 - val_loss: 2.2993 - val_acc: 0.6042\n",
            "Epoch 343/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.1788 - acc: 0.9450 - val_loss: 1.8712 - val_acc: 0.6458\n",
            "Epoch 344/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.1838 - acc: 0.9416 - val_loss: 1.8057 - val_acc: 0.6146\n",
            "Epoch 345/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.1642 - acc: 0.9502 - val_loss: 2.0626 - val_acc: 0.6250\n",
            "Epoch 346/1000\n",
            "37/37 [==============================] - 9s 214ms/step - loss: 0.2365 - acc: 0.9330 - val_loss: 1.7028 - val_acc: 0.6667\n",
            "Epoch 347/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.1283 - acc: 0.9570 - val_loss: 1.8102 - val_acc: 0.6146\n",
            "Epoch 348/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.2222 - acc: 0.9330 - val_loss: 1.8787 - val_acc: 0.7083\n",
            "Epoch 349/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1922 - acc: 0.9381 - val_loss: 2.1975 - val_acc: 0.6354\n",
            "Epoch 350/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1989 - acc: 0.9416 - val_loss: 1.5905 - val_acc: 0.7083\n",
            "Epoch 351/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.1175 - acc: 0.9570 - val_loss: 1.7682 - val_acc: 0.6771\n",
            "Epoch 352/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2159 - acc: 0.9244 - val_loss: 2.1102 - val_acc: 0.6562\n",
            "Epoch 353/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.1714 - acc: 0.9416 - val_loss: 1.6690 - val_acc: 0.6771\n",
            "Epoch 354/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.1737 - acc: 0.9433 - val_loss: 1.7053 - val_acc: 0.6979\n",
            "Epoch 355/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2490 - acc: 0.9210 - val_loss: 1.3710 - val_acc: 0.7188\n",
            "Epoch 356/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.1240 - acc: 0.9553 - val_loss: 1.9325 - val_acc: 0.6250\n",
            "Epoch 357/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1391 - acc: 0.9502 - val_loss: 2.0806 - val_acc: 0.6354\n",
            "Epoch 358/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.2065 - acc: 0.9381 - val_loss: 1.6492 - val_acc: 0.6667\n",
            "Epoch 359/1000\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 0.1790 - acc: 0.9450 - val_loss: 1.3446 - val_acc: 0.6250\n",
            "Epoch 360/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 0.1676 - acc: 0.9399 - val_loss: 1.9194 - val_acc: 0.6146\n",
            "Epoch 361/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.1687 - acc: 0.9296 - val_loss: 1.9289 - val_acc: 0.5833\n",
            "Epoch 362/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.1692 - acc: 0.9485 - val_loss: 1.7089 - val_acc: 0.6250\n",
            "Epoch 363/1000\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 0.1983 - acc: 0.9296 - val_loss: 2.2134 - val_acc: 0.5000\n",
            "Epoch 364/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1356 - acc: 0.9399 - val_loss: 1.9812 - val_acc: 0.5417\n",
            "Epoch 365/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 0.2013 - acc: 0.9399 - val_loss: 1.8216 - val_acc: 0.5833\n",
            "Epoch 366/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1555 - acc: 0.9381 - val_loss: 1.6394 - val_acc: 0.6146\n",
            "Epoch 367/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.1526 - acc: 0.9467 - val_loss: 1.7287 - val_acc: 0.6146\n",
            "Epoch 368/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1254 - acc: 0.9426 - val_loss: 1.7263 - val_acc: 0.6458\n",
            "Epoch 369/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.2511 - acc: 0.9141 - val_loss: 1.4200 - val_acc: 0.6354\n",
            "Epoch 370/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2135 - acc: 0.9381 - val_loss: 1.6057 - val_acc: 0.6354\n",
            "Epoch 371/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1893 - acc: 0.9433 - val_loss: 1.7470 - val_acc: 0.6979\n",
            "Epoch 372/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.1488 - acc: 0.9588 - val_loss: 1.9744 - val_acc: 0.6146\n",
            "Epoch 373/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.1370 - acc: 0.9570 - val_loss: 2.1202 - val_acc: 0.6146\n",
            "Epoch 374/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.1588 - acc: 0.9485 - val_loss: 1.6770 - val_acc: 0.6667\n",
            "Epoch 375/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.1130 - acc: 0.9588 - val_loss: 1.8011 - val_acc: 0.5729\n",
            "Epoch 376/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1642 - acc: 0.9364 - val_loss: 1.8995 - val_acc: 0.6042\n",
            "Epoch 377/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.1514 - acc: 0.9485 - val_loss: 1.4186 - val_acc: 0.6146\n",
            "Epoch 378/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1735 - acc: 0.9381 - val_loss: 1.5037 - val_acc: 0.6875\n",
            "Epoch 379/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.1646 - acc: 0.9278 - val_loss: 1.5854 - val_acc: 0.6562\n",
            "Epoch 380/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.1707 - acc: 0.9433 - val_loss: 1.8697 - val_acc: 0.5938\n",
            "Epoch 381/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1695 - acc: 0.9375 - val_loss: 1.9203 - val_acc: 0.5625\n",
            "Epoch 382/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1955 - acc: 0.9364 - val_loss: 1.8282 - val_acc: 0.6146\n",
            "Epoch 383/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1994 - acc: 0.9278 - val_loss: 1.6768 - val_acc: 0.6146\n",
            "Epoch 384/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.1442 - acc: 0.9485 - val_loss: 1.3778 - val_acc: 0.6771\n",
            "Epoch 385/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.1246 - acc: 0.9502 - val_loss: 1.3124 - val_acc: 0.6771\n",
            "Epoch 386/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1618 - acc: 0.9467 - val_loss: 1.8371 - val_acc: 0.6562\n",
            "Epoch 387/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1586 - acc: 0.9450 - val_loss: 1.3462 - val_acc: 0.6667\n",
            "Epoch 388/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.2024 - acc: 0.9347 - val_loss: 1.9421 - val_acc: 0.5625\n",
            "Epoch 389/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.1276 - acc: 0.9553 - val_loss: 2.1300 - val_acc: 0.6458\n",
            "Epoch 390/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.1055 - acc: 0.9605 - val_loss: 2.4956 - val_acc: 0.6354\n",
            "Epoch 391/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1985 - acc: 0.9313 - val_loss: 1.9173 - val_acc: 0.6042\n",
            "Epoch 392/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2321 - acc: 0.9261 - val_loss: 1.5030 - val_acc: 0.6042\n",
            "Epoch 393/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1214 - acc: 0.9553 - val_loss: 1.6457 - val_acc: 0.7083\n",
            "Epoch 394/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.1766 - acc: 0.9330 - val_loss: 2.0543 - val_acc: 0.6562\n",
            "Epoch 395/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1286 - acc: 0.9502 - val_loss: 1.6769 - val_acc: 0.6667\n",
            "Epoch 396/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1776 - acc: 0.9485 - val_loss: 1.5115 - val_acc: 0.7083\n",
            "Epoch 397/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 0.1850 - acc: 0.9502 - val_loss: 1.3077 - val_acc: 0.6875\n",
            "Epoch 398/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.1575 - acc: 0.9502 - val_loss: 1.0598 - val_acc: 0.7292\n",
            "Epoch 399/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.1522 - acc: 0.9502 - val_loss: 1.1476 - val_acc: 0.6875\n",
            "Epoch 400/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.1993 - acc: 0.9467 - val_loss: 0.9392 - val_acc: 0.7083\n",
            "Epoch 401/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.1779 - acc: 0.9433 - val_loss: 1.1650 - val_acc: 0.6979\n",
            "Epoch 402/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1578 - acc: 0.9433 - val_loss: 1.0691 - val_acc: 0.7083\n",
            "Epoch 403/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.1782 - acc: 0.9330 - val_loss: 1.3076 - val_acc: 0.6875\n",
            "Epoch 404/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1405 - acc: 0.9519 - val_loss: 1.4012 - val_acc: 0.7396\n",
            "Epoch 405/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.1500 - acc: 0.9510 - val_loss: 1.5533 - val_acc: 0.6979\n",
            "Epoch 406/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.1791 - acc: 0.9485 - val_loss: 1.4847 - val_acc: 0.6979\n",
            "Epoch 407/1000\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 0.1332 - acc: 0.9553 - val_loss: 1.3249 - val_acc: 0.7604\n",
            "Epoch 408/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1849 - acc: 0.9399 - val_loss: 1.6381 - val_acc: 0.6562\n",
            "Epoch 409/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.1497 - acc: 0.9588 - val_loss: 1.7318 - val_acc: 0.6562\n",
            "Epoch 410/1000\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 0.1410 - acc: 0.9536 - val_loss: 1.9480 - val_acc: 0.6562\n",
            "Epoch 411/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.1645 - acc: 0.9433 - val_loss: 1.9485 - val_acc: 0.6250\n",
            "Epoch 412/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.1801 - acc: 0.9433 - val_loss: 1.7091 - val_acc: 0.6458\n",
            "Epoch 413/1000\n",
            "37/37 [==============================] - 5s 112ms/step - loss: 0.1698 - acc: 0.9416 - val_loss: 1.5994 - val_acc: 0.6667\n",
            "Epoch 414/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.1471 - acc: 0.9381 - val_loss: 1.9568 - val_acc: 0.6667\n",
            "Epoch 415/1000\n",
            "37/37 [==============================] - 5s 114ms/step - loss: 0.1364 - acc: 0.9519 - val_loss: 2.1056 - val_acc: 0.6250\n",
            "Epoch 416/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1631 - acc: 0.9467 - val_loss: 2.6050 - val_acc: 0.6042\n",
            "Epoch 417/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.1990 - acc: 0.9341 - val_loss: 1.5215 - val_acc: 0.6771\n",
            "Epoch 418/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1700 - acc: 0.9399 - val_loss: 1.6791 - val_acc: 0.5938\n",
            "Epoch 419/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1313 - acc: 0.9622 - val_loss: 1.5196 - val_acc: 0.6146\n",
            "Epoch 420/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1691 - acc: 0.9278 - val_loss: 1.3961 - val_acc: 0.6979\n",
            "Epoch 421/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1654 - acc: 0.9433 - val_loss: 1.2527 - val_acc: 0.7083\n",
            "Epoch 422/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.1855 - acc: 0.9416 - val_loss: 1.3041 - val_acc: 0.6979\n",
            "Epoch 423/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.1123 - acc: 0.9622 - val_loss: 1.8909 - val_acc: 0.6146\n",
            "Epoch 424/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.1325 - acc: 0.9467 - val_loss: 1.5960 - val_acc: 0.6354\n",
            "Epoch 425/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.1715 - acc: 0.9450 - val_loss: 1.3948 - val_acc: 0.6354\n",
            "Epoch 426/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.1200 - acc: 0.9622 - val_loss: 1.6459 - val_acc: 0.6667\n",
            "Epoch 427/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1259 - acc: 0.9536 - val_loss: 1.2087 - val_acc: 0.6979\n",
            "Epoch 428/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1762 - acc: 0.9485 - val_loss: 1.3168 - val_acc: 0.6667\n",
            "Epoch 429/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.1804 - acc: 0.9416 - val_loss: 1.4790 - val_acc: 0.6250\n",
            "Epoch 430/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1323 - acc: 0.9553 - val_loss: 1.7041 - val_acc: 0.5833\n",
            "Epoch 431/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1791 - acc: 0.9467 - val_loss: 1.6749 - val_acc: 0.6250\n",
            "Epoch 432/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1385 - acc: 0.9588 - val_loss: 1.8598 - val_acc: 0.5625\n",
            "Epoch 433/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.1359 - acc: 0.9553 - val_loss: 1.9928 - val_acc: 0.6771\n",
            "Epoch 434/1000\n",
            "37/37 [==============================] - 5s 101ms/step - loss: 0.1504 - acc: 0.9622 - val_loss: 2.1643 - val_acc: 0.6771\n",
            "Epoch 435/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.1202 - acc: 0.9519 - val_loss: 2.2557 - val_acc: 0.6250\n",
            "Epoch 436/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.2363 - acc: 0.9330 - val_loss: 2.0418 - val_acc: 0.5729\n",
            "Epoch 437/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.1454 - acc: 0.9519 - val_loss: 2.0724 - val_acc: 0.6667\n",
            "Epoch 438/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.2207 - acc: 0.9433 - val_loss: 2.8353 - val_acc: 0.6354\n",
            "Epoch 439/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.1212 - acc: 0.9605 - val_loss: 2.2861 - val_acc: 0.6875\n",
            "Epoch 440/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.1316 - acc: 0.9588 - val_loss: 2.6307 - val_acc: 0.6875\n",
            "Epoch 441/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.2310 - acc: 0.9278 - val_loss: 2.1575 - val_acc: 0.6458\n",
            "Epoch 442/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.1231 - acc: 0.9639 - val_loss: 2.6012 - val_acc: 0.6562\n",
            "Epoch 443/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1501 - acc: 0.9450 - val_loss: 2.1004 - val_acc: 0.6250\n",
            "Epoch 444/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 0.1943 - acc: 0.9502 - val_loss: 2.1173 - val_acc: 0.6667\n",
            "Epoch 445/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.1702 - acc: 0.9553 - val_loss: 1.7452 - val_acc: 0.6042\n",
            "Epoch 446/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.1520 - acc: 0.9588 - val_loss: 1.8521 - val_acc: 0.6875\n",
            "Epoch 447/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.1146 - acc: 0.9519 - val_loss: 1.8849 - val_acc: 0.6667\n",
            "Epoch 448/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.1271 - acc: 0.9578 - val_loss: 2.2980 - val_acc: 0.5938\n",
            "Epoch 449/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.1555 - acc: 0.9467 - val_loss: 1.9105 - val_acc: 0.6250\n",
            "Epoch 450/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.2049 - acc: 0.9450 - val_loss: 2.2811 - val_acc: 0.7083\n",
            "Epoch 451/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.1740 - acc: 0.9485 - val_loss: 3.0579 - val_acc: 0.6562\n",
            "Epoch 452/1000\n",
            "37/37 [==============================] - 8s 211ms/step - loss: 0.1399 - acc: 0.9502 - val_loss: 2.4473 - val_acc: 0.6354\n",
            "Epoch 453/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.1241 - acc: 0.9588 - val_loss: 2.9427 - val_acc: 0.6458\n",
            "Epoch 454/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2676 - acc: 0.9364 - val_loss: 2.5381 - val_acc: 0.6458\n",
            "Epoch 455/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1909 - acc: 0.9347 - val_loss: 1.9851 - val_acc: 0.6562\n",
            "Epoch 456/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.2065 - acc: 0.9450 - val_loss: 1.8376 - val_acc: 0.7083\n",
            "Epoch 457/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.1985 - acc: 0.9485 - val_loss: 1.9407 - val_acc: 0.6875\n",
            "Epoch 458/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.1398 - acc: 0.9519 - val_loss: 2.1668 - val_acc: 0.6667\n",
            "Epoch 459/1000\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.1177 - acc: 0.9502 - val_loss: 2.8781 - val_acc: 0.6354\n",
            "Epoch 460/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1601 - acc: 0.9485 - val_loss: 2.9617 - val_acc: 0.6562\n",
            "Epoch 461/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.1463 - acc: 0.9570 - val_loss: 2.3648 - val_acc: 0.6875\n",
            "Epoch 462/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.1497 - acc: 0.9639 - val_loss: 3.3100 - val_acc: 0.6562\n",
            "Epoch 463/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.1587 - acc: 0.9433 - val_loss: 3.3551 - val_acc: 0.6771\n",
            "Epoch 464/1000\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.1878 - acc: 0.9485 - val_loss: 2.8777 - val_acc: 0.6667\n",
            "Epoch 465/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1658 - acc: 0.9467 - val_loss: 2.2716 - val_acc: 0.6562\n",
            "Epoch 466/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.1319 - acc: 0.9519 - val_loss: 1.8486 - val_acc: 0.6875\n",
            "Epoch 467/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.2137 - acc: 0.9416 - val_loss: 2.0780 - val_acc: 0.6875\n",
            "Epoch 468/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1202 - acc: 0.9570 - val_loss: 2.4737 - val_acc: 0.6458\n",
            "Epoch 469/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 0.1884 - acc: 0.9443 - val_loss: 2.2984 - val_acc: 0.6771\n",
            "Epoch 470/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1400 - acc: 0.9519 - val_loss: 2.4090 - val_acc: 0.6146\n",
            "Epoch 471/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.2106 - acc: 0.9416 - val_loss: 2.4265 - val_acc: 0.6562\n",
            "Epoch 472/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.2641 - acc: 0.9296 - val_loss: 2.1893 - val_acc: 0.6354\n",
            "Epoch 473/1000\n",
            "37/37 [==============================] - 5s 118ms/step - loss: 0.1650 - acc: 0.9570 - val_loss: 2.1257 - val_acc: 0.6667\n",
            "Epoch 474/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.1649 - acc: 0.9605 - val_loss: 2.0340 - val_acc: 0.6458\n",
            "Epoch 475/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 0.1584 - acc: 0.9416 - val_loss: 2.1384 - val_acc: 0.6562\n",
            "Epoch 476/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1609 - acc: 0.9433 - val_loss: 1.8310 - val_acc: 0.7188\n",
            "Epoch 477/1000\n",
            "37/37 [==============================] - 4s 82ms/step - loss: 0.1741 - acc: 0.9553 - val_loss: 1.7524 - val_acc: 0.6458\n",
            "Epoch 478/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.1679 - acc: 0.9450 - val_loss: 2.0564 - val_acc: 0.6667\n",
            "Epoch 479/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.2391 - acc: 0.9330 - val_loss: 2.1592 - val_acc: 0.6562\n",
            "Epoch 480/1000\n",
            "37/37 [==============================] - 4s 80ms/step - loss: 0.1184 - acc: 0.9519 - val_loss: 1.6183 - val_acc: 0.7083\n",
            "Epoch 481/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.1298 - acc: 0.9433 - val_loss: 1.8397 - val_acc: 0.6771\n",
            "Epoch 482/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1710 - acc: 0.9467 - val_loss: 1.7470 - val_acc: 0.7396\n",
            "Epoch 483/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.2051 - acc: 0.9433 - val_loss: 1.6942 - val_acc: 0.6667\n",
            "Epoch 484/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1447 - acc: 0.9502 - val_loss: 1.8983 - val_acc: 0.6250\n",
            "Epoch 485/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.1349 - acc: 0.9588 - val_loss: 1.4475 - val_acc: 0.7188\n",
            "Epoch 486/1000\n",
            "37/37 [==============================] - 3s 79ms/step - loss: 0.1072 - acc: 0.9691 - val_loss: 1.5834 - val_acc: 0.6771\n",
            "Epoch 487/1000\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 0.1754 - acc: 0.9416 - val_loss: 1.7386 - val_acc: 0.6667\n",
            "Epoch 488/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.1478 - acc: 0.9536 - val_loss: 1.8122 - val_acc: 0.6458\n",
            "Epoch 489/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1681 - acc: 0.9485 - val_loss: 1.5300 - val_acc: 0.6562\n",
            "Epoch 490/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1848 - acc: 0.9399 - val_loss: 1.8521 - val_acc: 0.6250\n",
            "Epoch 491/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1653 - acc: 0.9485 - val_loss: 1.5767 - val_acc: 0.6458\n",
            "Epoch 492/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1856 - acc: 0.9416 - val_loss: 1.6898 - val_acc: 0.6979\n",
            "Epoch 493/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.1427 - acc: 0.9536 - val_loss: 1.8971 - val_acc: 0.6354\n",
            "Epoch 494/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.1317 - acc: 0.9485 - val_loss: 2.4127 - val_acc: 0.5833\n",
            "Epoch 495/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.1445 - acc: 0.9553 - val_loss: 2.4093 - val_acc: 0.6042\n",
            "Epoch 496/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 0.1411 - acc: 0.9570 - val_loss: 2.5913 - val_acc: 0.6250\n",
            "Epoch 497/1000\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 0.1703 - acc: 0.9536 - val_loss: 2.4774 - val_acc: 0.6562\n",
            "Epoch 498/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1600 - acc: 0.9433 - val_loss: 2.3727 - val_acc: 0.6354\n",
            "Epoch 499/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.1563 - acc: 0.9553 - val_loss: 2.7719 - val_acc: 0.5104\n",
            "Epoch 500/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1419 - acc: 0.9485 - val_loss: 2.1837 - val_acc: 0.5938\n",
            "Epoch 501/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1432 - acc: 0.9450 - val_loss: 2.6434 - val_acc: 0.5938\n",
            "Epoch 502/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.1314 - acc: 0.9485 - val_loss: 1.9673 - val_acc: 0.6562\n",
            "Epoch 503/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1063 - acc: 0.9656 - val_loss: 2.6006 - val_acc: 0.5938\n",
            "Epoch 504/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1828 - acc: 0.9450 - val_loss: 2.5063 - val_acc: 0.6250\n",
            "Epoch 505/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1534 - acc: 0.9553 - val_loss: 3.4731 - val_acc: 0.6146\n",
            "Epoch 506/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.1344 - acc: 0.9553 - val_loss: 3.7495 - val_acc: 0.6042\n",
            "Epoch 507/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.1380 - acc: 0.9536 - val_loss: 2.7970 - val_acc: 0.6250\n",
            "Epoch 508/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1695 - acc: 0.9467 - val_loss: 2.7691 - val_acc: 0.6042\n",
            "Epoch 509/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.1354 - acc: 0.9536 - val_loss: 2.5265 - val_acc: 0.6354\n",
            "Epoch 510/1000\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.1352 - acc: 0.9605 - val_loss: 2.5065 - val_acc: 0.6354\n",
            "Epoch 511/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1704 - acc: 0.9416 - val_loss: 2.2401 - val_acc: 0.6146\n",
            "Epoch 512/1000\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.2194 - acc: 0.9364 - val_loss: 2.5724 - val_acc: 0.6458\n",
            "Epoch 513/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.1884 - acc: 0.9485 - val_loss: 2.3006 - val_acc: 0.6354\n",
            "Epoch 514/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.1355 - acc: 0.9561 - val_loss: 2.0375 - val_acc: 0.6979\n",
            "Epoch 515/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.1639 - acc: 0.9536 - val_loss: 2.0439 - val_acc: 0.6875\n",
            "Epoch 516/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.1458 - acc: 0.9570 - val_loss: 2.3536 - val_acc: 0.6562\n",
            "Epoch 517/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.1230 - acc: 0.9570 - val_loss: 2.0851 - val_acc: 0.6562\n",
            "Epoch 518/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.1828 - acc: 0.9502 - val_loss: 2.0428 - val_acc: 0.6354\n",
            "Epoch 519/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.1843 - acc: 0.9467 - val_loss: 2.4805 - val_acc: 0.6458\n",
            "Epoch 520/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1798 - acc: 0.9536 - val_loss: 2.2651 - val_acc: 0.6667\n",
            "Epoch 521/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 0.2688 - acc: 0.9296 - val_loss: 2.4059 - val_acc: 0.6875\n",
            "Epoch 522/1000\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.1269 - acc: 0.9553 - val_loss: 1.8701 - val_acc: 0.6875\n",
            "Epoch 523/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.1736 - acc: 0.9467 - val_loss: 1.8527 - val_acc: 0.6667\n",
            "Epoch 524/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.1577 - acc: 0.9485 - val_loss: 2.3205 - val_acc: 0.6250\n",
            "Epoch 525/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1176 - acc: 0.9605 - val_loss: 1.6602 - val_acc: 0.6979\n",
            "Epoch 526/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.1312 - acc: 0.9588 - val_loss: 2.0106 - val_acc: 0.7083\n",
            "Epoch 527/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1776 - acc: 0.9364 - val_loss: 1.9574 - val_acc: 0.6667\n",
            "Epoch 528/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1926 - acc: 0.9485 - val_loss: 1.6880 - val_acc: 0.6667\n",
            "Epoch 529/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1892 - acc: 0.9433 - val_loss: 1.1647 - val_acc: 0.7292\n",
            "Epoch 530/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.1660 - acc: 0.9570 - val_loss: 1.2923 - val_acc: 0.7083\n",
            "Epoch 531/1000\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.2000 - acc: 0.9347 - val_loss: 1.2597 - val_acc: 0.7083\n",
            "Epoch 532/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.1758 - acc: 0.9553 - val_loss: 1.3211 - val_acc: 0.6875\n",
            "Epoch 533/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.1412 - acc: 0.9485 - val_loss: 2.1059 - val_acc: 0.6354\n",
            "Epoch 534/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1685 - acc: 0.9416 - val_loss: 2.0609 - val_acc: 0.6562\n",
            "Epoch 535/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.1738 - acc: 0.9570 - val_loss: 2.0851 - val_acc: 0.6667\n",
            "Epoch 536/1000\n",
            "37/37 [==============================] - 9s 215ms/step - loss: 0.2029 - acc: 0.9519 - val_loss: 2.2746 - val_acc: 0.6458\n",
            "Epoch 537/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.2354 - acc: 0.9227 - val_loss: 2.2954 - val_acc: 0.5938\n",
            "Epoch 538/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1489 - acc: 0.9502 - val_loss: 2.3580 - val_acc: 0.6354\n",
            "Epoch 539/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.1491 - acc: 0.9467 - val_loss: 1.7092 - val_acc: 0.6875\n",
            "Epoch 540/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.2291 - acc: 0.9381 - val_loss: 1.5156 - val_acc: 0.7188\n",
            "Epoch 541/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.2037 - acc: 0.9433 - val_loss: 2.2697 - val_acc: 0.6250\n",
            "Epoch 542/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1546 - acc: 0.9553 - val_loss: 1.6299 - val_acc: 0.6771\n",
            "Epoch 543/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.1695 - acc: 0.9467 - val_loss: 2.5836 - val_acc: 0.6250\n",
            "Epoch 544/1000\n",
            "37/37 [==============================] - 4s 81ms/step - loss: 0.1575 - acc: 0.9485 - val_loss: 2.4331 - val_acc: 0.6146\n",
            "Epoch 545/1000\n",
            "37/37 [==============================] - 6s 141ms/step - loss: 0.0930 - acc: 0.9639 - val_loss: 1.8274 - val_acc: 0.6667\n",
            "Epoch 546/1000\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.2060 - acc: 0.9381 - val_loss: 1.9857 - val_acc: 0.6771\n",
            "Epoch 547/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1637 - acc: 0.9519 - val_loss: 2.0740 - val_acc: 0.6458\n",
            "Epoch 548/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.1942 - acc: 0.9313 - val_loss: 2.2111 - val_acc: 0.6146\n",
            "Epoch 549/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1400 - acc: 0.9450 - val_loss: 2.3469 - val_acc: 0.5938\n",
            "Epoch 550/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.1368 - acc: 0.9519 - val_loss: 2.0761 - val_acc: 0.6458\n",
            "Epoch 551/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1108 - acc: 0.9536 - val_loss: 1.6769 - val_acc: 0.6875\n",
            "Epoch 552/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 0.1659 - acc: 0.9399 - val_loss: 1.7243 - val_acc: 0.6667\n",
            "Epoch 553/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1809 - acc: 0.9536 - val_loss: 2.4215 - val_acc: 0.6875\n",
            "Epoch 554/1000\n",
            "37/37 [==============================] - 4s 79ms/step - loss: 0.1626 - acc: 0.9519 - val_loss: 2.2739 - val_acc: 0.6562\n",
            "Epoch 555/1000\n",
            "37/37 [==============================] - 9s 238ms/step - loss: 0.1264 - acc: 0.9570 - val_loss: 2.2467 - val_acc: 0.6458\n",
            "Epoch 556/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1691 - acc: 0.9553 - val_loss: 2.0317 - val_acc: 0.6667\n",
            "Epoch 557/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.1756 - acc: 0.9502 - val_loss: 1.8667 - val_acc: 0.6667\n",
            "Epoch 558/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1647 - acc: 0.9485 - val_loss: 2.2077 - val_acc: 0.6875\n",
            "Epoch 559/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1066 - acc: 0.9639 - val_loss: 2.3547 - val_acc: 0.7188\n",
            "Epoch 560/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1401 - acc: 0.9519 - val_loss: 2.2698 - val_acc: 0.6979\n",
            "Epoch 561/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1188 - acc: 0.9691 - val_loss: 2.1239 - val_acc: 0.6771\n",
            "Epoch 562/1000\n",
            "37/37 [==============================] - 5s 113ms/step - loss: 0.1752 - acc: 0.9467 - val_loss: 1.8128 - val_acc: 0.6979\n",
            "Epoch 563/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.1932 - acc: 0.9381 - val_loss: 1.8624 - val_acc: 0.6771\n",
            "Epoch 564/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.1685 - acc: 0.9467 - val_loss: 1.7833 - val_acc: 0.7292\n",
            "Epoch 565/1000\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.1581 - acc: 0.9553 - val_loss: 1.6285 - val_acc: 0.7396\n",
            "Epoch 566/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1749 - acc: 0.9364 - val_loss: 1.7823 - val_acc: 0.6979\n",
            "Epoch 567/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.1562 - acc: 0.9519 - val_loss: 1.7202 - val_acc: 0.7188\n",
            "Epoch 568/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1189 - acc: 0.9622 - val_loss: 2.9539 - val_acc: 0.6667\n",
            "Epoch 569/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1501 - acc: 0.9553 - val_loss: 2.7210 - val_acc: 0.6875\n",
            "Epoch 570/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.1620 - acc: 0.9485 - val_loss: 2.3576 - val_acc: 0.6875\n",
            "Epoch 571/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.2025 - acc: 0.9330 - val_loss: 2.4307 - val_acc: 0.6667\n",
            "Epoch 572/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.2098 - acc: 0.9502 - val_loss: 2.0359 - val_acc: 0.7188\n",
            "Epoch 573/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.1774 - acc: 0.9570 - val_loss: 2.1941 - val_acc: 0.6250\n",
            "Epoch 574/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.1555 - acc: 0.9433 - val_loss: 3.4693 - val_acc: 0.6458\n",
            "Epoch 575/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.2434 - acc: 0.9381 - val_loss: 2.5788 - val_acc: 0.6979\n",
            "Epoch 576/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.1643 - acc: 0.9605 - val_loss: 2.8497 - val_acc: 0.6562\n",
            "Epoch 577/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.1537 - acc: 0.9639 - val_loss: 2.3659 - val_acc: 0.7188\n",
            "Epoch 578/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.1889 - acc: 0.9381 - val_loss: 2.0171 - val_acc: 0.6979\n",
            "Epoch 579/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.1631 - acc: 0.9553 - val_loss: 2.8756 - val_acc: 0.6562\n",
            "Epoch 580/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1318 - acc: 0.9656 - val_loss: 2.3123 - val_acc: 0.6771\n",
            "Epoch 581/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.0974 - acc: 0.9639 - val_loss: 2.4916 - val_acc: 0.6875\n",
            "Epoch 582/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.2245 - acc: 0.9553 - val_loss: 2.0741 - val_acc: 0.6562\n",
            "Epoch 583/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.1787 - acc: 0.9467 - val_loss: 1.8358 - val_acc: 0.6875\n",
            "Epoch 584/1000\n",
            "37/37 [==============================] - 3s 81ms/step - loss: 0.1200 - acc: 0.9639 - val_loss: 2.0417 - val_acc: 0.6771\n",
            "Epoch 585/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.2420 - acc: 0.9433 - val_loss: 2.3446 - val_acc: 0.7083\n",
            "Epoch 586/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.2436 - acc: 0.9364 - val_loss: 3.5079 - val_acc: 0.6042\n",
            "Epoch 587/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.1447 - acc: 0.9519 - val_loss: 1.9625 - val_acc: 0.6771\n",
            "Epoch 588/1000\n",
            "37/37 [==============================] - 8s 212ms/step - loss: 0.2076 - acc: 0.9347 - val_loss: 1.7990 - val_acc: 0.7083\n",
            "Epoch 589/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.2242 - acc: 0.9467 - val_loss: 2.4220 - val_acc: 0.6875\n",
            "Epoch 590/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.1115 - acc: 0.9622 - val_loss: 1.7281 - val_acc: 0.7188\n",
            "Epoch 591/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 0.1180 - acc: 0.9656 - val_loss: 1.6663 - val_acc: 0.7396\n",
            "Epoch 592/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.1908 - acc: 0.9570 - val_loss: 1.6128 - val_acc: 0.7292\n",
            "Epoch 593/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.1587 - acc: 0.9519 - val_loss: 1.7041 - val_acc: 0.6979\n",
            "Epoch 594/1000\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.1148 - acc: 0.9639 - val_loss: 2.0515 - val_acc: 0.6771\n",
            "Epoch 595/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 0.1627 - acc: 0.9485 - val_loss: 2.2570 - val_acc: 0.6562\n",
            "Epoch 596/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2201 - acc: 0.9364 - val_loss: 1.7795 - val_acc: 0.6667\n",
            "Epoch 597/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.2387 - acc: 0.9450 - val_loss: 2.7812 - val_acc: 0.6667\n",
            "Epoch 598/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1710 - acc: 0.9553 - val_loss: 3.1963 - val_acc: 0.6458\n",
            "Epoch 599/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1642 - acc: 0.9588 - val_loss: 2.0450 - val_acc: 0.6771\n",
            "Epoch 600/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 0.1702 - acc: 0.9570 - val_loss: 2.5383 - val_acc: 0.6250\n",
            "Epoch 601/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.1900 - acc: 0.9399 - val_loss: 3.0024 - val_acc: 0.6146\n",
            "Epoch 602/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1774 - acc: 0.9622 - val_loss: 2.0533 - val_acc: 0.6458\n",
            "Epoch 603/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.1040 - acc: 0.9656 - val_loss: 1.4539 - val_acc: 0.6875\n",
            "Epoch 604/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2691 - acc: 0.9313 - val_loss: 2.5262 - val_acc: 0.6562\n",
            "Epoch 605/1000\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.1609 - acc: 0.9485 - val_loss: 2.1725 - val_acc: 0.6354\n",
            "Epoch 606/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1762 - acc: 0.9519 - val_loss: 2.4974 - val_acc: 0.6458\n",
            "Epoch 607/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.2255 - acc: 0.9296 - val_loss: 1.8801 - val_acc: 0.6771\n",
            "Epoch 608/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.1268 - acc: 0.9493 - val_loss: 2.2392 - val_acc: 0.6042\n",
            "Epoch 609/1000\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.1691 - acc: 0.9553 - val_loss: 1.8298 - val_acc: 0.6458\n",
            "Epoch 610/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 0.1582 - acc: 0.9433 - val_loss: 2.0233 - val_acc: 0.6146\n",
            "Epoch 611/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1524 - acc: 0.9519 - val_loss: 1.4618 - val_acc: 0.6875\n",
            "Epoch 612/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.1081 - acc: 0.9570 - val_loss: 1.5742 - val_acc: 0.6354\n",
            "Epoch 613/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.2198 - acc: 0.9416 - val_loss: 1.3121 - val_acc: 0.6875\n",
            "Epoch 614/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1301 - acc: 0.9519 - val_loss: 1.6608 - val_acc: 0.6562\n",
            "Epoch 615/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.1879 - acc: 0.9536 - val_loss: 2.1857 - val_acc: 0.6250\n",
            "Epoch 616/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1350 - acc: 0.9605 - val_loss: 2.5026 - val_acc: 0.6458\n",
            "Epoch 617/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1547 - acc: 0.9467 - val_loss: 2.4051 - val_acc: 0.5521\n",
            "Epoch 618/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1814 - acc: 0.9519 - val_loss: 2.3261 - val_acc: 0.6250\n",
            "Epoch 619/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.1184 - acc: 0.9588 - val_loss: 2.5403 - val_acc: 0.5625\n",
            "Epoch 620/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.2286 - acc: 0.9381 - val_loss: 2.3990 - val_acc: 0.5729\n",
            "Epoch 621/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1947 - acc: 0.9502 - val_loss: 2.4930 - val_acc: 0.6146\n",
            "Epoch 622/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1868 - acc: 0.9399 - val_loss: 2.4796 - val_acc: 0.6667\n",
            "Epoch 623/1000\n",
            "37/37 [==============================] - 3s 80ms/step - loss: 0.1779 - acc: 0.9502 - val_loss: 2.6328 - val_acc: 0.6562\n",
            "Epoch 624/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1263 - acc: 0.9588 - val_loss: 2.0471 - val_acc: 0.6875\n",
            "Epoch 625/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 0.1421 - acc: 0.9502 - val_loss: 1.9277 - val_acc: 0.6042\n",
            "Epoch 626/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.1975 - acc: 0.9485 - val_loss: 1.8533 - val_acc: 0.6146\n",
            "Epoch 627/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.1423 - acc: 0.9639 - val_loss: 1.8495 - val_acc: 0.6667\n",
            "Epoch 628/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.1311 - acc: 0.9553 - val_loss: 1.7263 - val_acc: 0.6875\n",
            "Epoch 629/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.2238 - acc: 0.9450 - val_loss: 2.2689 - val_acc: 0.6354\n",
            "Epoch 630/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.2324 - acc: 0.9433 - val_loss: 2.1626 - val_acc: 0.6771\n",
            "Epoch 631/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1750 - acc: 0.9553 - val_loss: 2.4368 - val_acc: 0.6354\n",
            "Epoch 632/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.1482 - acc: 0.9570 - val_loss: 2.0729 - val_acc: 0.6562\n",
            "Epoch 633/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1583 - acc: 0.9433 - val_loss: 1.9837 - val_acc: 0.6250\n",
            "Epoch 634/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1355 - acc: 0.9674 - val_loss: 1.7266 - val_acc: 0.7083\n",
            "Epoch 635/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.1236 - acc: 0.9502 - val_loss: 1.6278 - val_acc: 0.6875\n",
            "Epoch 636/1000\n",
            "37/37 [==============================] - 8s 223ms/step - loss: 0.1489 - acc: 0.9502 - val_loss: 2.0069 - val_acc: 0.6250\n",
            "Epoch 637/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.1532 - acc: 0.9519 - val_loss: 1.9475 - val_acc: 0.6875\n",
            "Epoch 638/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.2194 - acc: 0.9433 - val_loss: 1.6952 - val_acc: 0.6979\n",
            "Epoch 639/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1834 - acc: 0.9450 - val_loss: 1.7314 - val_acc: 0.6979\n",
            "Epoch 640/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1365 - acc: 0.9433 - val_loss: 1.4575 - val_acc: 0.6979\n",
            "Epoch 641/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.1388 - acc: 0.9622 - val_loss: 1.4843 - val_acc: 0.7500\n",
            "Epoch 642/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.1769 - acc: 0.9364 - val_loss: 1.6892 - val_acc: 0.7188\n",
            "Epoch 643/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.1927 - acc: 0.9467 - val_loss: 2.3515 - val_acc: 0.6458\n",
            "Epoch 644/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.1663 - acc: 0.9553 - val_loss: 1.9526 - val_acc: 0.6875\n",
            "Epoch 645/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1051 - acc: 0.9656 - val_loss: 2.3114 - val_acc: 0.6875\n",
            "Epoch 646/1000\n",
            "37/37 [==============================] - 9s 219ms/step - loss: 0.1200 - acc: 0.9605 - val_loss: 2.0292 - val_acc: 0.7604\n",
            "Epoch 647/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.1702 - acc: 0.9622 - val_loss: 2.8094 - val_acc: 0.6875\n",
            "Epoch 648/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.1075 - acc: 0.9605 - val_loss: 2.5667 - val_acc: 0.6979\n",
            "Epoch 649/1000\n",
            "37/37 [==============================] - 5s 101ms/step - loss: 0.2496 - acc: 0.9459 - val_loss: 2.3737 - val_acc: 0.7188\n",
            "Epoch 650/1000\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.1616 - acc: 0.9467 - val_loss: 2.5996 - val_acc: 0.6667\n",
            "Epoch 651/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.1597 - acc: 0.9450 - val_loss: 2.7376 - val_acc: 0.6771\n",
            "Epoch 652/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.0948 - acc: 0.9759 - val_loss: 2.9398 - val_acc: 0.6875\n",
            "Epoch 653/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1332 - acc: 0.9622 - val_loss: 2.5466 - val_acc: 0.6771\n",
            "Epoch 654/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.1903 - acc: 0.9433 - val_loss: 2.0083 - val_acc: 0.6771\n",
            "Epoch 655/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1022 - acc: 0.9605 - val_loss: 1.9804 - val_acc: 0.6562\n",
            "Epoch 656/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.2094 - acc: 0.9381 - val_loss: 1.8528 - val_acc: 0.6771\n",
            "Epoch 657/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 0.1445 - acc: 0.9605 - val_loss: 1.5246 - val_acc: 0.7083\n",
            "Epoch 658/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1617 - acc: 0.9502 - val_loss: 1.7781 - val_acc: 0.7188\n",
            "Epoch 659/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.1072 - acc: 0.9639 - val_loss: 1.6868 - val_acc: 0.7188\n",
            "Epoch 660/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1436 - acc: 0.9536 - val_loss: 1.7305 - val_acc: 0.6771\n",
            "Epoch 661/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1793 - acc: 0.9605 - val_loss: 1.7693 - val_acc: 0.6979\n",
            "Epoch 662/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.2204 - acc: 0.9399 - val_loss: 1.5026 - val_acc: 0.7083\n",
            "Epoch 663/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.1362 - acc: 0.9570 - val_loss: 1.4607 - val_acc: 0.6875\n",
            "Epoch 664/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.1870 - acc: 0.9519 - val_loss: 1.2657 - val_acc: 0.7083\n",
            "Epoch 665/1000\n",
            "37/37 [==============================] - 10s 249ms/step - loss: 0.1780 - acc: 0.9519 - val_loss: 1.9485 - val_acc: 0.6458\n",
            "Epoch 666/1000\n",
            "37/37 [==============================] - 8s 223ms/step - loss: 0.1047 - acc: 0.9656 - val_loss: 1.5529 - val_acc: 0.6979\n",
            "Epoch 667/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.1751 - acc: 0.9467 - val_loss: 2.1353 - val_acc: 0.6667\n",
            "Epoch 668/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.2109 - acc: 0.9399 - val_loss: 1.4155 - val_acc: 0.6979\n",
            "Epoch 669/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.1588 - acc: 0.9450 - val_loss: 1.1592 - val_acc: 0.7292\n",
            "Epoch 670/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1892 - acc: 0.9519 - val_loss: 1.6536 - val_acc: 0.6458\n",
            "Epoch 671/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1866 - acc: 0.9519 - val_loss: 1.5698 - val_acc: 0.6875\n",
            "Epoch 672/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1964 - acc: 0.9433 - val_loss: 1.4865 - val_acc: 0.6979\n",
            "Epoch 673/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.2259 - acc: 0.9399 - val_loss: 1.6647 - val_acc: 0.6458\n",
            "Epoch 674/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.0998 - acc: 0.9639 - val_loss: 1.8332 - val_acc: 0.6146\n",
            "Epoch 675/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.1843 - acc: 0.9467 - val_loss: 1.5473 - val_acc: 0.7083\n",
            "Epoch 676/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.2549 - acc: 0.9244 - val_loss: 1.8927 - val_acc: 0.7188\n",
            "Epoch 677/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1313 - acc: 0.9639 - val_loss: 2.5975 - val_acc: 0.6875\n",
            "Epoch 678/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1118 - acc: 0.9639 - val_loss: 1.8182 - val_acc: 0.6667\n",
            "Epoch 679/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.1571 - acc: 0.9459 - val_loss: 1.9159 - val_acc: 0.6250\n",
            "Epoch 680/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 0.1707 - acc: 0.9588 - val_loss: 1.5664 - val_acc: 0.6250\n",
            "Epoch 681/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1210 - acc: 0.9622 - val_loss: 2.5902 - val_acc: 0.4896\n",
            "Epoch 682/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1330 - acc: 0.9639 - val_loss: 2.1559 - val_acc: 0.6042\n",
            "Epoch 683/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.0900 - acc: 0.9691 - val_loss: 2.3740 - val_acc: 0.6146\n",
            "Epoch 684/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1532 - acc: 0.9493 - val_loss: 2.5397 - val_acc: 0.6667\n",
            "Epoch 685/1000\n",
            "37/37 [==============================] - 5s 118ms/step - loss: 0.1007 - acc: 0.9656 - val_loss: 2.3664 - val_acc: 0.6875\n",
            "Epoch 686/1000\n",
            "37/37 [==============================] - 5s 105ms/step - loss: 0.1516 - acc: 0.9502 - val_loss: 2.1731 - val_acc: 0.6771\n",
            "Epoch 687/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.1543 - acc: 0.9450 - val_loss: 2.3198 - val_acc: 0.6667\n",
            "Epoch 688/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.1939 - acc: 0.9399 - val_loss: 1.8993 - val_acc: 0.6562\n",
            "Epoch 689/1000\n",
            "37/37 [==============================] - 8s 214ms/step - loss: 0.2323 - acc: 0.9416 - val_loss: 2.1298 - val_acc: 0.6875\n",
            "Epoch 690/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.1201 - acc: 0.9656 - val_loss: 1.9782 - val_acc: 0.7083\n",
            "Epoch 691/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1952 - acc: 0.9485 - val_loss: 2.0454 - val_acc: 0.6250\n",
            "Epoch 692/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.1628 - acc: 0.9519 - val_loss: 1.8140 - val_acc: 0.6771\n",
            "Epoch 693/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.2100 - acc: 0.9450 - val_loss: 2.1446 - val_acc: 0.6458\n",
            "Epoch 694/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.1136 - acc: 0.9622 - val_loss: 2.0577 - val_acc: 0.6667\n",
            "Epoch 695/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.1379 - acc: 0.9588 - val_loss: 2.1573 - val_acc: 0.6667\n",
            "Epoch 696/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.1202 - acc: 0.9605 - val_loss: 2.1828 - val_acc: 0.6771\n",
            "Epoch 697/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.1594 - acc: 0.9502 - val_loss: 2.1342 - val_acc: 0.6562\n",
            "Epoch 698/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 0.1725 - acc: 0.9450 - val_loss: 1.6259 - val_acc: 0.6667\n",
            "Epoch 699/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1566 - acc: 0.9553 - val_loss: 2.2323 - val_acc: 0.6667\n",
            "Epoch 700/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1815 - acc: 0.9416 - val_loss: 1.9661 - val_acc: 0.6562\n",
            "Epoch 701/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1188 - acc: 0.9656 - val_loss: 2.0904 - val_acc: 0.6667\n",
            "Epoch 702/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1932 - acc: 0.9467 - val_loss: 2.1005 - val_acc: 0.6146\n",
            "Epoch 703/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.1683 - acc: 0.9570 - val_loss: 1.7752 - val_acc: 0.7083\n",
            "Epoch 704/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1646 - acc: 0.9536 - val_loss: 2.0471 - val_acc: 0.6458\n",
            "Epoch 705/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1474 - acc: 0.9553 - val_loss: 1.4402 - val_acc: 0.6875\n",
            "Epoch 706/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.1695 - acc: 0.9570 - val_loss: 1.7553 - val_acc: 0.7083\n",
            "Epoch 707/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 0.1288 - acc: 0.9622 - val_loss: 1.3927 - val_acc: 0.7292\n",
            "Epoch 708/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.2144 - acc: 0.9433 - val_loss: 1.5533 - val_acc: 0.6979\n",
            "Epoch 709/1000\n",
            "37/37 [==============================] - 5s 104ms/step - loss: 0.2358 - acc: 0.9502 - val_loss: 1.3616 - val_acc: 0.6562\n",
            "Epoch 710/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1378 - acc: 0.9553 - val_loss: 2.0356 - val_acc: 0.6146\n",
            "Epoch 711/1000\n",
            "37/37 [==============================] - 9s 244ms/step - loss: 0.2342 - acc: 0.9536 - val_loss: 1.8332 - val_acc: 0.6458\n",
            "Epoch 712/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.2238 - acc: 0.9605 - val_loss: 1.6468 - val_acc: 0.7396\n",
            "Epoch 713/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.1504 - acc: 0.9536 - val_loss: 1.3401 - val_acc: 0.7083\n",
            "Epoch 714/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1861 - acc: 0.9570 - val_loss: 1.8375 - val_acc: 0.6354\n",
            "Epoch 715/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1435 - acc: 0.9595 - val_loss: 2.0433 - val_acc: 0.6250\n",
            "Epoch 716/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1837 - acc: 0.9399 - val_loss: 1.9835 - val_acc: 0.5938\n",
            "Epoch 717/1000\n",
            "37/37 [==============================] - 10s 239ms/step - loss: 0.1640 - acc: 0.9611 - val_loss: 2.0879 - val_acc: 0.6562\n",
            "Epoch 718/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.0951 - acc: 0.9708 - val_loss: 2.0873 - val_acc: 0.6562\n",
            "Epoch 719/1000\n",
            "37/37 [==============================] - 8s 213ms/step - loss: 0.1404 - acc: 0.9588 - val_loss: 2.2774 - val_acc: 0.5833\n",
            "Epoch 720/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.1745 - acc: 0.9605 - val_loss: 2.4346 - val_acc: 0.6562\n",
            "Epoch 721/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.1564 - acc: 0.9536 - val_loss: 2.1506 - val_acc: 0.5729\n",
            "Epoch 722/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.1425 - acc: 0.9570 - val_loss: 2.2746 - val_acc: 0.5833\n",
            "Epoch 723/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1245 - acc: 0.9570 - val_loss: 2.6220 - val_acc: 0.5208\n",
            "Epoch 724/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.0870 - acc: 0.9730 - val_loss: 2.2068 - val_acc: 0.5729\n",
            "Epoch 725/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1220 - acc: 0.9674 - val_loss: 2.7161 - val_acc: 0.6354\n",
            "Epoch 726/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.1569 - acc: 0.9485 - val_loss: 2.5577 - val_acc: 0.6354\n",
            "Epoch 727/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.1748 - acc: 0.9364 - val_loss: 2.0773 - val_acc: 0.6562\n",
            "Epoch 728/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.1069 - acc: 0.9691 - val_loss: 1.8260 - val_acc: 0.6146\n",
            "Epoch 729/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.1245 - acc: 0.9570 - val_loss: 2.2069 - val_acc: 0.5938\n",
            "Epoch 730/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.1416 - acc: 0.9519 - val_loss: 1.9197 - val_acc: 0.6354\n",
            "Epoch 731/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1437 - acc: 0.9502 - val_loss: 1.9419 - val_acc: 0.6146\n",
            "Epoch 732/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.1130 - acc: 0.9708 - val_loss: 2.1117 - val_acc: 0.6667\n",
            "Epoch 733/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.1971 - acc: 0.9450 - val_loss: 2.7791 - val_acc: 0.6667\n",
            "Epoch 734/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.1450 - acc: 0.9588 - val_loss: 2.6871 - val_acc: 0.6771\n",
            "Epoch 735/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.1043 - acc: 0.9691 - val_loss: 2.6990 - val_acc: 0.5938\n",
            "Epoch 736/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.1421 - acc: 0.9588 - val_loss: 2.5923 - val_acc: 0.6667\n",
            "Epoch 737/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.1072 - acc: 0.9605 - val_loss: 2.0817 - val_acc: 0.6771\n",
            "Epoch 738/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.1485 - acc: 0.9570 - val_loss: 1.6646 - val_acc: 0.6562\n",
            "Epoch 739/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.0759 - acc: 0.9725 - val_loss: 1.5343 - val_acc: 0.7292\n",
            "Epoch 740/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.1795 - acc: 0.9502 - val_loss: 1.8537 - val_acc: 0.6458\n",
            "Epoch 741/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1458 - acc: 0.9605 - val_loss: 1.4469 - val_acc: 0.6979\n",
            "Epoch 742/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.1462 - acc: 0.9553 - val_loss: 2.1261 - val_acc: 0.6562\n",
            "Epoch 743/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1864 - acc: 0.9502 - val_loss: 2.0227 - val_acc: 0.5833\n",
            "Epoch 744/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1873 - acc: 0.9519 - val_loss: 2.2967 - val_acc: 0.6667\n",
            "Epoch 745/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.1118 - acc: 0.9622 - val_loss: 2.3654 - val_acc: 0.6667\n",
            "Epoch 746/1000\n",
            "37/37 [==============================] - 3s 83ms/step - loss: 0.0831 - acc: 0.9759 - val_loss: 2.0148 - val_acc: 0.6146\n",
            "Epoch 747/1000\n",
            "37/37 [==============================] - 5s 118ms/step - loss: 0.2026 - acc: 0.9553 - val_loss: 1.5838 - val_acc: 0.6250\n",
            "Epoch 748/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1820 - acc: 0.9467 - val_loss: 1.8733 - val_acc: 0.6875\n",
            "Epoch 749/1000\n",
            "37/37 [==============================] - 9s 239ms/step - loss: 0.1522 - acc: 0.9605 - val_loss: 2.3460 - val_acc: 0.6562\n",
            "Epoch 750/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1718 - acc: 0.9399 - val_loss: 2.6995 - val_acc: 0.6354\n",
            "Epoch 751/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.1476 - acc: 0.9605 - val_loss: 2.4967 - val_acc: 0.6146\n",
            "Epoch 752/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.1367 - acc: 0.9519 - val_loss: 1.9636 - val_acc: 0.6354\n",
            "Epoch 753/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1521 - acc: 0.9450 - val_loss: 1.9980 - val_acc: 0.6250\n",
            "Epoch 754/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.1411 - acc: 0.9553 - val_loss: 1.8304 - val_acc: 0.6667\n",
            "Epoch 755/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.1713 - acc: 0.9450 - val_loss: 2.8711 - val_acc: 0.6667\n",
            "Epoch 756/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 0.1508 - acc: 0.9553 - val_loss: 2.2358 - val_acc: 0.5312\n",
            "Epoch 757/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1703 - acc: 0.9622 - val_loss: 1.7664 - val_acc: 0.7083\n",
            "Epoch 758/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.1817 - acc: 0.9502 - val_loss: 1.7746 - val_acc: 0.6875\n",
            "Epoch 759/1000\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.1867 - acc: 0.9502 - val_loss: 1.8468 - val_acc: 0.7083\n",
            "Epoch 760/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1854 - acc: 0.9536 - val_loss: 2.9120 - val_acc: 0.6354\n",
            "Epoch 761/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.1082 - acc: 0.9759 - val_loss: 1.8118 - val_acc: 0.6875\n",
            "Epoch 762/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.1574 - acc: 0.9467 - val_loss: 2.0875 - val_acc: 0.7188\n",
            "Epoch 763/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.1242 - acc: 0.9622 - val_loss: 2.1893 - val_acc: 0.6979\n",
            "Epoch 764/1000\n",
            "37/37 [==============================] - 9s 245ms/step - loss: 0.1704 - acc: 0.9450 - val_loss: 2.1485 - val_acc: 0.6458\n",
            "Epoch 765/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1160 - acc: 0.9691 - val_loss: 2.2134 - val_acc: 0.6562\n",
            "Epoch 766/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1359 - acc: 0.9656 - val_loss: 1.5521 - val_acc: 0.6354\n",
            "Epoch 767/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.1086 - acc: 0.9674 - val_loss: 2.5710 - val_acc: 0.5000\n",
            "Epoch 768/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.1449 - acc: 0.9502 - val_loss: 2.2172 - val_acc: 0.6562\n",
            "Epoch 769/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 0.1315 - acc: 0.9639 - val_loss: 2.2691 - val_acc: 0.6771\n",
            "Epoch 770/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.1752 - acc: 0.9570 - val_loss: 1.8562 - val_acc: 0.6771\n",
            "Epoch 771/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1295 - acc: 0.9622 - val_loss: 2.1006 - val_acc: 0.6979\n",
            "Epoch 772/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.0828 - acc: 0.9759 - val_loss: 2.2608 - val_acc: 0.6458\n",
            "Epoch 773/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1372 - acc: 0.9588 - val_loss: 2.3035 - val_acc: 0.6979\n",
            "Epoch 774/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.1270 - acc: 0.9691 - val_loss: 2.2309 - val_acc: 0.6458\n",
            "Epoch 775/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1651 - acc: 0.9588 - val_loss: 1.9313 - val_acc: 0.6458\n",
            "Epoch 776/1000\n",
            "37/37 [==============================] - 3s 82ms/step - loss: 0.1166 - acc: 0.9656 - val_loss: 2.0350 - val_acc: 0.5833\n",
            "Epoch 777/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2350 - acc: 0.9416 - val_loss: 2.1422 - val_acc: 0.6458\n",
            "Epoch 778/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.0829 - acc: 0.9708 - val_loss: 2.4316 - val_acc: 0.6146\n",
            "Epoch 779/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.1158 - acc: 0.9639 - val_loss: 2.2083 - val_acc: 0.6771\n",
            "Epoch 780/1000\n",
            "37/37 [==============================] - 7s 159ms/step - loss: 0.2015 - acc: 0.9467 - val_loss: 1.8903 - val_acc: 0.6771\n",
            "Epoch 781/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.2277 - acc: 0.9364 - val_loss: 2.5783 - val_acc: 0.5312\n",
            "Epoch 782/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.0951 - acc: 0.9674 - val_loss: 1.8878 - val_acc: 0.5938\n",
            "Epoch 783/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1359 - acc: 0.9605 - val_loss: 2.0698 - val_acc: 0.6250\n",
            "Epoch 784/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1542 - acc: 0.9536 - val_loss: 2.1861 - val_acc: 0.6354\n",
            "Epoch 785/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1857 - acc: 0.9459 - val_loss: 1.9813 - val_acc: 0.6562\n",
            "Epoch 786/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1496 - acc: 0.9519 - val_loss: 1.9749 - val_acc: 0.6562\n",
            "Epoch 787/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1404 - acc: 0.9485 - val_loss: 2.9527 - val_acc: 0.6354\n",
            "Epoch 788/1000\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 0.1035 - acc: 0.9639 - val_loss: 2.7508 - val_acc: 0.6458\n",
            "Epoch 789/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.1190 - acc: 0.9553 - val_loss: 2.4688 - val_acc: 0.6250\n",
            "Epoch 790/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 0.1736 - acc: 0.9553 - val_loss: 2.0789 - val_acc: 0.6354\n",
            "Epoch 791/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.0898 - acc: 0.9674 - val_loss: 2.6621 - val_acc: 0.6458\n",
            "Epoch 792/1000\n",
            "37/37 [==============================] - 4s 99ms/step - loss: 0.1125 - acc: 0.9605 - val_loss: 2.0567 - val_acc: 0.6771\n",
            "Epoch 793/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1678 - acc: 0.9588 - val_loss: 1.7472 - val_acc: 0.6354\n",
            "Epoch 794/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.2436 - acc: 0.9450 - val_loss: 2.1492 - val_acc: 0.6354\n",
            "Epoch 795/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1294 - acc: 0.9570 - val_loss: 2.2255 - val_acc: 0.6354\n",
            "Epoch 796/1000\n",
            "37/37 [==============================] - 10s 238ms/step - loss: 0.1190 - acc: 0.9674 - val_loss: 2.2159 - val_acc: 0.6562\n",
            "Epoch 797/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.1482 - acc: 0.9485 - val_loss: 2.2062 - val_acc: 0.6354\n",
            "Epoch 798/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.1273 - acc: 0.9570 - val_loss: 2.2240 - val_acc: 0.6458\n",
            "Epoch 799/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1537 - acc: 0.9519 - val_loss: 2.2781 - val_acc: 0.6562\n",
            "Epoch 800/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.1515 - acc: 0.9588 - val_loss: 2.2190 - val_acc: 0.6979\n",
            "Epoch 801/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.1040 - acc: 0.9674 - val_loss: 1.8128 - val_acc: 0.6667\n",
            "Epoch 802/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1292 - acc: 0.9570 - val_loss: 1.9510 - val_acc: 0.6667\n",
            "Epoch 803/1000\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 0.1158 - acc: 0.9588 - val_loss: 2.0720 - val_acc: 0.6458\n",
            "Epoch 804/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.1340 - acc: 0.9674 - val_loss: 2.2282 - val_acc: 0.6562\n",
            "Epoch 805/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.2047 - acc: 0.9485 - val_loss: 1.7457 - val_acc: 0.6667\n",
            "Epoch 806/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.0993 - acc: 0.9656 - val_loss: 1.6035 - val_acc: 0.6875\n",
            "Epoch 807/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1006 - acc: 0.9605 - val_loss: 1.8192 - val_acc: 0.6354\n",
            "Epoch 808/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1707 - acc: 0.9639 - val_loss: 2.9898 - val_acc: 0.5833\n",
            "Epoch 809/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.2189 - acc: 0.9536 - val_loss: 2.5264 - val_acc: 0.5938\n",
            "Epoch 810/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1298 - acc: 0.9759 - val_loss: 3.3431 - val_acc: 0.6146\n",
            "Epoch 811/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.2273 - acc: 0.9553 - val_loss: 2.8545 - val_acc: 0.6458\n",
            "Epoch 812/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 0.1415 - acc: 0.9588 - val_loss: 2.2266 - val_acc: 0.6146\n",
            "Epoch 813/1000\n",
            "37/37 [==============================] - 9s 234ms/step - loss: 0.1394 - acc: 0.9536 - val_loss: 2.1522 - val_acc: 0.6667\n",
            "Epoch 814/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1209 - acc: 0.9536 - val_loss: 2.2397 - val_acc: 0.5833\n",
            "Epoch 815/1000\n",
            "37/37 [==============================] - 5s 119ms/step - loss: 0.1491 - acc: 0.9536 - val_loss: 2.1922 - val_acc: 0.5625\n",
            "Epoch 816/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.1460 - acc: 0.9493 - val_loss: 1.8363 - val_acc: 0.6562\n",
            "Epoch 817/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.2563 - acc: 0.9381 - val_loss: 1.9583 - val_acc: 0.5625\n",
            "Epoch 818/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1126 - acc: 0.9656 - val_loss: 1.9428 - val_acc: 0.6250\n",
            "Epoch 819/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.1192 - acc: 0.9622 - val_loss: 2.0180 - val_acc: 0.5938\n",
            "Epoch 820/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1330 - acc: 0.9588 - val_loss: 1.9549 - val_acc: 0.5833\n",
            "Epoch 821/1000\n",
            "37/37 [==============================] - 4s 85ms/step - loss: 0.1897 - acc: 0.9553 - val_loss: 2.0128 - val_acc: 0.6562\n",
            "Epoch 822/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1276 - acc: 0.9605 - val_loss: 1.9084 - val_acc: 0.6771\n",
            "Epoch 823/1000\n",
            "37/37 [==============================] - 5s 116ms/step - loss: 0.1413 - acc: 0.9502 - val_loss: 1.6331 - val_acc: 0.5938\n",
            "Epoch 824/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.1881 - acc: 0.9347 - val_loss: 1.9458 - val_acc: 0.5625\n",
            "Epoch 825/1000\n",
            "37/37 [==============================] - 8s 222ms/step - loss: 0.1734 - acc: 0.9674 - val_loss: 1.9578 - val_acc: 0.6354\n",
            "Epoch 826/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.1101 - acc: 0.9553 - val_loss: 1.7912 - val_acc: 0.6458\n",
            "Epoch 827/1000\n",
            "37/37 [==============================] - 8s 215ms/step - loss: 0.1721 - acc: 0.9553 - val_loss: 2.1249 - val_acc: 0.6250\n",
            "Epoch 828/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1523 - acc: 0.9467 - val_loss: 1.9115 - val_acc: 0.5417\n",
            "Epoch 829/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.1607 - acc: 0.9588 - val_loss: 2.2999 - val_acc: 0.6562\n",
            "Epoch 830/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1190 - acc: 0.9588 - val_loss: 2.0405 - val_acc: 0.5938\n",
            "Epoch 831/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1849 - acc: 0.9588 - val_loss: 1.8717 - val_acc: 0.6354\n",
            "Epoch 832/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.0814 - acc: 0.9588 - val_loss: 1.8865 - val_acc: 0.6771\n",
            "Epoch 833/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1723 - acc: 0.9536 - val_loss: 2.1096 - val_acc: 0.6250\n",
            "Epoch 834/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1485 - acc: 0.9553 - val_loss: 1.9474 - val_acc: 0.5833\n",
            "Epoch 835/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.0829 - acc: 0.9725 - val_loss: 1.9411 - val_acc: 0.6458\n",
            "Epoch 836/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.1595 - acc: 0.9467 - val_loss: 1.8553 - val_acc: 0.6771\n",
            "Epoch 837/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.1637 - acc: 0.9519 - val_loss: 1.8387 - val_acc: 0.6354\n",
            "Epoch 838/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1492 - acc: 0.9467 - val_loss: 1.9650 - val_acc: 0.6146\n",
            "Epoch 839/1000\n",
            "37/37 [==============================] - 4s 93ms/step - loss: 0.1451 - acc: 0.9519 - val_loss: 2.3098 - val_acc: 0.5208\n",
            "Epoch 840/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.1714 - acc: 0.9467 - val_loss: 2.1220 - val_acc: 0.6771\n",
            "Epoch 841/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1746 - acc: 0.9570 - val_loss: 2.4811 - val_acc: 0.6146\n",
            "Epoch 842/1000\n",
            "37/37 [==============================] - 9s 247ms/step - loss: 0.1714 - acc: 0.9578 - val_loss: 2.3875 - val_acc: 0.6667\n",
            "Epoch 843/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.1947 - acc: 0.9519 - val_loss: 1.7095 - val_acc: 0.6562\n",
            "Epoch 844/1000\n",
            "37/37 [==============================] - 10s 241ms/step - loss: 0.1936 - acc: 0.9588 - val_loss: 1.5196 - val_acc: 0.6771\n",
            "Epoch 845/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1308 - acc: 0.9622 - val_loss: 1.8627 - val_acc: 0.6771\n",
            "Epoch 846/1000\n",
            "37/37 [==============================] - 8s 216ms/step - loss: 0.1378 - acc: 0.9605 - val_loss: 1.9376 - val_acc: 0.5417\n",
            "Epoch 847/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.0993 - acc: 0.9674 - val_loss: 1.9847 - val_acc: 0.6562\n",
            "Epoch 848/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.1391 - acc: 0.9553 - val_loss: 2.6936 - val_acc: 0.6458\n",
            "Epoch 849/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.0788 - acc: 0.9708 - val_loss: 2.1774 - val_acc: 0.5833\n",
            "Epoch 850/1000\n",
            "37/37 [==============================] - 4s 83ms/step - loss: 0.1552 - acc: 0.9536 - val_loss: 2.3882 - val_acc: 0.6042\n",
            "Epoch 851/1000\n",
            "37/37 [==============================] - 10s 245ms/step - loss: 0.2074 - acc: 0.9536 - val_loss: 2.6478 - val_acc: 0.6146\n",
            "Epoch 852/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1200 - acc: 0.9639 - val_loss: 1.9763 - val_acc: 0.6250\n",
            "Epoch 853/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.0899 - acc: 0.9759 - val_loss: 2.2738 - val_acc: 0.5521\n",
            "Epoch 854/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1364 - acc: 0.9656 - val_loss: 2.1637 - val_acc: 0.6146\n",
            "Epoch 855/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.1559 - acc: 0.9570 - val_loss: 2.1047 - val_acc: 0.6354\n",
            "Epoch 856/1000\n",
            "37/37 [==============================] - 9s 247ms/step - loss: 0.1295 - acc: 0.9605 - val_loss: 2.2428 - val_acc: 0.6562\n",
            "Epoch 857/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1935 - acc: 0.9588 - val_loss: 2.3152 - val_acc: 0.6042\n",
            "Epoch 858/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.1188 - acc: 0.9605 - val_loss: 1.9058 - val_acc: 0.6250\n",
            "Epoch 859/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 0.1408 - acc: 0.9553 - val_loss: 1.7142 - val_acc: 0.6667\n",
            "Epoch 860/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1418 - acc: 0.9536 - val_loss: 1.9450 - val_acc: 0.5625\n",
            "Epoch 861/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1729 - acc: 0.9553 - val_loss: 1.6611 - val_acc: 0.6250\n",
            "Epoch 862/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.1644 - acc: 0.9570 - val_loss: 1.8645 - val_acc: 0.6354\n",
            "Epoch 863/1000\n",
            "37/37 [==============================] - 4s 87ms/step - loss: 0.2162 - acc: 0.9416 - val_loss: 2.0594 - val_acc: 0.5833\n",
            "Epoch 864/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.1397 - acc: 0.9570 - val_loss: 2.3193 - val_acc: 0.6354\n",
            "Epoch 865/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.1825 - acc: 0.9502 - val_loss: 1.8874 - val_acc: 0.6354\n",
            "Epoch 866/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1432 - acc: 0.9502 - val_loss: 1.8157 - val_acc: 0.6979\n",
            "Epoch 867/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.1105 - acc: 0.9622 - val_loss: 2.4986 - val_acc: 0.6250\n",
            "Epoch 868/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1372 - acc: 0.9622 - val_loss: 2.5049 - val_acc: 0.6042\n",
            "Epoch 869/1000\n",
            "37/37 [==============================] - 9s 243ms/step - loss: 0.1400 - acc: 0.9536 - val_loss: 2.1259 - val_acc: 0.6562\n",
            "Epoch 870/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.1088 - acc: 0.9656 - val_loss: 1.9374 - val_acc: 0.6979\n",
            "Epoch 871/1000\n",
            "37/37 [==============================] - 9s 248ms/step - loss: 0.2001 - acc: 0.9485 - val_loss: 2.0160 - val_acc: 0.6667\n",
            "Epoch 872/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.0624 - acc: 0.9794 - val_loss: 2.0135 - val_acc: 0.6562\n",
            "Epoch 873/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.1208 - acc: 0.9536 - val_loss: 2.0353 - val_acc: 0.6042\n",
            "Epoch 874/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.1531 - acc: 0.9605 - val_loss: 1.9426 - val_acc: 0.6250\n",
            "Epoch 875/1000\n",
            "37/37 [==============================] - 9s 231ms/step - loss: 0.1749 - acc: 0.9502 - val_loss: 2.0982 - val_acc: 0.5938\n",
            "Epoch 876/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1250 - acc: 0.9674 - val_loss: 2.3459 - val_acc: 0.6042\n",
            "Epoch 877/1000\n",
            "37/37 [==============================] - 4s 95ms/step - loss: 0.1328 - acc: 0.9570 - val_loss: 2.3775 - val_acc: 0.6042\n",
            "Epoch 878/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1862 - acc: 0.9433 - val_loss: 2.9902 - val_acc: 0.6146\n",
            "Epoch 879/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1219 - acc: 0.9502 - val_loss: 2.3025 - val_acc: 0.6042\n",
            "Epoch 880/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.1310 - acc: 0.9605 - val_loss: 3.5830 - val_acc: 0.5104\n",
            "Epoch 881/1000\n",
            "37/37 [==============================] - 4s 91ms/step - loss: 0.1979 - acc: 0.9433 - val_loss: 3.4315 - val_acc: 0.5312\n",
            "Epoch 882/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1413 - acc: 0.9536 - val_loss: 3.1219 - val_acc: 0.5000\n",
            "Epoch 883/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1925 - acc: 0.9381 - val_loss: 2.1769 - val_acc: 0.6354\n",
            "Epoch 884/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.0905 - acc: 0.9777 - val_loss: 2.6959 - val_acc: 0.6250\n",
            "Epoch 885/1000\n",
            "37/37 [==============================] - 4s 92ms/step - loss: 0.0769 - acc: 0.9588 - val_loss: 2.8226 - val_acc: 0.6354\n",
            "Epoch 886/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.1413 - acc: 0.9570 - val_loss: 2.1096 - val_acc: 0.6458\n",
            "Epoch 887/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.1249 - acc: 0.9674 - val_loss: 2.7383 - val_acc: 0.6042\n",
            "Epoch 888/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 0.1557 - acc: 0.9622 - val_loss: 2.8264 - val_acc: 0.5104\n",
            "Epoch 889/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1379 - acc: 0.9639 - val_loss: 2.3214 - val_acc: 0.6146\n",
            "Epoch 890/1000\n",
            "37/37 [==============================] - 9s 247ms/step - loss: 0.1922 - acc: 0.9553 - val_loss: 2.4991 - val_acc: 0.6042\n",
            "Epoch 891/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.1098 - acc: 0.9605 - val_loss: 2.4927 - val_acc: 0.5938\n",
            "Epoch 892/1000\n",
            "37/37 [==============================] - 4s 110ms/step - loss: 0.0928 - acc: 0.9639 - val_loss: 2.0279 - val_acc: 0.6562\n",
            "Epoch 893/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.1584 - acc: 0.9502 - val_loss: 1.7263 - val_acc: 0.7604\n",
            "Epoch 894/1000\n",
            "37/37 [==============================] - 5s 110ms/step - loss: 0.1139 - acc: 0.9708 - val_loss: 2.2687 - val_acc: 0.6979\n",
            "Epoch 895/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1058 - acc: 0.9639 - val_loss: 1.9238 - val_acc: 0.6771\n",
            "Epoch 896/1000\n",
            "37/37 [==============================] - 4s 90ms/step - loss: 0.1498 - acc: 0.9553 - val_loss: 1.8554 - val_acc: 0.6979\n",
            "Epoch 897/1000\n",
            "37/37 [==============================] - 9s 217ms/step - loss: 0.0774 - acc: 0.9777 - val_loss: 1.8840 - val_acc: 0.6250\n",
            "Epoch 898/1000\n",
            "37/37 [==============================] - 9s 218ms/step - loss: 0.1172 - acc: 0.9588 - val_loss: 2.0262 - val_acc: 0.6562\n",
            "Epoch 899/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.0763 - acc: 0.9759 - val_loss: 2.0976 - val_acc: 0.6562\n",
            "Epoch 900/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1985 - acc: 0.9519 - val_loss: 1.9098 - val_acc: 0.6979\n",
            "Epoch 901/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.1784 - acc: 0.9622 - val_loss: 2.0294 - val_acc: 0.6562\n",
            "Epoch 902/1000\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.2300 - acc: 0.9467 - val_loss: 2.2531 - val_acc: 0.6354\n",
            "Epoch 903/1000\n",
            "37/37 [==============================] - 5s 106ms/step - loss: 0.1194 - acc: 0.9674 - val_loss: 2.2393 - val_acc: 0.6562\n",
            "Epoch 904/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.1413 - acc: 0.9611 - val_loss: 2.3990 - val_acc: 0.5833\n",
            "Epoch 905/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.1328 - acc: 0.9674 - val_loss: 2.0621 - val_acc: 0.6146\n",
            "Epoch 906/1000\n",
            "37/37 [==============================] - 9s 232ms/step - loss: 0.1193 - acc: 0.9605 - val_loss: 1.8933 - val_acc: 0.6875\n",
            "Epoch 907/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.1020 - acc: 0.9622 - val_loss: 2.0676 - val_acc: 0.6354\n",
            "Epoch 908/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1255 - acc: 0.9622 - val_loss: 1.8883 - val_acc: 0.6667\n",
            "Epoch 909/1000\n",
            "37/37 [==============================] - 9s 235ms/step - loss: 0.1766 - acc: 0.9485 - val_loss: 1.6318 - val_acc: 0.6979\n",
            "Epoch 910/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.1997 - acc: 0.9605 - val_loss: 2.1068 - val_acc: 0.6458\n",
            "Epoch 911/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.1064 - acc: 0.9674 - val_loss: 2.5967 - val_acc: 0.5625\n",
            "Epoch 912/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.1380 - acc: 0.9656 - val_loss: 2.3452 - val_acc: 0.6042\n",
            "Epoch 913/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.2079 - acc: 0.9553 - val_loss: 1.9824 - val_acc: 0.6458\n",
            "Epoch 914/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1226 - acc: 0.9639 - val_loss: 2.6142 - val_acc: 0.5729\n",
            "Epoch 915/1000\n",
            "37/37 [==============================] - 4s 109ms/step - loss: 0.0950 - acc: 0.9622 - val_loss: 2.1980 - val_acc: 0.5833\n",
            "Epoch 916/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.1753 - acc: 0.9519 - val_loss: 2.2147 - val_acc: 0.6042\n",
            "Epoch 917/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1318 - acc: 0.9588 - val_loss: 2.5939 - val_acc: 0.5833\n",
            "Epoch 918/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.1154 - acc: 0.9639 - val_loss: 2.1532 - val_acc: 0.5938\n",
            "Epoch 919/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1808 - acc: 0.9450 - val_loss: 2.4426 - val_acc: 0.5938\n",
            "Epoch 920/1000\n",
            "37/37 [==============================] - 9s 237ms/step - loss: 0.1523 - acc: 0.9622 - val_loss: 2.2029 - val_acc: 0.6667\n",
            "Epoch 921/1000\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.0952 - acc: 0.9691 - val_loss: 2.1505 - val_acc: 0.6667\n",
            "Epoch 922/1000\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.1671 - acc: 0.9536 - val_loss: 2.3890 - val_acc: 0.6979\n",
            "Epoch 923/1000\n",
            "37/37 [==============================] - 4s 97ms/step - loss: 0.1107 - acc: 0.9639 - val_loss: 2.3895 - val_acc: 0.6667\n",
            "Epoch 924/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1637 - acc: 0.9605 - val_loss: 2.2348 - val_acc: 0.6771\n",
            "Epoch 925/1000\n",
            "37/37 [==============================] - 9s 223ms/step - loss: 0.1014 - acc: 0.9725 - val_loss: 2.2587 - val_acc: 0.6042\n",
            "Epoch 926/1000\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.1711 - acc: 0.9639 - val_loss: 2.6652 - val_acc: 0.5417\n",
            "Epoch 927/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1536 - acc: 0.9467 - val_loss: 1.7496 - val_acc: 0.6562\n",
            "Epoch 928/1000\n",
            "37/37 [==============================] - 8s 218ms/step - loss: 0.1933 - acc: 0.9467 - val_loss: 2.7626 - val_acc: 0.5729\n",
            "Epoch 929/1000\n",
            "37/37 [==============================] - 9s 242ms/step - loss: 0.1539 - acc: 0.9639 - val_loss: 2.0600 - val_acc: 0.6667\n",
            "Epoch 930/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1659 - acc: 0.9570 - val_loss: 1.8933 - val_acc: 0.6458\n",
            "Epoch 931/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1264 - acc: 0.9656 - val_loss: 3.2802 - val_acc: 0.5104\n",
            "Epoch 932/1000\n",
            "37/37 [==============================] - 9s 236ms/step - loss: 0.1367 - acc: 0.9485 - val_loss: 1.9827 - val_acc: 0.7188\n",
            "Epoch 933/1000\n",
            "37/37 [==============================] - 4s 94ms/step - loss: 0.1542 - acc: 0.9527 - val_loss: 2.2283 - val_acc: 0.7292\n",
            "Epoch 934/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.2130 - acc: 0.9399 - val_loss: 2.3823 - val_acc: 0.6771\n",
            "Epoch 935/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1303 - acc: 0.9485 - val_loss: 2.3543 - val_acc: 0.6979\n",
            "Epoch 936/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.0739 - acc: 0.9708 - val_loss: 2.4939 - val_acc: 0.6042\n",
            "Epoch 937/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.1274 - acc: 0.9628 - val_loss: 2.8763 - val_acc: 0.6250\n",
            "Epoch 938/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.1010 - acc: 0.9628 - val_loss: 2.1933 - val_acc: 0.6771\n",
            "Epoch 939/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.1982 - acc: 0.9467 - val_loss: 2.3387 - val_acc: 0.6458\n",
            "Epoch 940/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1027 - acc: 0.9622 - val_loss: 1.8445 - val_acc: 0.6458\n",
            "Epoch 941/1000\n",
            "37/37 [==============================] - 5s 111ms/step - loss: 0.1301 - acc: 0.9553 - val_loss: 2.4535 - val_acc: 0.6562\n",
            "Epoch 942/1000\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 0.1803 - acc: 0.9467 - val_loss: 2.1795 - val_acc: 0.6458\n",
            "Epoch 943/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.1909 - acc: 0.9536 - val_loss: 2.5032 - val_acc: 0.6458\n",
            "Epoch 944/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.1408 - acc: 0.9588 - val_loss: 2.9187 - val_acc: 0.6354\n",
            "Epoch 945/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.2081 - acc: 0.9330 - val_loss: 2.4550 - val_acc: 0.5938\n",
            "Epoch 946/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.1203 - acc: 0.9656 - val_loss: 2.2436 - val_acc: 0.6354\n",
            "Epoch 947/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1223 - acc: 0.9605 - val_loss: 2.0835 - val_acc: 0.6250\n",
            "Epoch 948/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.2190 - acc: 0.9485 - val_loss: 2.1404 - val_acc: 0.6146\n",
            "Epoch 949/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1877 - acc: 0.9553 - val_loss: 2.3064 - val_acc: 0.6146\n",
            "Epoch 950/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.1061 - acc: 0.9656 - val_loss: 2.1294 - val_acc: 0.6562\n",
            "Epoch 951/1000\n",
            "37/37 [==============================] - 4s 100ms/step - loss: 0.2264 - acc: 0.9519 - val_loss: 2.0368 - val_acc: 0.6354\n",
            "Epoch 952/1000\n",
            "37/37 [==============================] - 4s 86ms/step - loss: 0.1701 - acc: 0.9536 - val_loss: 2.2628 - val_acc: 0.6250\n",
            "Epoch 953/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.1178 - acc: 0.9656 - val_loss: 2.3988 - val_acc: 0.5938\n",
            "Epoch 954/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.1100 - acc: 0.9622 - val_loss: 2.4691 - val_acc: 0.5938\n",
            "Epoch 955/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1430 - acc: 0.9656 - val_loss: 2.5518 - val_acc: 0.5729\n",
            "Epoch 956/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1481 - acc: 0.9502 - val_loss: 2.7043 - val_acc: 0.5521\n",
            "Epoch 957/1000\n",
            "37/37 [==============================] - 4s 105ms/step - loss: 0.1016 - acc: 0.9674 - val_loss: 2.5892 - val_acc: 0.6146\n",
            "Epoch 958/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1125 - acc: 0.9639 - val_loss: 2.2640 - val_acc: 0.6354\n",
            "Epoch 959/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.1770 - acc: 0.9588 - val_loss: 2.2636 - val_acc: 0.6354\n",
            "Epoch 960/1000\n",
            "37/37 [==============================] - 9s 247ms/step - loss: 0.1482 - acc: 0.9502 - val_loss: 2.3301 - val_acc: 0.5938\n",
            "Epoch 961/1000\n",
            "37/37 [==============================] - 4s 88ms/step - loss: 0.1537 - acc: 0.9639 - val_loss: 2.5894 - val_acc: 0.6667\n",
            "Epoch 962/1000\n",
            "37/37 [==============================] - 10s 244ms/step - loss: 0.1628 - acc: 0.9605 - val_loss: 2.3515 - val_acc: 0.6354\n",
            "Epoch 963/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.1406 - acc: 0.9570 - val_loss: 2.7906 - val_acc: 0.6042\n",
            "Epoch 964/1000\n",
            "37/37 [==============================] - 8s 219ms/step - loss: 0.1997 - acc: 0.9399 - val_loss: 3.3447 - val_acc: 0.6250\n",
            "Epoch 965/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.1310 - acc: 0.9708 - val_loss: 3.7451 - val_acc: 0.6562\n",
            "Epoch 966/1000\n",
            "37/37 [==============================] - 9s 225ms/step - loss: 0.0857 - acc: 0.9691 - val_loss: 3.0506 - val_acc: 0.6354\n",
            "Epoch 967/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1034 - acc: 0.9674 - val_loss: 2.6981 - val_acc: 0.6667\n",
            "Epoch 968/1000\n",
            "37/37 [==============================] - 5s 118ms/step - loss: 0.2197 - acc: 0.9381 - val_loss: 2.6200 - val_acc: 0.6875\n",
            "Epoch 969/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.0955 - acc: 0.9656 - val_loss: 2.2798 - val_acc: 0.6667\n",
            "Epoch 970/1000\n",
            "37/37 [==============================] - 4s 107ms/step - loss: 0.1586 - acc: 0.9553 - val_loss: 2.6969 - val_acc: 0.6771\n",
            "Epoch 971/1000\n",
            "37/37 [==============================] - 4s 89ms/step - loss: 0.1310 - acc: 0.9622 - val_loss: 2.7068 - val_acc: 0.6771\n",
            "Epoch 972/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1916 - acc: 0.9467 - val_loss: 2.3095 - val_acc: 0.6771\n",
            "Epoch 973/1000\n",
            "37/37 [==============================] - 9s 230ms/step - loss: 0.1548 - acc: 0.9450 - val_loss: 2.3699 - val_acc: 0.5625\n",
            "Epoch 974/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.1521 - acc: 0.9485 - val_loss: 1.9782 - val_acc: 0.5938\n",
            "Epoch 975/1000\n",
            "37/37 [==============================] - 9s 226ms/step - loss: 0.1356 - acc: 0.9588 - val_loss: 2.1663 - val_acc: 0.6979\n",
            "Epoch 976/1000\n",
            "37/37 [==============================] - 9s 224ms/step - loss: 0.0593 - acc: 0.9794 - val_loss: 2.2700 - val_acc: 0.6667\n",
            "Epoch 977/1000\n",
            "37/37 [==============================] - 9s 245ms/step - loss: 0.1052 - acc: 0.9691 - val_loss: 2.2985 - val_acc: 0.6667\n",
            "Epoch 978/1000\n",
            "37/37 [==============================] - 4s 101ms/step - loss: 0.1606 - acc: 0.9536 - val_loss: 1.9481 - val_acc: 0.6979\n",
            "Epoch 979/1000\n",
            "37/37 [==============================] - 5s 115ms/step - loss: 0.1259 - acc: 0.9605 - val_loss: 2.2213 - val_acc: 0.6354\n",
            "Epoch 980/1000\n",
            "37/37 [==============================] - 9s 222ms/step - loss: 0.0752 - acc: 0.9777 - val_loss: 2.3813 - val_acc: 0.6250\n",
            "Epoch 981/1000\n",
            "37/37 [==============================] - 8s 221ms/step - loss: 0.1127 - acc: 0.9639 - val_loss: 2.2504 - val_acc: 0.6667\n",
            "Epoch 982/1000\n",
            "37/37 [==============================] - 9s 233ms/step - loss: 0.1735 - acc: 0.9467 - val_loss: 2.2476 - val_acc: 0.6771\n",
            "Epoch 983/1000\n",
            "37/37 [==============================] - 8s 220ms/step - loss: 0.2230 - acc: 0.9485 - val_loss: 2.2814 - val_acc: 0.6562\n",
            "Epoch 984/1000\n",
            "37/37 [==============================] - 4s 84ms/step - loss: 0.1562 - acc: 0.9570 - val_loss: 2.1152 - val_acc: 0.6771\n",
            "Epoch 985/1000\n",
            "37/37 [==============================] - 9s 220ms/step - loss: 0.1619 - acc: 0.9485 - val_loss: 1.7965 - val_acc: 0.6458\n",
            "Epoch 986/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1189 - acc: 0.9605 - val_loss: 2.1108 - val_acc: 0.6146\n",
            "Epoch 987/1000\n",
            "37/37 [==============================] - 4s 108ms/step - loss: 0.1824 - acc: 0.9519 - val_loss: 2.3607 - val_acc: 0.6042\n",
            "Epoch 988/1000\n",
            "37/37 [==============================] - 9s 229ms/step - loss: 0.1636 - acc: 0.9570 - val_loss: 2.2780 - val_acc: 0.5625\n",
            "Epoch 989/1000\n",
            "37/37 [==============================] - 4s 104ms/step - loss: 0.1159 - acc: 0.9725 - val_loss: 2.0908 - val_acc: 0.5938\n",
            "Epoch 990/1000\n",
            "37/37 [==============================] - 4s 98ms/step - loss: 0.1438 - acc: 0.9536 - val_loss: 2.2290 - val_acc: 0.6771\n",
            "Epoch 991/1000\n",
            "37/37 [==============================] - 9s 228ms/step - loss: 0.1498 - acc: 0.9536 - val_loss: 2.2947 - val_acc: 0.6562\n",
            "Epoch 992/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1992 - acc: 0.9605 - val_loss: 2.1190 - val_acc: 0.6979\n",
            "Epoch 993/1000\n",
            "37/37 [==============================] - 9s 221ms/step - loss: 0.1443 - acc: 0.9691 - val_loss: 2.3681 - val_acc: 0.6562\n",
            "Epoch 994/1000\n",
            "37/37 [==============================] - 9s 241ms/step - loss: 0.1785 - acc: 0.9570 - val_loss: 2.2544 - val_acc: 0.6458\n",
            "Epoch 995/1000\n",
            "37/37 [==============================] - 4s 96ms/step - loss: 0.1174 - acc: 0.9691 - val_loss: 1.9425 - val_acc: 0.6875\n",
            "Epoch 996/1000\n",
            "37/37 [==============================] - 9s 240ms/step - loss: 0.1990 - acc: 0.9467 - val_loss: 2.5001 - val_acc: 0.6458\n",
            "Epoch 997/1000\n",
            "37/37 [==============================] - 4s 102ms/step - loss: 0.0919 - acc: 0.9639 - val_loss: 2.1912 - val_acc: 0.6667\n",
            "Epoch 998/1000\n",
            "37/37 [==============================] - 9s 246ms/step - loss: 0.2486 - acc: 0.9364 - val_loss: 2.1069 - val_acc: 0.6354\n",
            "Epoch 999/1000\n",
            "37/37 [==============================] - 9s 227ms/step - loss: 0.1947 - acc: 0.9399 - val_loss: 1.9627 - val_acc: 0.6771\n",
            "Epoch 1000/1000\n",
            "37/37 [==============================] - 4s 106ms/step - loss: 0.0823 - acc: 0.9725 - val_loss: 2.1961 - val_acc: 0.6458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history "
      ],
      "metadata": {
        "id": "tO_KJFVJu6ya",
        "outputId": "4d39a031-1ced-44b5-b388-d66d7dd27f5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [2.283203125,\n",
              "  1.7156797647476196,\n",
              "  1.4155213832855225,\n",
              "  1.3588844537734985,\n",
              "  1.3528099060058594,\n",
              "  1.1713958978652954,\n",
              "  0.9728572964668274,\n",
              "  1.0243877172470093,\n",
              "  0.7862246632575989,\n",
              "  0.7759873867034912,\n",
              "  0.741085410118103,\n",
              "  0.9321109652519226,\n",
              "  0.6871484518051147,\n",
              "  0.6943192481994629,\n",
              "  0.5876014232635498,\n",
              "  0.6043407917022705,\n",
              "  0.6373891830444336,\n",
              "  0.6161970496177673,\n",
              "  0.4830911159515381,\n",
              "  0.5327103734016418,\n",
              "  0.49563267827033997,\n",
              "  0.45611608028411865,\n",
              "  0.44067785143852234,\n",
              "  0.5424473881721497,\n",
              "  0.527411699295044,\n",
              "  0.40952807664871216,\n",
              "  0.486551433801651,\n",
              "  0.4065146744251251,\n",
              "  0.4471990168094635,\n",
              "  0.43704649806022644,\n",
              "  0.44661059975624084,\n",
              "  0.42065897583961487,\n",
              "  0.40913575887680054,\n",
              "  0.38353079557418823,\n",
              "  0.34904274344444275,\n",
              "  0.3156821131706238,\n",
              "  0.34153521060943604,\n",
              "  0.33416080474853516,\n",
              "  0.38499268889427185,\n",
              "  0.3744553029537201,\n",
              "  0.3505367040634155,\n",
              "  0.3340277671813965,\n",
              "  0.29134511947631836,\n",
              "  0.3573031723499298,\n",
              "  0.37340250611305237,\n",
              "  0.33020544052124023,\n",
              "  0.2865258455276489,\n",
              "  0.33939096331596375,\n",
              "  0.30070745944976807,\n",
              "  0.3119906485080719,\n",
              "  0.3030102849006653,\n",
              "  0.3049646019935608,\n",
              "  0.31339630484580994,\n",
              "  0.23774506151676178,\n",
              "  0.32211095094680786,\n",
              "  0.2765844166278839,\n",
              "  0.25129371881484985,\n",
              "  0.3145088851451874,\n",
              "  0.2777625024318695,\n",
              "  0.3182269334793091,\n",
              "  0.2940595746040344,\n",
              "  0.2640194296836853,\n",
              "  0.21634669601917267,\n",
              "  0.2550699710845947,\n",
              "  0.35168662667274475,\n",
              "  0.24582698941230774,\n",
              "  0.2237129807472229,\n",
              "  0.28762003779411316,\n",
              "  0.2634371817111969,\n",
              "  0.281266450881958,\n",
              "  0.21761997044086456,\n",
              "  0.2157275676727295,\n",
              "  0.24674738943576813,\n",
              "  0.29539406299591064,\n",
              "  0.2401205599308014,\n",
              "  0.26081663370132446,\n",
              "  0.2547326385974884,\n",
              "  0.20850232243537903,\n",
              "  0.26400265097618103,\n",
              "  0.279620885848999,\n",
              "  0.2591501772403717,\n",
              "  0.2456706166267395,\n",
              "  0.24635756015777588,\n",
              "  0.2437378615140915,\n",
              "  0.2605823874473572,\n",
              "  0.23078541457653046,\n",
              "  0.2492753118276596,\n",
              "  0.2757558524608612,\n",
              "  0.2501447796821594,\n",
              "  0.21527689695358276,\n",
              "  0.21670083701610565,\n",
              "  0.20428785681724548,\n",
              "  0.25459757447242737,\n",
              "  0.23833701014518738,\n",
              "  0.21241992712020874,\n",
              "  0.23693975806236267,\n",
              "  0.1845470815896988,\n",
              "  0.2568632662296295,\n",
              "  0.25613561272621155,\n",
              "  0.23958954215049744,\n",
              "  0.2580701410770416,\n",
              "  0.25675472617149353,\n",
              "  0.21013832092285156,\n",
              "  0.22571521997451782,\n",
              "  0.19414067268371582,\n",
              "  0.26385968923568726,\n",
              "  0.2195381373167038,\n",
              "  0.20532412827014923,\n",
              "  0.24671438336372375,\n",
              "  0.22406910359859467,\n",
              "  0.2577876150608063,\n",
              "  0.22970277070999146,\n",
              "  0.2328530251979828,\n",
              "  0.21655748784542084,\n",
              "  0.20116189122200012,\n",
              "  0.2598828375339508,\n",
              "  0.24628596007823944,\n",
              "  0.1974090188741684,\n",
              "  0.25029048323631287,\n",
              "  0.21497972309589386,\n",
              "  0.21233196556568146,\n",
              "  0.20936883985996246,\n",
              "  0.20106631517410278,\n",
              "  0.24658764898777008,\n",
              "  0.2420307993888855,\n",
              "  0.2403743714094162,\n",
              "  0.21361595392227173,\n",
              "  0.20398680865764618,\n",
              "  0.26892712712287903,\n",
              "  0.18543604016304016,\n",
              "  0.17871657013893127,\n",
              "  0.22042398154735565,\n",
              "  0.13734027743339539,\n",
              "  0.21343518793582916,\n",
              "  0.17098574340343475,\n",
              "  0.1919882595539093,\n",
              "  0.24322965741157532,\n",
              "  0.24070028960704803,\n",
              "  0.21111977100372314,\n",
              "  0.2095102071762085,\n",
              "  0.19933396577835083,\n",
              "  0.28787097334861755,\n",
              "  0.1676824539899826,\n",
              "  0.17249511182308197,\n",
              "  0.24483340978622437,\n",
              "  0.19756996631622314,\n",
              "  0.1769753396511078,\n",
              "  0.18761220574378967,\n",
              "  0.1787136346101761,\n",
              "  0.17941179871559143,\n",
              "  0.19312362372875214,\n",
              "  0.21677428483963013,\n",
              "  0.1697673499584198,\n",
              "  0.16216519474983215,\n",
              "  0.1967255175113678,\n",
              "  0.20085012912750244,\n",
              "  0.21135562658309937,\n",
              "  0.17753934860229492,\n",
              "  0.18052265048027039,\n",
              "  0.20987914502620697,\n",
              "  0.21099026501178741,\n",
              "  0.1918405145406723,\n",
              "  0.16774345934391022,\n",
              "  0.16621655225753784,\n",
              "  0.1956428736448288,\n",
              "  0.16082431375980377,\n",
              "  0.1937299221754074,\n",
              "  0.25187814235687256,\n",
              "  0.17655211687088013,\n",
              "  0.17935414612293243,\n",
              "  0.2206738442182541,\n",
              "  0.1793103665113449,\n",
              "  0.17350085079669952,\n",
              "  0.16606344282627106,\n",
              "  0.12044951319694519,\n",
              "  0.25494512915611267,\n",
              "  0.19334405660629272,\n",
              "  0.18326030671596527,\n",
              "  0.25131726264953613,\n",
              "  0.1793079525232315,\n",
              "  0.1841340810060501,\n",
              "  0.16427378356456757,\n",
              "  0.18501800298690796,\n",
              "  0.20073656737804413,\n",
              "  0.1832669973373413,\n",
              "  0.2405247688293457,\n",
              "  0.18202564120292664,\n",
              "  0.12783756852149963,\n",
              "  0.18183353543281555,\n",
              "  0.20070193707942963,\n",
              "  0.17612947523593903,\n",
              "  0.2065836787223816,\n",
              "  0.15664443373680115,\n",
              "  0.21953321993350983,\n",
              "  0.19692689180374146,\n",
              "  0.17268149554729462,\n",
              "  0.17557302117347717,\n",
              "  0.13162262737751007,\n",
              "  0.14755864441394806,\n",
              "  0.18994592130184174,\n",
              "  0.19719748198986053,\n",
              "  0.17122069001197815,\n",
              "  0.2203357070684433,\n",
              "  0.2166498452425003,\n",
              "  0.1998467594385147,\n",
              "  0.20840199291706085,\n",
              "  0.2061547487974167,\n",
              "  0.18049155175685883,\n",
              "  0.16568540036678314,\n",
              "  0.1639113575220108,\n",
              "  0.20290382206439972,\n",
              "  0.21376056969165802,\n",
              "  0.2091498076915741,\n",
              "  0.22195526957511902,\n",
              "  0.13851496577262878,\n",
              "  0.160318061709404,\n",
              "  0.2571089565753937,\n",
              "  0.16965141892433167,\n",
              "  0.2090657353401184,\n",
              "  0.20589397847652435,\n",
              "  0.1718374788761139,\n",
              "  0.2053133100271225,\n",
              "  0.1831626445055008,\n",
              "  0.19434666633605957,\n",
              "  0.19481633603572845,\n",
              "  0.1874384582042694,\n",
              "  0.19839544594287872,\n",
              "  0.17891566455364227,\n",
              "  0.16246049106121063,\n",
              "  0.15847539901733398,\n",
              "  0.15590839087963104,\n",
              "  0.2213057428598404,\n",
              "  0.17710843682289124,\n",
              "  0.20261192321777344,\n",
              "  0.20282943546772003,\n",
              "  0.18880204856395721,\n",
              "  0.17251406610012054,\n",
              "  0.130512073636055,\n",
              "  0.18188023567199707,\n",
              "  0.18792486190795898,\n",
              "  0.24483461678028107,\n",
              "  0.12741492688655853,\n",
              "  0.2000814527273178,\n",
              "  0.18503695726394653,\n",
              "  0.18322093784809113,\n",
              "  0.2320667803287506,\n",
              "  0.19492672383785248,\n",
              "  0.1991986483335495,\n",
              "  0.1889471858739853,\n",
              "  0.18432295322418213,\n",
              "  0.19711725413799286,\n",
              "  0.13592413067817688,\n",
              "  0.12610457837581635,\n",
              "  0.1756897270679474,\n",
              "  0.16208094358444214,\n",
              "  0.23896783590316772,\n",
              "  0.1636333465576172,\n",
              "  0.1511610746383667,\n",
              "  0.17884385585784912,\n",
              "  0.16785338521003723,\n",
              "  0.16657432913780212,\n",
              "  0.15304897725582123,\n",
              "  0.11950471252202988,\n",
              "  0.16461601853370667,\n",
              "  0.14966513216495514,\n",
              "  0.15899671614170074,\n",
              "  0.19402270019054413,\n",
              "  0.17209987342357635,\n",
              "  0.18891334533691406,\n",
              "  0.2455741912126541,\n",
              "  0.20452909171581268,\n",
              "  0.13436567783355713,\n",
              "  0.20384621620178223,\n",
              "  0.17863894999027252,\n",
              "  0.21217180788516998,\n",
              "  0.15402621030807495,\n",
              "  0.22645296156406403,\n",
              "  0.15173177421092987,\n",
              "  0.15679332613945007,\n",
              "  0.18589776754379272,\n",
              "  0.1533205658197403,\n",
              "  0.2012600302696228,\n",
              "  0.14736932516098022,\n",
              "  0.17182688415050507,\n",
              "  0.15076953172683716,\n",
              "  0.18940074741840363,\n",
              "  0.1345478892326355,\n",
              "  0.14328032732009888,\n",
              "  0.16354312002658844,\n",
              "  0.1086353063583374,\n",
              "  0.16438227891921997,\n",
              "  0.16815434396266937,\n",
              "  0.12152484059333801,\n",
              "  0.20784911513328552,\n",
              "  0.17083211243152618,\n",
              "  0.22810626029968262,\n",
              "  0.14846539497375488,\n",
              "  0.1410594880580902,\n",
              "  0.2364339977502823,\n",
              "  0.21688212454319,\n",
              "  0.10843534767627716,\n",
              "  0.16874565184116364,\n",
              "  0.17055022716522217,\n",
              "  0.1409095972776413,\n",
              "  0.19660420715808868,\n",
              "  0.16155420243740082,\n",
              "  0.18870720267295837,\n",
              "  0.2366403490304947,\n",
              "  0.18110445141792297,\n",
              "  0.21589407324790955,\n",
              "  0.15929469466209412,\n",
              "  0.0934646800160408,\n",
              "  0.16570919752120972,\n",
              "  0.20848093926906586,\n",
              "  0.1607542186975479,\n",
              "  0.17744025588035583,\n",
              "  0.15995582938194275,\n",
              "  0.12044361233711243,\n",
              "  0.13756079971790314,\n",
              "  0.29730626940727234,\n",
              "  0.14837691187858582,\n",
              "  0.14241339266300201,\n",
              "  0.1324056088924408,\n",
              "  0.16627348959445953,\n",
              "  0.15519700944423676,\n",
              "  0.1367006003856659,\n",
              "  0.24553562700748444,\n",
              "  0.20010577142238617,\n",
              "  0.17471037805080414,\n",
              "  0.2226671278476715,\n",
              "  0.19882610440254211,\n",
              "  0.19616785645484924,\n",
              "  0.20671340823173523,\n",
              "  0.1505546122789383,\n",
              "  0.12840858101844788,\n",
              "  0.17690524458885193,\n",
              "  0.17205972969532013,\n",
              "  0.14339353144168854,\n",
              "  0.176091268658638,\n",
              "  0.23644426465034485,\n",
              "  0.2606583535671234,\n",
              "  0.2000661939382553,\n",
              "  0.1787717044353485,\n",
              "  0.1838310956954956,\n",
              "  0.16423071920871735,\n",
              "  0.2364877164363861,\n",
              "  0.12825854122638702,\n",
              "  0.2221887856721878,\n",
              "  0.192226380109787,\n",
              "  0.19885022938251495,\n",
              "  0.11753591150045395,\n",
              "  0.21593955159187317,\n",
              "  0.17138002812862396,\n",
              "  0.1736728698015213,\n",
              "  0.24902698397636414,\n",
              "  0.12399937957525253,\n",
              "  0.13912707567214966,\n",
              "  0.2064952701330185,\n",
              "  0.1789986789226532,\n",
              "  0.16764436662197113,\n",
              "  0.16873089969158173,\n",
              "  0.16922207176685333,\n",
              "  0.19828064739704132,\n",
              "  0.13559095561504364,\n",
              "  0.20128095149993896,\n",
              "  0.15552738308906555,\n",
              "  0.15262562036514282,\n",
              "  0.12541741132736206,\n",
              "  0.2510509490966797,\n",
              "  0.2134520262479782,\n",
              "  0.18929652869701385,\n",
              "  0.14878065884113312,\n",
              "  0.13701574504375458,\n",
              "  0.15876075625419617,\n",
              "  0.11297272890806198,\n",
              "  0.16415846347808838,\n",
              "  0.1513899713754654,\n",
              "  0.1735365241765976,\n",
              "  0.1646186262369156,\n",
              "  0.17069806158542633,\n",
              "  0.16954228281974792,\n",
              "  0.1955469399690628,\n",
              "  0.19941844046115875,\n",
              "  0.1441861391067505,\n",
              "  0.12455661594867706,\n",
              "  0.16182859241962433,\n",
              "  0.15864767134189606,\n",
              "  0.20237357914447784,\n",
              "  0.12755432724952698,\n",
              "  0.10548482090234756,\n",
              "  0.1985313892364502,\n",
              "  0.23213264346122742,\n",
              "  0.12139297276735306,\n",
              "  0.1766020506620407,\n",
              "  0.1286451518535614,\n",
              "  0.1775866001844406,\n",
              "  0.18498556315898895,\n",
              "  0.1575453132390976,\n",
              "  0.1521756649017334,\n",
              "  0.19926677644252777,\n",
              "  0.1778745949268341,\n",
              "  0.15783080458641052,\n",
              "  0.17823393642902374,\n",
              "  0.14054614305496216,\n",
              "  0.149968683719635,\n",
              "  0.17908978462219238,\n",
              "  0.13324326276779175,\n",
              "  0.18492156267166138,\n",
              "  0.14967535436153412,\n",
              "  0.14104139804840088,\n",
              "  0.16449357569217682,\n",
              "  0.1800667941570282,\n",
              "  0.1698269098997116,\n",
              "  0.1470581591129303,\n",
              "  0.13641183078289032,\n",
              "  0.16307862102985382,\n",
              "  0.19896969199180603,\n",
              "  0.17001526057720184,\n",
              "  0.1313033103942871,\n",
              "  0.1691192090511322,\n",
              "  0.1654493361711502,\n",
              "  0.1854802519083023,\n",
              "  0.11230893433094025,\n",
              "  0.13253484666347504,\n",
              "  0.1714666783809662,\n",
              "  0.11998258531093597,\n",
              "  0.12592139840126038,\n",
              "  0.17615923285484314,\n",
              "  0.18035435676574707,\n",
              "  0.13226479291915894,\n",
              "  0.17907927930355072,\n",
              "  0.13854457437992096,\n",
              "  0.1359330117702484,\n",
              "  0.15043100714683533,\n",
              "  0.12016085535287857,\n",
              "  0.2362871915102005,\n",
              "  0.14543046057224274,\n",
              "  0.22073861956596375,\n",
              "  0.12123287469148636,\n",
              "  0.13162042200565338,\n",
              "  0.2309517115354538,\n",
              "  0.1231408640742302,\n",
              "  0.15013422071933746,\n",
              "  0.19432617723941803,\n",
              "  0.17020127177238464,\n",
              "  0.1519632786512375,\n",
              "  0.11457818001508713,\n",
              "  0.1270528882741928,\n",
              "  0.15545323491096497,\n",
              "  0.2049005925655365,\n",
              "  0.1740109771490097,\n",
              "  0.13990773260593414,\n",
              "  0.12411195039749146,\n",
              "  0.2675987184047699,\n",
              "  0.19090209901332855,\n",
              "  0.20651765167713165,\n",
              "  0.198451966047287,\n",
              "  0.13983507454395294,\n",
              "  0.11767525970935822,\n",
              "  0.16014094650745392,\n",
              "  0.14628706872463226,\n",
              "  0.14966286718845367,\n",
              "  0.15868288278579712,\n",
              "  0.18775369226932526,\n",
              "  0.1657993346452713,\n",
              "  0.13185909390449524,\n",
              "  0.21373766660690308,\n",
              "  0.1202249675989151,\n",
              "  0.188409224152565,\n",
              "  0.13997399806976318,\n",
              "  0.21055954694747925,\n",
              "  0.2641027271747589,\n",
              "  0.1650119572877884,\n",
              "  0.16487911343574524,\n",
              "  0.15842704474925995,\n",
              "  0.1609252691268921,\n",
              "  0.17411014437675476,\n",
              "  0.16786368191242218,\n",
              "  0.23909835517406464,\n",
              "  0.1183987483382225,\n",
              "  0.12983959913253784,\n",
              "  0.1710353046655655,\n",
              "  0.20510730147361755,\n",
              "  0.14469663798809052,\n",
              "  0.13485956192016602,\n",
              "  0.10721880197525024,\n",
              "  0.1753794103860855,\n",
              "  0.14781546592712402,\n",
              "  0.16813109815120697,\n",
              "  0.18476150929927826,\n",
              "  0.16534720361232758,\n",
              "  0.185598686337471,\n",
              "  0.1427265703678131,\n",
              "  0.13174249231815338,\n",
              "  0.1445491909980774,\n",
              "  0.14110921323299408,\n",
              "  0.17026038467884064,\n",
              "  0.16002875566482544,\n",
              "  0.15629781782627106,\n",
              "  0.1418982893228531,\n",
              "  0.1431644707918167,\n",
              "  0.13136152923107147,\n",
              "  0.10632069408893585,\n",
              "  0.1828429102897644,\n",
              "  0.1534377485513687,\n",
              "  0.13435427844524384,\n",
              "  0.13802333176136017,\n",
              "  0.16950692236423492,\n",
              "  0.135374054312706,\n",
              "  0.13518758118152618,\n",
              "  0.17042680084705353,\n",
              "  0.21939094364643097,\n",
              "  0.18837285041809082,\n",
              "  0.1354958862066269,\n",
              "  0.16394752264022827,\n",
              "  0.14583146572113037,\n",
              "  0.12299778312444687,\n",
              "  0.18277864158153534,\n",
              "  0.18431201577186584,\n",
              "  0.1798381358385086,\n",
              "  0.2688400149345398,\n",
              "  0.12688188254833221,\n",
              "  0.17357246577739716,\n",
              "  0.1577003300189972,\n",
              "  0.11758512258529663,\n",
              "  0.1311706006526947,\n",
              "  0.17764271795749664,\n",
              "  0.19262702763080597,\n",
              "  0.18916161358356476,\n",
              "  0.1659974604845047,\n",
              "  0.19995194673538208,\n",
              "  0.17582879960536957,\n",
              "  0.1411760002374649,\n",
              "  0.16851848363876343,\n",
              "  0.1738225668668747,\n",
              "  0.2029007375240326,\n",
              "  0.23537255823612213,\n",
              "  0.14891460537910461,\n",
              "  0.1490887850522995,\n",
              "  0.22906725108623505,\n",
              "  0.20370233058929443,\n",
              "  0.1545599102973938,\n",
              "  0.16953715682029724,\n",
              "  0.1574825942516327,\n",
              "  0.09298250824213028,\n",
              "  0.2059776932001114,\n",
              "  0.1636981964111328,\n",
              "  0.19421632587909698,\n",
              "  0.14003805816173553,\n",
              "  0.13684214651584625,\n",
              "  0.11082728207111359,\n",
              "  0.16594743728637695,\n",
              "  0.18088136613368988,\n",
              "  0.1626088172197342,\n",
              "  0.12637224793434143,\n",
              "  0.16910430788993835,\n",
              "  0.17561517655849457,\n",
              "  0.16469447314739227,\n",
              "  0.10661277920007706,\n",
              "  0.14013360440731049,\n",
              "  0.11880049854516983,\n",
              "  0.17517423629760742,\n",
              "  0.1932036131620407,\n",
              "  0.16854408383369446,\n",
              "  0.15805287659168243,\n",
              "  0.17491364479064941,\n",
              "  0.1562419980764389,\n",
              "  0.1188904419541359,\n",
              "  0.15013740956783295,\n",
              "  0.1620100438594818,\n",
              "  0.20252470672130585,\n",
              "  0.2097608745098114,\n",
              "  0.1774328500032425,\n",
              "  0.15552684664726257,\n",
              "  0.24336816370487213,\n",
              "  0.16432929039001465,\n",
              "  0.15366600453853607,\n",
              "  0.1888820230960846,\n",
              "  0.16309905052185059,\n",
              "  0.13180825114250183,\n",
              "  0.0974324643611908,\n",
              "  0.22449475526809692,\n",
              "  0.1787266582250595,\n",
              "  0.12004444748163223,\n",
              "  0.24199038743972778,\n",
              "  0.24362851679325104,\n",
              "  0.14466089010238647,\n",
              "  0.20762498676776886,\n",
              "  0.2241944968700409,\n",
              "  0.1114799901843071,\n",
              "  0.11802911758422852,\n",
              "  0.1908096820116043,\n",
              "  0.15870848298072815,\n",
              "  0.11483550816774368,\n",
              "  0.16273196041584015,\n",
              "  0.2200866937637329,\n",
              "  0.23872312903404236,\n",
              "  0.17096038162708282,\n",
              "  0.1641743779182434,\n",
              "  0.17022600769996643,\n",
              "  0.18998457491397858,\n",
              "  0.17735694348812103,\n",
              "  0.10402382165193558,\n",
              "  0.26913347840309143,\n",
              "  0.160903662443161,\n",
              "  0.17619067430496216,\n",
              "  0.22553184628486633,\n",
              "  0.12681294977664948,\n",
              "  0.1691403090953827,\n",
              "  0.15818335115909576,\n",
              "  0.15243852138519287,\n",
              "  0.10807063430547714,\n",
              "  0.21981249749660492,\n",
              "  0.13006514310836792,\n",
              "  0.18794327974319458,\n",
              "  0.13498833775520325,\n",
              "  0.15469391644001007,\n",
              "  0.18136924505233765,\n",
              "  0.11836031824350357,\n",
              "  0.2286098450422287,\n",
              "  0.19473896920681,\n",
              "  0.18678729236125946,\n",
              "  0.1779222935438156,\n",
              "  0.12634840607643127,\n",
              "  0.14214028418064117,\n",
              "  0.19745227694511414,\n",
              "  0.14230036735534668,\n",
              "  0.13106630742549896,\n",
              "  0.22382275760173798,\n",
              "  0.2323983609676361,\n",
              "  0.17500783503055573,\n",
              "  0.14818468689918518,\n",
              "  0.15829390287399292,\n",
              "  0.1355389505624771,\n",
              "  0.12356041371822357,\n",
              "  0.14890098571777344,\n",
              "  0.15315526723861694,\n",
              "  0.2194063365459442,\n",
              "  0.18344873189926147,\n",
              "  0.13648203015327454,\n",
              "  0.13880130648612976,\n",
              "  0.17690876126289368,\n",
              "  0.19265155494213104,\n",
              "  0.16628694534301758,\n",
              "  0.1050710380077362,\n",
              "  0.11998916417360306,\n",
              "  0.17021708190441132,\n",
              "  0.10752556473016739,\n",
              "  0.24961447715759277,\n",
              "  0.16155870258808136,\n",
              "  0.15973275899887085,\n",
              "  0.09481232613325119,\n",
              "  0.13315553963184357,\n",
              "  0.19033674895763397,\n",
              "  0.10215207189321518,\n",
              "  0.2093697190284729,\n",
              "  0.1444718837738037,\n",
              "  0.16166844964027405,\n",
              "  0.1072450652718544,\n",
              "  0.14362163841724396,\n",
              "  0.1792750209569931,\n",
              "  0.22041910886764526,\n",
              "  0.13623574376106262,\n",
              "  0.1869690716266632,\n",
              "  0.17798466980457306,\n",
              "  0.10471481829881668,\n",
              "  0.1750735193490982,\n",
              "  0.2108583152294159,\n",
              "  0.15877912938594818,\n",
              "  0.18921147286891937,\n",
              "  0.18658408522605896,\n",
              "  0.19644734263420105,\n",
              "  0.22589989006519318,\n",
              "  0.09977857768535614,\n",
              "  0.184337317943573,\n",
              "  0.2549346685409546,\n",
              "  0.13125158846378326,\n",
              "  0.1118437871336937,\n",
              "  0.15707583725452423,\n",
              "  0.1706961989402771,\n",
              "  0.12098000198602676,\n",
              "  0.13297000527381897,\n",
              "  0.0899820625782013,\n",
              "  0.1532386690378189,\n",
              "  0.1006636694073677,\n",
              "  0.1516007035970688,\n",
              "  0.15431173145771027,\n",
              "  0.19391639530658722,\n",
              "  0.2322520613670349,\n",
              "  0.12010529637336731,\n",
              "  0.1951567828655243,\n",
              "  0.16284140944480896,\n",
              "  0.21004949510097504,\n",
              "  0.11362738162279129,\n",
              "  0.1379341334104538,\n",
              "  0.12024129927158356,\n",
              "  0.15940317511558533,\n",
              "  0.1725165843963623,\n",
              "  0.1565832495689392,\n",
              "  0.18149587512016296,\n",
              "  0.11881055682897568,\n",
              "  0.19316574931144714,\n",
              "  0.1682722121477127,\n",
              "  0.1646355390548706,\n",
              "  0.14735810458660126,\n",
              "  0.16947585344314575,\n",
              "  0.12877486646175385,\n",
              "  0.21438799798488617,\n",
              "  0.23581837117671967,\n",
              "  0.13781613111495972,\n",
              "  0.23424971103668213,\n",
              "  0.22377368807792664,\n",
              "  0.15038546919822693,\n",
              "  0.18609417974948883,\n",
              "  0.14347323775291443,\n",
              "  0.18366025388240814,\n",
              "  0.16404379904270172,\n",
              "  0.09511620551347733,\n",
              "  0.1404438614845276,\n",
              "  0.1745097041130066,\n",
              "  0.15635798871517181,\n",
              "  0.14245092868804932,\n",
              "  0.12450078874826431,\n",
              "  0.08701908588409424,\n",
              "  0.12204473465681076,\n",
              "  0.15692001581192017,\n",
              "  0.1748342514038086,\n",
              "  0.10688710957765579,\n",
              "  0.1245000883936882,\n",
              "  0.14162905514240265,\n",
              "  0.14368951320648193,\n",
              "  0.1130453422665596,\n",
              "  0.19706346094608307,\n",
              "  0.14495962858200073,\n",
              "  0.10426709055900574,\n",
              "  0.14212332665920258,\n",
              "  0.10715118050575256,\n",
              "  0.14852939546108246,\n",
              "  0.07594389468431473,\n",
              "  0.179488867521286,\n",
              "  0.1457877904176712,\n",
              "  0.14616286754608154,\n",
              "  0.1864006221294403,\n",
              "  0.1872800588607788,\n",
              "  0.11175357550382614,\n",
              "  0.08306892961263657,\n",
              "  0.20257261395454407,\n",
              "  0.18201930820941925,\n",
              "  0.15215350687503815,\n",
              "  0.17176997661590576,\n",
              "  0.14763237535953522,\n",
              "  0.13669711351394653,\n",
              "  0.1521020084619522,\n",
              "  0.1410798728466034,\n",
              "  0.1712578386068344,\n",
              "  0.1507512778043747,\n",
              "  0.1702767312526703,\n",
              "  0.18167991936206818,\n",
              "  0.18670117855072021,\n",
              "  0.18543913960456848,\n",
              "  0.10816909372806549,\n",
              "  0.15744896233081818,\n",
              "  0.12421047687530518,\n",
              "  0.1704186499118805,\n",
              "  0.1159563884139061,\n",
              "  0.135883167386055,\n",
              "  0.10855795443058014,\n",
              "  0.14491599798202515,\n",
              "  0.13146044313907623,\n",
              "  0.175172820687294,\n",
              "  0.12953998148441315,\n",
              "  0.08284024149179459,\n",
              "  0.1372409164905548,\n",
              "  0.12697263062000275,\n",
              "  0.16507138311862946,\n",
              "  0.11663708090782166,\n",
              "  0.2349875271320343,\n",
              "  0.0829058364033699,\n",
              "  0.11578987538814545,\n",
              "  0.20150160789489746,\n",
              "  0.2276540994644165,\n",
              "  0.09507279843091965,\n",
              "  0.13586387038230896,\n",
              "  0.15418335795402527,\n",
              "  0.18570475280284882,\n",
              "  0.1495577096939087,\n",
              "  0.14042873680591583,\n",
              "  0.10351374745368958,\n",
              "  0.11902782320976257,\n",
              "  0.17363341152668,\n",
              "  0.08979658037424088,\n",
              "  0.11253717541694641,\n",
              "  0.1677701622247696,\n",
              "  0.2436123639345169,\n",
              "  0.12939795851707458,\n",
              "  0.11899638921022415,\n",
              "  0.14821596443653107,\n",
              "  0.12732918560504913,\n",
              "  0.15368428826332092,\n",
              "  0.1514667272567749,\n",
              "  0.10402429848909378,\n",
              "  0.12918181717395782,\n",
              "  0.1157861277461052,\n",
              "  0.13403639197349548,\n",
              "  0.20473724603652954,\n",
              "  0.09933555871248245,\n",
              "  0.1005968302488327,\n",
              "  0.17066945135593414,\n",
              "  0.2188601940870285,\n",
              "  0.129751518368721,\n",
              "  0.22726450860500336,\n",
              "  0.14148099720478058,\n",
              "  0.1394028216600418,\n",
              "  0.12090194970369339,\n",
              "  0.149134561419487,\n",
              "  0.14599251747131348,\n",
              "  0.2563283145427704,\n",
              "  0.11259633302688599,\n",
              "  0.11915420740842819,\n",
              "  0.13302116096019745,\n",
              "  0.18970926105976105,\n",
              "  0.1275807023048401,\n",
              "  0.14126022160053253,\n",
              "  0.1880515217781067,\n",
              "  0.17340078949928284,\n",
              "  0.11006747931241989,\n",
              "  0.17211240530014038,\n",
              "  0.15231484174728394,\n",
              "  0.1606644093990326,\n",
              "  0.11904844641685486,\n",
              "  0.18493697047233582,\n",
              "  0.08139743655920029,\n",
              "  0.1722831130027771,\n",
              "  0.14849133789539337,\n",
              "  0.0829334557056427,\n",
              "  0.15951797366142273,\n",
              "  0.163667693734169,\n",
              "  0.1492176204919815,\n",
              "  0.1451435387134552,\n",
              "  0.17136839032173157,\n",
              "  0.17455191910266876,\n",
              "  0.17144235968589783,\n",
              "  0.19472093880176544,\n",
              "  0.19356130063533783,\n",
              "  0.13080725073814392,\n",
              "  0.137785404920578,\n",
              "  0.09925155341625214,\n",
              "  0.13911177217960358,\n",
              "  0.07882952690124512,\n",
              "  0.15521728992462158,\n",
              "  0.2073650062084198,\n",
              "  0.12001503258943558,\n",
              "  0.08988726884126663,\n",
              "  0.13639314472675323,\n",
              "  0.15586133301258087,\n",
              "  0.12945394217967987,\n",
              "  0.19349858164787292,\n",
              "  0.11878403276205063,\n",
              "  0.14082194864749908,\n",
              "  0.14179804921150208,\n",
              "  0.17290420830249786,\n",
              "  0.16438931226730347,\n",
              "  0.2162289321422577,\n",
              "  0.1396949589252472,\n",
              "  0.18251678347587585,\n",
              "  0.14316166937351227,\n",
              "  0.11046308279037476,\n",
              "  0.13718457520008087,\n",
              "  0.14000888168811798,\n",
              "  0.10878925025463104,\n",
              "  0.20013277232646942,\n",
              "  0.062431592494249344,\n",
              "  0.12084684520959854,\n",
              "  0.15306034684181213,\n",
              "  0.17494536936283112,\n",
              "  0.12496473640203476,\n",
              "  0.1327781081199646,\n",
              "  0.18618571758270264,\n",
              "  0.12192083150148392,\n",
              "  0.1309506595134735,\n",
              "  0.1978893280029297,\n",
              "  0.14132516086101532,\n",
              "  0.1924733966588974,\n",
              "  0.09051024913787842,\n",
              "  0.07690506428480148,\n",
              "  0.14130541682243347,\n",
              "  0.12494254112243652,\n",
              "  0.15571828186511993,\n",
              "  0.13791176676750183,\n",
              "  0.19216041266918182,\n",
              "  0.10976487398147583,\n",
              "  0.09283295273780823,\n",
              "  0.15835969150066376,\n",
              "  0.11388997733592987,\n",
              "  0.10580993443727493,\n",
              "  0.14979302883148193,\n",
              "  0.07740502059459686,\n",
              "  0.11718099564313889,\n",
              "  0.0762939378619194,\n",
              "  0.19848068058490753,\n",
              "  0.1784193366765976,\n",
              "  0.23004557192325592,\n",
              "  0.11939286440610886,\n",
              "  0.1412682831287384,\n",
              "  0.13279880583286285,\n",
              "  0.1192789226770401,\n",
              "  0.10202334076166153,\n",
              "  0.1254756897687912,\n",
              "  0.1766287386417389,\n",
              "  0.19966326653957367,\n",
              "  0.10642712563276291,\n",
              "  0.1380278617143631,\n",
              "  0.2079022228717804,\n",
              "  0.12259486317634583,\n",
              "  0.09496304392814636,\n",
              "  0.17527002096176147,\n",
              "  0.1317526400089264,\n",
              "  0.11538639664649963,\n",
              "  0.18076564371585846,\n",
              "  0.15231184661388397,\n",
              "  0.09517359733581543,\n",
              "  0.1671316772699356,\n",
              "  0.11070938408374786,\n",
              "  0.16374032199382782,\n",
              "  0.10141913592815399,\n",
              "  0.17110575735569,\n",
              "  0.15362724661827087,\n",
              "  0.1933077722787857,\n",
              "  0.15388746559619904,\n",
              "  0.16585776209831238,\n",
              "  0.1264278143644333,\n",
              "  0.13668256998062134,\n",
              "  0.15417829155921936,\n",
              "  0.21296057105064392,\n",
              "  0.130323588848114,\n",
              "  0.07394640892744064,\n",
              "  0.12742094695568085,\n",
              "  0.10097922384738922,\n",
              "  0.1982409805059433,\n",
              "  0.10266154259443283,\n",
              "  0.13012897968292236,\n",
              "  0.180257648229599,\n",
              "  0.19089066982269287,\n",
              "  0.14080768823623657,\n",
              "  0.20814746618270874,\n",
              "  0.1203354001045227,\n",
              "  0.12227939814329147,\n",
              "  0.21899071335792542,\n",
              "  0.1877385824918747,\n",
              "  0.10605459660291672,\n",
              "  0.22635623812675476,\n",
              "  0.17005367577075958,\n",
              "  0.11784043163061142,\n",
              "  0.11002520471811295,\n",
              "  0.14297638833522797,\n",
              "  0.14807884395122528,\n",
              "  0.1016470268368721,\n",
              "  0.1124759390950203,\n",
              "  0.17701414227485657,\n",
              "  0.14824476838111877,\n",
              "  0.15367934107780457,\n",
              "  0.16278178989887238,\n",
              "  0.14063087105751038,\n",
              "  0.19965848326683044,\n",
              "  0.13102248311042786,\n",
              "  0.08573632687330246,\n",
              "  0.10335764288902283,\n",
              "  0.2196660190820694,\n",
              "  0.09550449252128601,\n",
              "  0.15860164165496826,\n",
              "  0.13101695477962494,\n",
              "  0.19159747660160065,\n",
              "  0.15483033657073975,\n",
              "  0.1520654410123825,\n",
              "  0.1355678290128708,\n",
              "  0.05926364287734032,\n",
              "  0.10517310351133347,\n",
              "  0.1606307476758957,\n",
              "  0.12585830688476562,\n",
              "  0.07520598918199539,\n",
              "  0.11270792782306671,\n",
              "  0.1735471934080124,\n",
              "  0.22303399443626404,\n",
              "  0.15620499849319458,\n",
              "  0.1619127243757248,\n",
              "  0.11892403662204742,\n",
              "  0.1824437379837036,\n",
              "  0.16357281804084778,\n",
              "  0.11592575162649155,\n",
              "  0.1437513828277588,\n",
              "  0.14977064728736877,\n",
              "  0.19921134412288666,\n",
              "  0.1442805379629135,\n",
              "  0.17845290899276733,\n",
              "  0.11735960841178894,\n",
              "  0.1990114450454712,\n",
              "  0.09192611277103424,\n",
              "  0.2486019879579544,\n",
              "  0.19473415613174438,\n",
              "  0.082339808344841],\n",
              " 'acc': [0.5670102834701538,\n",
              "  0.6391752362251282,\n",
              "  0.6638513803482056,\n",
              "  0.6786941289901733,\n",
              "  0.6597937941551208,\n",
              "  0.668384850025177,\n",
              "  0.6941580772399902,\n",
              "  0.6821305751800537,\n",
              "  0.7371134161949158,\n",
              "  0.7285223603248596,\n",
              "  0.7195945978164673,\n",
              "  0.6907216310501099,\n",
              "  0.7542955279350281,\n",
              "  0.7508590817451477,\n",
              "  0.7852233648300171,\n",
              "  0.7577319741249084,\n",
              "  0.738831639289856,\n",
              "  0.7250859141349792,\n",
              "  0.7766323089599609,\n",
              "  0.7817869186401367,\n",
              "  0.8058419227600098,\n",
              "  0.7972508668899536,\n",
              "  0.8024054765701294,\n",
              "  0.7783505320549011,\n",
              "  0.7663230299949646,\n",
              "  0.8178694248199463,\n",
              "  0.7920961976051331,\n",
              "  0.8144329786300659,\n",
              "  0.8024054765701294,\n",
              "  0.8195876479148865,\n",
              "  0.7972508668899536,\n",
              "  0.8247422575950623,\n",
              "  0.8127147555351257,\n",
              "  0.8230240345001221,\n",
              "  0.869415819644928,\n",
              "  0.8608247637748718,\n",
              "  0.8556700944900513,\n",
              "  0.8659793734550476,\n",
              "  0.8367697596549988,\n",
              "  0.8367697596549988,\n",
              "  0.8608247637748718,\n",
              "  0.8573883175849915,\n",
              "  0.8659793734550476,\n",
              "  0.8470790386199951,\n",
              "  0.8522336483001709,\n",
              "  0.8556700944900513,\n",
              "  0.8814433217048645,\n",
              "  0.8625429272651672,\n",
              "  0.8780068755149841,\n",
              "  0.8797250986099243,\n",
              "  0.8659793734550476,\n",
              "  0.869415819644928,\n",
              "  0.8728522062301636,\n",
              "  0.907216489315033,\n",
              "  0.869415819644928,\n",
              "  0.8865979313850403,\n",
              "  0.9037800431251526,\n",
              "  0.8573883175849915,\n",
              "  0.8676975965499878,\n",
              "  0.869415819644928,\n",
              "  0.876288652420044,\n",
              "  0.8900343775749207,\n",
              "  0.907216489315033,\n",
              "  0.8969594836235046,\n",
              "  0.8711340427398682,\n",
              "  0.8917526006698608,\n",
              "  0.907216489315033,\n",
              "  0.8917526006698608,\n",
              "  0.9089347124099731,\n",
              "  0.8865979313850403,\n",
              "  0.9020618796348572,\n",
              "  0.9123711585998535,\n",
              "  0.8934707641601562,\n",
              "  0.8917526006698608,\n",
              "  0.8969072103500366,\n",
              "  0.8986254334449768,\n",
              "  0.8797250986099243,\n",
              "  0.9140893220901489,\n",
              "  0.8951889872550964,\n",
              "  0.8917526006698608,\n",
              "  0.9037800431251526,\n",
              "  0.8917526006698608,\n",
              "  0.9020618796348572,\n",
              "  0.9106529355049133,\n",
              "  0.8951889872550964,\n",
              "  0.9020618796348572,\n",
              "  0.8935810923576355,\n",
              "  0.8969072103500366,\n",
              "  0.8865979313850403,\n",
              "  0.9209622144699097,\n",
              "  0.9189189076423645,\n",
              "  0.9175257682800293,\n",
              "  0.907216489315033,\n",
              "  0.8986254334449768,\n",
              "  0.9295532703399658,\n",
              "  0.9123711585998535,\n",
              "  0.9278350472450256,\n",
              "  0.9140893220901489,\n",
              "  0.8917526006698608,\n",
              "  0.9123711585998535,\n",
              "  0.8934707641601562,\n",
              "  0.8883161544799805,\n",
              "  0.931271493434906,\n",
              "  0.9192439913749695,\n",
              "  0.9243986010551453,\n",
              "  0.9106529355049133,\n",
              "  0.9089347124099731,\n",
              "  0.9123711585998535,\n",
              "  0.9089347124099731,\n",
              "  0.9158075451850891,\n",
              "  0.900343656539917,\n",
              "  0.9123711585998535,\n",
              "  0.9140893220901489,\n",
              "  0.9054982662200928,\n",
              "  0.9278350472450256,\n",
              "  0.9138513803482056,\n",
              "  0.8934707641601562,\n",
              "  0.9192439913749695,\n",
              "  0.9037800431251526,\n",
              "  0.9037800431251526,\n",
              "  0.9226804375648499,\n",
              "  0.9192439913749695,\n",
              "  0.9089347124099731,\n",
              "  0.907216489315033,\n",
              "  0.9089347124099731,\n",
              "  0.9192439913749695,\n",
              "  0.9243986010551453,\n",
              "  0.9106529355049133,\n",
              "  0.8831614851951599,\n",
              "  0.9261168241500854,\n",
              "  0.9261168241500854,\n",
              "  0.9175257682800293,\n",
              "  0.9484536051750183,\n",
              "  0.9158075451850891,\n",
              "  0.9398625493049622,\n",
              "  0.9243986010551453,\n",
              "  0.907216489315033,\n",
              "  0.9106529355049133,\n",
              "  0.9209622144699097,\n",
              "  0.9192439913749695,\n",
              "  0.9295532703399658,\n",
              "  0.8951889872550964,\n",
              "  0.9295532703399658,\n",
              "  0.938144326210022,\n",
              "  0.8934707641601562,\n",
              "  0.9295532703399658,\n",
              "  0.9209622144699097,\n",
              "  0.9261168241500854,\n",
              "  0.931271493434906,\n",
              "  0.938144326210022,\n",
              "  0.9175257682800293,\n",
              "  0.9226804375648499,\n",
              "  0.9295532703399658,\n",
              "  0.9347078800201416,\n",
              "  0.931271493434906,\n",
              "  0.9192439913749695,\n",
              "  0.931271493434906,\n",
              "  0.9432989954948425,\n",
              "  0.9158075451850891,\n",
              "  0.9089347124099731,\n",
              "  0.9261168241500854,\n",
              "  0.9243986010551453,\n",
              "  0.9243986010551453,\n",
              "  0.931271493434906,\n",
              "  0.9261168241500854,\n",
              "  0.9398625493049622,\n",
              "  0.9307432174682617,\n",
              "  0.9192439913749695,\n",
              "  0.9261168241500854,\n",
              "  0.9329897165298462,\n",
              "  0.9278350472450256,\n",
              "  0.9295532703399658,\n",
              "  0.9261168241500854,\n",
              "  0.9226804375648499,\n",
              "  0.9518900513648987,\n",
              "  0.9106529355049133,\n",
              "  0.938144326210022,\n",
              "  0.9278350472450256,\n",
              "  0.907216489315033,\n",
              "  0.9243986010551453,\n",
              "  0.9364261031150818,\n",
              "  0.9295532703399658,\n",
              "  0.931271493434906,\n",
              "  0.9226804375648499,\n",
              "  0.9398625493049622,\n",
              "  0.9192439913749695,\n",
              "  0.9347078800201416,\n",
              "  0.9587628841400146,\n",
              "  0.9364261031150818,\n",
              "  0.9243986010551453,\n",
              "  0.9347078800201416,\n",
              "  0.931271493434906,\n",
              "  0.9450171589851379,\n",
              "  0.9261168241500854,\n",
              "  0.9278350472450256,\n",
              "  0.9364261031150818,\n",
              "  0.9278350472450256,\n",
              "  0.9536082744598389,\n",
              "  0.9415807723999023,\n",
              "  0.9329897165298462,\n",
              "  0.938144326210022,\n",
              "  0.938144326210022,\n",
              "  0.9243986010551453,\n",
              "  0.9209622144699097,\n",
              "  0.9347078800201416,\n",
              "  0.9192439913749695,\n",
              "  0.9295532703399658,\n",
              "  0.938144326210022,\n",
              "  0.9398625493049622,\n",
              "  0.938144326210022,\n",
              "  0.9347078800201416,\n",
              "  0.9329897165298462,\n",
              "  0.931271493434906,\n",
              "  0.931271493434906,\n",
              "  0.9501718282699585,\n",
              "  0.9261168241500854,\n",
              "  0.9140893220901489,\n",
              "  0.9432989954948425,\n",
              "  0.9329897165298462,\n",
              "  0.9243986010551453,\n",
              "  0.9415807723999023,\n",
              "  0.931271493434906,\n",
              "  0.9398625493049622,\n",
              "  0.9243986010551453,\n",
              "  0.9243986010551453,\n",
              "  0.9295532703399658,\n",
              "  0.9278350472450256,\n",
              "  0.9278350472450256,\n",
              "  0.9364261031150818,\n",
              "  0.9329897165298462,\n",
              "  0.9347078800201416,\n",
              "  0.938144326210022,\n",
              "  0.9364261031150818,\n",
              "  0.938144326210022,\n",
              "  0.9256756901741028,\n",
              "  0.9295532703399658,\n",
              "  0.9415807723999023,\n",
              "  0.9484536051750183,\n",
              "  0.9347078800201416,\n",
              "  0.9295532703399658,\n",
              "  0.9158075451850891,\n",
              "  0.9415807723999023,\n",
              "  0.938144326210022,\n",
              "  0.938144326210022,\n",
              "  0.938144326210022,\n",
              "  0.9209622144699097,\n",
              "  0.9347078800201416,\n",
              "  0.9278350472450256,\n",
              "  0.9278350472450256,\n",
              "  0.9484536051750183,\n",
              "  0.9261168241500854,\n",
              "  0.9467353820800781,\n",
              "  0.9501718282699585,\n",
              "  0.9347078800201416,\n",
              "  0.9415807723999023,\n",
              "  0.9347078800201416,\n",
              "  0.9415807723999023,\n",
              "  0.9432989954948425,\n",
              "  0.9398625493049622,\n",
              "  0.9432989954948425,\n",
              "  0.9536082744598389,\n",
              "  0.9295532703399658,\n",
              "  0.9493243098258972,\n",
              "  0.9467353820800781,\n",
              "  0.9432989954948425,\n",
              "  0.9484536051750183,\n",
              "  0.938144326210022,\n",
              "  0.9501718282699585,\n",
              "  0.9295532703399658,\n",
              "  0.9123711585998535,\n",
              "  0.9295532703399658,\n",
              "  0.9467353820800781,\n",
              "  0.9398625493049622,\n",
              "  0.9432989954948425,\n",
              "  0.9347078800201416,\n",
              "  0.9364261031150818,\n",
              "  0.9158075451850891,\n",
              "  0.9432989954948425,\n",
              "  0.9364261031150818,\n",
              "  0.938144326210022,\n",
              "  0.9553264379501343,\n",
              "  0.9329897165298462,\n",
              "  0.9553264379501343,\n",
              "  0.9398625493049622,\n",
              "  0.9467353820800781,\n",
              "  0.9432989954948425,\n",
              "  0.9484536051750183,\n",
              "  0.938144326210022,\n",
              "  0.9364261031150818,\n",
              "  0.9553264379501343,\n",
              "  0.9484536051750183,\n",
              "  0.9347078800201416,\n",
              "  0.9518900513648987,\n",
              "  0.9364261031150818,\n",
              "  0.9450171589851379,\n",
              "  0.9192439913749695,\n",
              "  0.9467353820800781,\n",
              "  0.9536082744598389,\n",
              "  0.938144326210022,\n",
              "  0.9364261031150818,\n",
              "  0.9604811072349548,\n",
              "  0.9398625493049622,\n",
              "  0.9364261031150818,\n",
              "  0.9450171589851379,\n",
              "  0.9329897165298462,\n",
              "  0.9450171589851379,\n",
              "  0.9364261031150818,\n",
              "  0.938144326210022,\n",
              "  0.9501718282699585,\n",
              "  0.9209622144699097,\n",
              "  0.9329897165298462,\n",
              "  0.9536082744598389,\n",
              "  0.9398625493049622,\n",
              "  0.9243986010551453,\n",
              "  0.9484536051750183,\n",
              "  0.9278350472450256,\n",
              "  0.9484536051750183,\n",
              "  0.9484536051750183,\n",
              "  0.9587628841400146,\n",
              "  0.9295532703399658,\n",
              "  0.9518900513648987,\n",
              "  0.9553264379501343,\n",
              "  0.9484536051750183,\n",
              "  0.9364261031150818,\n",
              "  0.9518900513648987,\n",
              "  0.9484536051750183,\n",
              "  0.9347078800201416,\n",
              "  0.938144326210022,\n",
              "  0.9206081032752991,\n",
              "  0.9243986010551453,\n",
              "  0.9329897165298462,\n",
              "  0.907216489315033,\n",
              "  0.9261168241500854,\n",
              "  0.9415807723999023,\n",
              "  0.9587628841400146,\n",
              "  0.9484536051750183,\n",
              "  0.9364261031150818,\n",
              "  0.9398625493049622,\n",
              "  0.9415807723999023,\n",
              "  0.9347078800201416,\n",
              "  0.9226804375648499,\n",
              "  0.9398625493049622,\n",
              "  0.9450171589851379,\n",
              "  0.9415807723999023,\n",
              "  0.9501718282699585,\n",
              "  0.9329897165298462,\n",
              "  0.9570446610450745,\n",
              "  0.9329897165298462,\n",
              "  0.938144326210022,\n",
              "  0.9415807723999023,\n",
              "  0.9570446610450745,\n",
              "  0.9243986010551453,\n",
              "  0.9415807723999023,\n",
              "  0.9432989954948425,\n",
              "  0.9209622144699097,\n",
              "  0.9553264379501343,\n",
              "  0.9501718282699585,\n",
              "  0.938144326210022,\n",
              "  0.9450171589851379,\n",
              "  0.9398625493049622,\n",
              "  0.9295532703399658,\n",
              "  0.9484536051750183,\n",
              "  0.9295532703399658,\n",
              "  0.9398625493049622,\n",
              "  0.9398625493049622,\n",
              "  0.938144326210022,\n",
              "  0.9467353820800781,\n",
              "  0.9425675868988037,\n",
              "  0.9140893220901489,\n",
              "  0.938144326210022,\n",
              "  0.9432989954948425,\n",
              "  0.9587628841400146,\n",
              "  0.9570446610450745,\n",
              "  0.9484536051750183,\n",
              "  0.9587628841400146,\n",
              "  0.9364261031150818,\n",
              "  0.9484536051750183,\n",
              "  0.938144326210022,\n",
              "  0.9278350472450256,\n",
              "  0.9432989954948425,\n",
              "  0.9375,\n",
              "  0.9364261031150818,\n",
              "  0.9278350472450256,\n",
              "  0.9484536051750183,\n",
              "  0.9501718282699585,\n",
              "  0.9467353820800781,\n",
              "  0.9450171589851379,\n",
              "  0.9347078800201416,\n",
              "  0.9553264379501343,\n",
              "  0.9604811072349548,\n",
              "  0.931271493434906,\n",
              "  0.9261168241500854,\n",
              "  0.9553264379501343,\n",
              "  0.9329897165298462,\n",
              "  0.9501718282699585,\n",
              "  0.9484536051750183,\n",
              "  0.9501718282699585,\n",
              "  0.9501718282699585,\n",
              "  0.9501718282699585,\n",
              "  0.9467353820800781,\n",
              "  0.9432989954948425,\n",
              "  0.9432989954948425,\n",
              "  0.9329897165298462,\n",
              "  0.9518900513648987,\n",
              "  0.9510135054588318,\n",
              "  0.9484536051750183,\n",
              "  0.9553264379501343,\n",
              "  0.9398625493049622,\n",
              "  0.9587628841400146,\n",
              "  0.9536082744598389,\n",
              "  0.9432989954948425,\n",
              "  0.9432989954948425,\n",
              "  0.9415807723999023,\n",
              "  0.938144326210022,\n",
              "  0.9518900513648987,\n",
              "  0.9467353820800781,\n",
              "  0.9341216087341309,\n",
              "  0.9398625493049622,\n",
              "  0.962199330329895,\n",
              "  0.9278350472450256,\n",
              "  0.9432989954948425,\n",
              "  0.9415807723999023,\n",
              "  0.962199330329895,\n",
              "  0.9467353820800781,\n",
              "  0.9450171589851379,\n",
              "  0.962199330329895,\n",
              "  0.9536082744598389,\n",
              "  0.9484536051750183,\n",
              "  0.9415807723999023,\n",
              "  0.9553264379501343,\n",
              "  0.9467353820800781,\n",
              "  0.9587628841400146,\n",
              "  0.9553264379501343,\n",
              "  0.962199330329895,\n",
              "  0.9518900513648987,\n",
              "  0.9329897165298462,\n",
              "  0.9518900513648987,\n",
              "  0.9432989954948425,\n",
              "  0.9604811072349548,\n",
              "  0.9587628841400146,\n",
              "  0.9278350472450256,\n",
              "  0.9639175534248352,\n",
              "  0.9450171589851379,\n",
              "  0.9501718282699585,\n",
              "  0.9553264379501343,\n",
              "  0.9587628841400146,\n",
              "  0.9518900513648987,\n",
              "  0.9577702879905701,\n",
              "  0.9467353820800781,\n",
              "  0.9450171589851379,\n",
              "  0.9484536051750183,\n",
              "  0.9501718282699585,\n",
              "  0.9587628841400146,\n",
              "  0.9364261031150818,\n",
              "  0.9347078800201416,\n",
              "  0.9450171589851379,\n",
              "  0.9484536051750183,\n",
              "  0.9518900513648987,\n",
              "  0.9501718282699585,\n",
              "  0.9484536051750183,\n",
              "  0.9570446610450745,\n",
              "  0.9639175534248352,\n",
              "  0.9432989954948425,\n",
              "  0.9484536051750183,\n",
              "  0.9467353820800781,\n",
              "  0.9518900513648987,\n",
              "  0.9415807723999023,\n",
              "  0.9570446610450745,\n",
              "  0.9442567825317383,\n",
              "  0.9518900513648987,\n",
              "  0.9415807723999023,\n",
              "  0.9295532703399658,\n",
              "  0.9570446610450745,\n",
              "  0.9604811072349548,\n",
              "  0.9415807723999023,\n",
              "  0.9432989954948425,\n",
              "  0.9553264379501343,\n",
              "  0.9450171589851379,\n",
              "  0.9329897165298462,\n",
              "  0.9518900513648987,\n",
              "  0.9432989954948425,\n",
              "  0.9467353820800781,\n",
              "  0.9432989954948425,\n",
              "  0.9501718282699585,\n",
              "  0.9587628841400146,\n",
              "  0.969072163105011,\n",
              "  0.9415807723999023,\n",
              "  0.9536082744598389,\n",
              "  0.9484536051750183,\n",
              "  0.9398625493049622,\n",
              "  0.9484536051750183,\n",
              "  0.9415807723999023,\n",
              "  0.9536082744598389,\n",
              "  0.9484536051750183,\n",
              "  0.9553264379501343,\n",
              "  0.9570446610450745,\n",
              "  0.9536082744598389,\n",
              "  0.9432989954948425,\n",
              "  0.9553264379501343,\n",
              "  0.9484536051750183,\n",
              "  0.9450171589851379,\n",
              "  0.9484536051750183,\n",
              "  0.9656357169151306,\n",
              "  0.9450171589851379,\n",
              "  0.9553264379501343,\n",
              "  0.9553264379501343,\n",
              "  0.9536082744598389,\n",
              "  0.9467353820800781,\n",
              "  0.9536082744598389,\n",
              "  0.9604811072349548,\n",
              "  0.9415807723999023,\n",
              "  0.9364261031150818,\n",
              "  0.9484536051750183,\n",
              "  0.9560810923576355,\n",
              "  0.9536082744598389,\n",
              "  0.9570446610450745,\n",
              "  0.9570446610450745,\n",
              "  0.9501718282699585,\n",
              "  0.9467353820800781,\n",
              "  0.9536082744598389,\n",
              "  0.9295532703399658,\n",
              "  0.9553264379501343,\n",
              "  0.9467353820800781,\n",
              "  0.9484536051750183,\n",
              "  0.9604811072349548,\n",
              "  0.9587628841400146,\n",
              "  0.9364261031150818,\n",
              "  0.9484536051750183,\n",
              "  0.9432989954948425,\n",
              "  0.9570446610450745,\n",
              "  0.9347078800201416,\n",
              "  0.9553264379501343,\n",
              "  0.9484536051750183,\n",
              "  0.9415807723999023,\n",
              "  0.9570446610450745,\n",
              "  0.9518900513648987,\n",
              "  0.9226804375648499,\n",
              "  0.9501718282699585,\n",
              "  0.9467353820800781,\n",
              "  0.938144326210022,\n",
              "  0.9432989954948425,\n",
              "  0.9553264379501343,\n",
              "  0.9467353820800781,\n",
              "  0.9484536051750183,\n",
              "  0.9639175534248352,\n",
              "  0.938144326210022,\n",
              "  0.9518900513648987,\n",
              "  0.931271493434906,\n",
              "  0.9450171589851379,\n",
              "  0.9518900513648987,\n",
              "  0.9536082744598389,\n",
              "  0.9398625493049622,\n",
              "  0.9536082744598389,\n",
              "  0.9518900513648987,\n",
              "  0.9570446610450745,\n",
              "  0.9553264379501343,\n",
              "  0.9501718282699585,\n",
              "  0.9484536051750183,\n",
              "  0.9639175534248352,\n",
              "  0.9518900513648987,\n",
              "  0.969072163105011,\n",
              "  0.9467353820800781,\n",
              "  0.938144326210022,\n",
              "  0.9467353820800781,\n",
              "  0.9553264379501343,\n",
              "  0.9364261031150818,\n",
              "  0.9518900513648987,\n",
              "  0.962199330329895,\n",
              "  0.9553264379501343,\n",
              "  0.9484536051750183,\n",
              "  0.9329897165298462,\n",
              "  0.9501718282699585,\n",
              "  0.9570446610450745,\n",
              "  0.9432989954948425,\n",
              "  0.938144326210022,\n",
              "  0.9604811072349548,\n",
              "  0.9639175534248352,\n",
              "  0.938144326210022,\n",
              "  0.9553264379501343,\n",
              "  0.9656357169151306,\n",
              "  0.9639175534248352,\n",
              "  0.9553264379501343,\n",
              "  0.9467353820800781,\n",
              "  0.9639175534248352,\n",
              "  0.9432989954948425,\n",
              "  0.9364261031150818,\n",
              "  0.9518900513648987,\n",
              "  0.9347078800201416,\n",
              "  0.9467353820800781,\n",
              "  0.962199330329895,\n",
              "  0.9656357169151306,\n",
              "  0.9570446610450745,\n",
              "  0.9518900513648987,\n",
              "  0.9639175534248352,\n",
              "  0.9484536051750183,\n",
              "  0.9364261031150818,\n",
              "  0.9450171589851379,\n",
              "  0.9553264379501343,\n",
              "  0.9587628841400146,\n",
              "  0.9570446610450745,\n",
              "  0.9398625493049622,\n",
              "  0.962199330329895,\n",
              "  0.9656357169151306,\n",
              "  0.931271493434906,\n",
              "  0.9484536051750183,\n",
              "  0.9518900513648987,\n",
              "  0.9295532703399658,\n",
              "  0.9493243098258972,\n",
              "  0.9553264379501343,\n",
              "  0.9432989954948425,\n",
              "  0.9518900513648987,\n",
              "  0.9570446610450745,\n",
              "  0.9415807723999023,\n",
              "  0.9518900513648987,\n",
              "  0.9536082744598389,\n",
              "  0.9604811072349548,\n",
              "  0.9467353820800781,\n",
              "  0.9518900513648987,\n",
              "  0.9587628841400146,\n",
              "  0.938144326210022,\n",
              "  0.9501718282699585,\n",
              "  0.9398625493049622,\n",
              "  0.9501718282699585,\n",
              "  0.9587628841400146,\n",
              "  0.9501718282699585,\n",
              "  0.9484536051750183,\n",
              "  0.9639175534248352,\n",
              "  0.9553264379501343,\n",
              "  0.9450171589851379,\n",
              "  0.9432989954948425,\n",
              "  0.9553264379501343,\n",
              "  0.9570446610450745,\n",
              "  0.9432989954948425,\n",
              "  0.9673539400100708,\n",
              "  0.9501718282699585,\n",
              "  0.9501718282699585,\n",
              "  0.9518900513648987,\n",
              "  0.9432989954948425,\n",
              "  0.9450171589851379,\n",
              "  0.9432989954948425,\n",
              "  0.962199330329895,\n",
              "  0.9364261031150818,\n",
              "  0.9467353820800781,\n",
              "  0.9553264379501343,\n",
              "  0.9656357169151306,\n",
              "  0.9604811072349548,\n",
              "  0.962199330329895,\n",
              "  0.9604811072349548,\n",
              "  0.9459459185600281,\n",
              "  0.9467353820800781,\n",
              "  0.9450171589851379,\n",
              "  0.975944995880127,\n",
              "  0.962199330329895,\n",
              "  0.9432989954948425,\n",
              "  0.9604811072349548,\n",
              "  0.938144326210022,\n",
              "  0.9604811072349548,\n",
              "  0.9501718282699585,\n",
              "  0.9639175534248352,\n",
              "  0.9536082744598389,\n",
              "  0.9604811072349548,\n",
              "  0.9398625493049622,\n",
              "  0.9570446610450745,\n",
              "  0.9518900513648987,\n",
              "  0.9518900513648987,\n",
              "  0.9656357169151306,\n",
              "  0.9467353820800781,\n",
              "  0.9398625493049622,\n",
              "  0.9450171589851379,\n",
              "  0.9518900513648987,\n",
              "  0.9518900513648987,\n",
              "  0.9432989954948425,\n",
              "  0.9398625493049622,\n",
              "  0.9639175534248352,\n",
              "  0.9467353820800781,\n",
              "  0.9243986010551453,\n",
              "  0.9639175534248352,\n",
              "  0.9639175534248352,\n",
              "  0.9459459185600281,\n",
              "  0.9587628841400146,\n",
              "  0.962199330329895,\n",
              "  0.9639175534248352,\n",
              "  0.969072163105011,\n",
              "  0.9493243098258972,\n",
              "  0.9656357169151306,\n",
              "  0.9501718282699585,\n",
              "  0.9450171589851379,\n",
              "  0.9398625493049622,\n",
              "  0.9415807723999023,\n",
              "  0.9656357169151306,\n",
              "  0.9484536051750183,\n",
              "  0.9518900513648987,\n",
              "  0.9450171589851379,\n",
              "  0.962199330329895,\n",
              "  0.9587628841400146,\n",
              "  0.9604811072349548,\n",
              "  0.9501718282699585,\n",
              "  0.9450171589851379,\n",
              "  0.9553264379501343,\n",
              "  0.9415807723999023,\n",
              "  0.9656357169151306,\n",
              "  0.9467353820800781,\n",
              "  0.9570446610450745,\n",
              "  0.9536082744598389,\n",
              "  0.9553264379501343,\n",
              "  0.9570446610450745,\n",
              "  0.962199330329895,\n",
              "  0.9432989954948425,\n",
              "  0.9501718282699585,\n",
              "  0.9553264379501343,\n",
              "  0.9536082744598389,\n",
              "  0.9604811072349548,\n",
              "  0.9536082744598389,\n",
              "  0.9570446610450745,\n",
              "  0.9594594836235046,\n",
              "  0.9398625493049622,\n",
              "  0.9611486196517944,\n",
              "  0.9707903861999512,\n",
              "  0.9587628841400146,\n",
              "  0.9604811072349548,\n",
              "  0.9536082744598389,\n",
              "  0.9570446610450745,\n",
              "  0.9570446610450745,\n",
              "  0.9729729890823364,\n",
              "  0.9673539400100708,\n",
              "  0.9484536051750183,\n",
              "  0.9364261031150818,\n",
              "  0.969072163105011,\n",
              "  0.9570446610450745,\n",
              "  0.9518900513648987,\n",
              "  0.9501718282699585,\n",
              "  0.9707903861999512,\n",
              "  0.9450171589851379,\n",
              "  0.9587628841400146,\n",
              "  0.969072163105011,\n",
              "  0.9587628841400146,\n",
              "  0.9604811072349548,\n",
              "  0.9570446610450745,\n",
              "  0.9725086092948914,\n",
              "  0.9501718282699585,\n",
              "  0.9604811072349548,\n",
              "  0.9553264379501343,\n",
              "  0.9501718282699585,\n",
              "  0.9518900513648987,\n",
              "  0.962199330329895,\n",
              "  0.975944995880127,\n",
              "  0.9553264379501343,\n",
              "  0.9467353820800781,\n",
              "  0.9604811072349548,\n",
              "  0.9398625493049622,\n",
              "  0.9604811072349548,\n",
              "  0.9518900513648987,\n",
              "  0.9450171589851379,\n",
              "  0.9553264379501343,\n",
              "  0.9450171589851379,\n",
              "  0.9553264379501343,\n",
              "  0.962199330329895,\n",
              "  0.9501718282699585,\n",
              "  0.9501718282699585,\n",
              "  0.9536082744598389,\n",
              "  0.975944995880127,\n",
              "  0.9467353820800781,\n",
              "  0.962199330329895,\n",
              "  0.9450171589851379,\n",
              "  0.969072163105011,\n",
              "  0.9656357169151306,\n",
              "  0.9673539400100708,\n",
              "  0.9501718282699585,\n",
              "  0.9639175534248352,\n",
              "  0.9570446610450745,\n",
              "  0.962199330329895,\n",
              "  0.975944995880127,\n",
              "  0.9587628841400146,\n",
              "  0.969072163105011,\n",
              "  0.9587628841400146,\n",
              "  0.9656357169151306,\n",
              "  0.9415807723999023,\n",
              "  0.9707903861999512,\n",
              "  0.9639175534248352,\n",
              "  0.9467353820800781,\n",
              "  0.9364261031150818,\n",
              "  0.9673539400100708,\n",
              "  0.9604811072349548,\n",
              "  0.9536082744598389,\n",
              "  0.9459459185600281,\n",
              "  0.9518900513648987,\n",
              "  0.9484536051750183,\n",
              "  0.9639175534248352,\n",
              "  0.9553264379501343,\n",
              "  0.9553264379501343,\n",
              "  0.9673539400100708,\n",
              "  0.9604811072349548,\n",
              "  0.9587628841400146,\n",
              "  0.9450171589851379,\n",
              "  0.9570446610450745,\n",
              "  0.9673539400100708,\n",
              "  0.9484536051750183,\n",
              "  0.9570446610450745,\n",
              "  0.9518900513648987,\n",
              "  0.9587628841400146,\n",
              "  0.9673539400100708,\n",
              "  0.9570446610450745,\n",
              "  0.9587628841400146,\n",
              "  0.9673539400100708,\n",
              "  0.9484536051750183,\n",
              "  0.9656357169151306,\n",
              "  0.9604811072349548,\n",
              "  0.9639175534248352,\n",
              "  0.9536082744598389,\n",
              "  0.975944995880127,\n",
              "  0.9553264379501343,\n",
              "  0.9587628841400146,\n",
              "  0.9536082744598389,\n",
              "  0.9536082744598389,\n",
              "  0.9536082744598389,\n",
              "  0.9493243098258972,\n",
              "  0.938144326210022,\n",
              "  0.9656357169151306,\n",
              "  0.962199330329895,\n",
              "  0.9587628841400146,\n",
              "  0.9553264379501343,\n",
              "  0.9604811072349548,\n",
              "  0.9501718282699585,\n",
              "  0.9347078800201416,\n",
              "  0.9673539400100708,\n",
              "  0.9553264379501343,\n",
              "  0.9553264379501343,\n",
              "  0.9467353820800781,\n",
              "  0.9587628841400146,\n",
              "  0.9587628841400146,\n",
              "  0.9587628841400146,\n",
              "  0.9587628841400146,\n",
              "  0.9536082744598389,\n",
              "  0.9553264379501343,\n",
              "  0.9725086092948914,\n",
              "  0.9467353820800781,\n",
              "  0.9518900513648987,\n",
              "  0.9467353820800781,\n",
              "  0.9518900513648987,\n",
              "  0.9467353820800781,\n",
              "  0.9570446610450745,\n",
              "  0.9577702879905701,\n",
              "  0.9518900513648987,\n",
              "  0.9587628841400146,\n",
              "  0.962199330329895,\n",
              "  0.9604811072349548,\n",
              "  0.9673539400100708,\n",
              "  0.9553264379501343,\n",
              "  0.9707903861999512,\n",
              "  0.9536082744598389,\n",
              "  0.9536082744598389,\n",
              "  0.9639175534248352,\n",
              "  0.975944995880127,\n",
              "  0.9656357169151306,\n",
              "  0.9570446610450745,\n",
              "  0.9604811072349548,\n",
              "  0.9587628841400146,\n",
              "  0.9604811072349548,\n",
              "  0.9553264379501343,\n",
              "  0.9536082744598389,\n",
              "  0.9553264379501343,\n",
              "  0.9570446610450745,\n",
              "  0.9415807723999023,\n",
              "  0.9570446610450745,\n",
              "  0.9501718282699585,\n",
              "  0.9501718282699585,\n",
              "  0.962199330329895,\n",
              "  0.962199330329895,\n",
              "  0.9536082744598389,\n",
              "  0.9656357169151306,\n",
              "  0.9484536051750183,\n",
              "  0.9793814420700073,\n",
              "  0.9536082744598389,\n",
              "  0.9604811072349548,\n",
              "  0.9501718282699585,\n",
              "  0.9673539400100708,\n",
              "  0.9570446610450745,\n",
              "  0.9432989954948425,\n",
              "  0.9501718282699585,\n",
              "  0.9604811072349548,\n",
              "  0.9432989954948425,\n",
              "  0.9536082744598389,\n",
              "  0.938144326210022,\n",
              "  0.9776632189750671,\n",
              "  0.9587628841400146,\n",
              "  0.9570446610450745,\n",
              "  0.9673539400100708,\n",
              "  0.962199330329895,\n",
              "  0.9639175534248352,\n",
              "  0.9553264379501343,\n",
              "  0.9604811072349548,\n",
              "  0.9639175534248352,\n",
              "  0.9501718282699585,\n",
              "  0.9707903861999512,\n",
              "  0.9639175534248352,\n",
              "  0.9553264379501343,\n",
              "  0.9776632189750671,\n",
              "  0.9587628841400146,\n",
              "  0.975944995880127,\n",
              "  0.9518900513648987,\n",
              "  0.962199330329895,\n",
              "  0.9467353820800781,\n",
              "  0.9673539400100708,\n",
              "  0.9611486196517944,\n",
              "  0.9673539400100708,\n",
              "  0.9604811072349548,\n",
              "  0.962199330329895,\n",
              "  0.962199330329895,\n",
              "  0.9484536051750183,\n",
              "  0.9604811072349548,\n",
              "  0.9673539400100708,\n",
              "  0.9656357169151306,\n",
              "  0.9553264379501343,\n",
              "  0.9639175534248352,\n",
              "  0.962199330329895,\n",
              "  0.9518900513648987,\n",
              "  0.9587628841400146,\n",
              "  0.9639175534248352,\n",
              "  0.9450171589851379,\n",
              "  0.962199330329895,\n",
              "  0.969072163105011,\n",
              "  0.9536082744598389,\n",
              "  0.9639175534248352,\n",
              "  0.9604811072349548,\n",
              "  0.9725086092948914,\n",
              "  0.9639175534248352,\n",
              "  0.9467353820800781,\n",
              "  0.9467353820800781,\n",
              "  0.9639175534248352,\n",
              "  0.9570446610450745,\n",
              "  0.9656357169151306,\n",
              "  0.9484536051750183,\n",
              "  0.9527027010917664,\n",
              "  0.9398625493049622,\n",
              "  0.9484536051750183,\n",
              "  0.9707903861999512,\n",
              "  0.962837815284729,\n",
              "  0.962837815284729,\n",
              "  0.9467353820800781,\n",
              "  0.962199330329895,\n",
              "  0.9553264379501343,\n",
              "  0.9467353820800781,\n",
              "  0.9536082744598389,\n",
              "  0.9587628841400146,\n",
              "  0.9329897165298462,\n",
              "  0.9656357169151306,\n",
              "  0.9604811072349548,\n",
              "  0.9484536051750183,\n",
              "  0.9553264379501343,\n",
              "  0.9656357169151306,\n",
              "  0.9518900513648987,\n",
              "  0.9536082744598389,\n",
              "  0.9656357169151306,\n",
              "  0.962199330329895,\n",
              "  0.9656357169151306,\n",
              "  0.9501718282699585,\n",
              "  0.9673539400100708,\n",
              "  0.9639175534248352,\n",
              "  0.9587628841400146,\n",
              "  0.9501718282699585,\n",
              "  0.9639175534248352,\n",
              "  0.9604811072349548,\n",
              "  0.9570446610450745,\n",
              "  0.9398625493049622,\n",
              "  0.9707903861999512,\n",
              "  0.969072163105011,\n",
              "  0.9673539400100708,\n",
              "  0.938144326210022,\n",
              "  0.9656357169151306,\n",
              "  0.9553264379501343,\n",
              "  0.962199330329895,\n",
              "  0.9467353820800781,\n",
              "  0.9450171589851379,\n",
              "  0.9484536051750183,\n",
              "  0.9587628841400146,\n",
              "  0.9793814420700073,\n",
              "  0.969072163105011,\n",
              "  0.9536082744598389,\n",
              "  0.9604811072349548,\n",
              "  0.9776632189750671,\n",
              "  0.9639175534248352,\n",
              "  0.9467353820800781,\n",
              "  0.9484536051750183,\n",
              "  0.9570446610450745,\n",
              "  0.9484536051750183,\n",
              "  0.9604811072349548,\n",
              "  0.9518900513648987,\n",
              "  0.9570446610450745,\n",
              "  0.9725086092948914,\n",
              "  0.9536082744598389,\n",
              "  0.9536082744598389,\n",
              "  0.9604811072349548,\n",
              "  0.969072163105011,\n",
              "  0.9570446610450745,\n",
              "  0.969072163105011,\n",
              "  0.9467353820800781,\n",
              "  0.9639175534248352,\n",
              "  0.9364261031150818,\n",
              "  0.9398625493049622,\n",
              "  0.9725086092948914],\n",
              " 'val_loss': [0.8469046950340271,\n",
              "  2.9448375701904297,\n",
              "  1.6020174026489258,\n",
              "  1.1226571798324585,\n",
              "  1.379268765449524,\n",
              "  1.2219470739364624,\n",
              "  2.0476467609405518,\n",
              "  1.7154299020767212,\n",
              "  1.2728739976882935,\n",
              "  2.8910744190216064,\n",
              "  2.062098503112793,\n",
              "  1.2622159719467163,\n",
              "  1.042202115058899,\n",
              "  1.6307660341262817,\n",
              "  0.9945377707481384,\n",
              "  1.3811702728271484,\n",
              "  0.8123354911804199,\n",
              "  0.6931015849113464,\n",
              "  0.6941370368003845,\n",
              "  0.5069363713264465,\n",
              "  0.9330604076385498,\n",
              "  1.2549177408218384,\n",
              "  0.6408336758613586,\n",
              "  2.163642406463623,\n",
              "  0.8399145603179932,\n",
              "  1.0021579265594482,\n",
              "  1.1305612325668335,\n",
              "  1.0660271644592285,\n",
              "  0.7394755482673645,\n",
              "  1.0180567502975464,\n",
              "  0.9531733393669128,\n",
              "  0.7613942623138428,\n",
              "  1.149966835975647,\n",
              "  1.0081934928894043,\n",
              "  0.7100973725318909,\n",
              "  0.8331260681152344,\n",
              "  0.7666729092597961,\n",
              "  1.433390736579895,\n",
              "  0.8525645732879639,\n",
              "  1.255793571472168,\n",
              "  1.3420864343643188,\n",
              "  0.9322843551635742,\n",
              "  1.2220317125320435,\n",
              "  1.1873589754104614,\n",
              "  1.0882447957992554,\n",
              "  0.9128217697143555,\n",
              "  1.1626898050308228,\n",
              "  1.5227165222167969,\n",
              "  0.8093996047973633,\n",
              "  0.8947073817253113,\n",
              "  0.8033679127693176,\n",
              "  1.0659618377685547,\n",
              "  0.8417848944664001,\n",
              "  0.8124393820762634,\n",
              "  1.0491721630096436,\n",
              "  0.9287192225456238,\n",
              "  0.8632087111473083,\n",
              "  1.0054398775100708,\n",
              "  1.0550662279129028,\n",
              "  0.9098101258277893,\n",
              "  1.3028022050857544,\n",
              "  1.4247163534164429,\n",
              "  1.0450891256332397,\n",
              "  1.4063911437988281,\n",
              "  0.8290157318115234,\n",
              "  0.985789954662323,\n",
              "  1.052789330482483,\n",
              "  0.955981969833374,\n",
              "  1.1560479402542114,\n",
              "  1.1467570066452026,\n",
              "  0.8537012934684753,\n",
              "  1.059760570526123,\n",
              "  0.8073079586029053,\n",
              "  1.0444005727767944,\n",
              "  1.7842885255813599,\n",
              "  1.0994575023651123,\n",
              "  1.268079400062561,\n",
              "  1.1565818786621094,\n",
              "  1.148481011390686,\n",
              "  0.8630006313323975,\n",
              "  0.946964681148529,\n",
              "  0.7604930996894836,\n",
              "  0.805072546005249,\n",
              "  0.7326691746711731,\n",
              "  1.0033046007156372,\n",
              "  1.3310095071792603,\n",
              "  0.9786701202392578,\n",
              "  1.192654013633728,\n",
              "  1.7501827478408813,\n",
              "  1.3326414823532104,\n",
              "  1.2232861518859863,\n",
              "  1.3120557069778442,\n",
              "  1.456292748451233,\n",
              "  1.44536292552948,\n",
              "  1.207783579826355,\n",
              "  1.357716679573059,\n",
              "  0.9606645107269287,\n",
              "  0.9651329517364502,\n",
              "  1.2190136909484863,\n",
              "  1.3960756063461304,\n",
              "  1.3221014738082886,\n",
              "  1.567930817604065,\n",
              "  1.8246917724609375,\n",
              "  1.9654264450073242,\n",
              "  1.6070307493209839,\n",
              "  1.561075210571289,\n",
              "  1.4067440032958984,\n",
              "  1.3068026304244995,\n",
              "  1.2156275510787964,\n",
              "  1.5680290460586548,\n",
              "  1.341126799583435,\n",
              "  1.3887349367141724,\n",
              "  1.3747259378433228,\n",
              "  1.2367967367172241,\n",
              "  1.697615623474121,\n",
              "  1.5519253015518188,\n",
              "  1.2820264101028442,\n",
              "  1.5886483192443848,\n",
              "  1.2714794874191284,\n",
              "  1.5352692604064941,\n",
              "  1.6695013046264648,\n",
              "  1.3088774681091309,\n",
              "  1.4396954774856567,\n",
              "  1.4372453689575195,\n",
              "  1.3707579374313354,\n",
              "  1.5930427312850952,\n",
              "  1.3292220830917358,\n",
              "  1.4398328065872192,\n",
              "  1.4233465194702148,\n",
              "  1.264651894569397,\n",
              "  1.7507514953613281,\n",
              "  1.7912389039993286,\n",
              "  1.3695071935653687,\n",
              "  1.6953239440917969,\n",
              "  1.6287375688552856,\n",
              "  1.511163592338562,\n",
              "  1.5904264450073242,\n",
              "  2.0193963050842285,\n",
              "  1.4572440385818481,\n",
              "  1.524701714515686,\n",
              "  1.4534605741500854,\n",
              "  1.2670615911483765,\n",
              "  1.2812973260879517,\n",
              "  1.2335456609725952,\n",
              "  1.2853745222091675,\n",
              "  1.1408710479736328,\n",
              "  1.3252085447311401,\n",
              "  1.6823054552078247,\n",
              "  1.4184828996658325,\n",
              "  1.0599533319473267,\n",
              "  1.0918322801589966,\n",
              "  1.3587857484817505,\n",
              "  1.17062509059906,\n",
              "  1.5199891328811646,\n",
              "  1.1564050912857056,\n",
              "  1.37904691696167,\n",
              "  1.4365819692611694,\n",
              "  1.8869906663894653,\n",
              "  1.3493345975875854,\n",
              "  1.6322221755981445,\n",
              "  1.728337287902832,\n",
              "  1.4006835222244263,\n",
              "  1.1885318756103516,\n",
              "  1.0524981021881104,\n",
              "  1.5835505723953247,\n",
              "  1.931601643562317,\n",
              "  2.111332654953003,\n",
              "  1.731676459312439,\n",
              "  1.6602991819381714,\n",
              "  1.9794872999191284,\n",
              "  1.6273250579833984,\n",
              "  1.574804663658142,\n",
              "  1.6339678764343262,\n",
              "  1.677053451538086,\n",
              "  1.5016522407531738,\n",
              "  1.2151551246643066,\n",
              "  1.197571873664856,\n",
              "  1.3684110641479492,\n",
              "  1.1709318161010742,\n",
              "  1.321804404258728,\n",
              "  1.4048290252685547,\n",
              "  1.240554928779602,\n",
              "  1.4856630563735962,\n",
              "  2.0048773288726807,\n",
              "  1.7218713760375977,\n",
              "  2.1154470443725586,\n",
              "  1.9402087926864624,\n",
              "  2.314558744430542,\n",
              "  2.0868523120880127,\n",
              "  2.28035831451416,\n",
              "  2.002431631088257,\n",
              "  1.4585498571395874,\n",
              "  1.5316804647445679,\n",
              "  1.6037040948867798,\n",
              "  1.7053202390670776,\n",
              "  1.916735053062439,\n",
              "  1.7605358362197876,\n",
              "  2.2781870365142822,\n",
              "  1.8202482461929321,\n",
              "  1.7599915266036987,\n",
              "  2.016575574874878,\n",
              "  1.6123536825180054,\n",
              "  1.9233883619308472,\n",
              "  1.6830121278762817,\n",
              "  1.3437906503677368,\n",
              "  1.6476486921310425,\n",
              "  1.7942028045654297,\n",
              "  1.224353551864624,\n",
              "  1.6516331434249878,\n",
              "  1.5369387865066528,\n",
              "  1.8032582998275757,\n",
              "  1.5365962982177734,\n",
              "  1.8415600061416626,\n",
              "  1.8595844507217407,\n",
              "  1.525717854499817,\n",
              "  2.060317039489746,\n",
              "  1.7481341361999512,\n",
              "  1.8212181329727173,\n",
              "  1.8585880994796753,\n",
              "  1.5494003295898438,\n",
              "  1.0744128227233887,\n",
              "  1.6546436548233032,\n",
              "  1.0755387544631958,\n",
              "  1.2074428796768188,\n",
              "  1.5344327688217163,\n",
              "  1.740139365196228,\n",
              "  1.4783010482788086,\n",
              "  1.760170817375183,\n",
              "  1.6413618326187134,\n",
              "  2.1222691535949707,\n",
              "  1.4754866361618042,\n",
              "  1.5694268941879272,\n",
              "  1.3284772634506226,\n",
              "  2.021991014480591,\n",
              "  1.7357231378555298,\n",
              "  1.6877517700195312,\n",
              "  1.2664347887039185,\n",
              "  1.4815963506698608,\n",
              "  1.3979530334472656,\n",
              "  1.4891433715820312,\n",
              "  1.750801682472229,\n",
              "  1.700623869895935,\n",
              "  2.1986639499664307,\n",
              "  2.1127939224243164,\n",
              "  2.1264543533325195,\n",
              "  2.616143226623535,\n",
              "  1.9650911092758179,\n",
              "  2.123835325241089,\n",
              "  1.5381704568862915,\n",
              "  1.2795603275299072,\n",
              "  1.5472989082336426,\n",
              "  1.6679128408432007,\n",
              "  2.8007869720458984,\n",
              "  1.639830231666565,\n",
              "  1.6188044548034668,\n",
              "  1.4082612991333008,\n",
              "  1.2874835729599,\n",
              "  1.536092758178711,\n",
              "  1.235181450843811,\n",
              "  1.8233839273452759,\n",
              "  2.3201053142547607,\n",
              "  1.9733847379684448,\n",
              "  1.6576873064041138,\n",
              "  1.2017484903335571,\n",
              "  1.6332435607910156,\n",
              "  1.7170451879501343,\n",
              "  1.9688671827316284,\n",
              "  1.8943060636520386,\n",
              "  1.5848861932754517,\n",
              "  1.7214417457580566,\n",
              "  1.5200592279434204,\n",
              "  1.6122207641601562,\n",
              "  2.175236940383911,\n",
              "  2.618212938308716,\n",
              "  2.2736828327178955,\n",
              "  2.5873806476593018,\n",
              "  2.3957204818725586,\n",
              "  2.8102424144744873,\n",
              "  1.986362099647522,\n",
              "  2.145665407180786,\n",
              "  2.1622259616851807,\n",
              "  2.2901394367218018,\n",
              "  2.2875595092773438,\n",
              "  2.1519744396209717,\n",
              "  2.3571155071258545,\n",
              "  2.3236448764801025,\n",
              "  2.2854902744293213,\n",
              "  2.1986382007598877,\n",
              "  1.8369518518447876,\n",
              "  2.2955615520477295,\n",
              "  2.3536012172698975,\n",
              "  2.4955296516418457,\n",
              "  2.39047908782959,\n",
              "  2.0139052867889404,\n",
              "  1.747362732887268,\n",
              "  1.7487858533859253,\n",
              "  2.0625951290130615,\n",
              "  2.0085763931274414,\n",
              "  1.8524130582809448,\n",
              "  1.5482856035232544,\n",
              "  1.9230667352676392,\n",
              "  2.1444106101989746,\n",
              "  1.5149739980697632,\n",
              "  1.7435246706008911,\n",
              "  1.652854323387146,\n",
              "  2.180891752243042,\n",
              "  1.9780193567276,\n",
              "  2.0021069049835205,\n",
              "  1.5508979558944702,\n",
              "  1.5767074823379517,\n",
              "  1.7174838781356812,\n",
              "  1.5188103914260864,\n",
              "  1.3617576360702515,\n",
              "  1.5498313903808594,\n",
              "  1.7363542318344116,\n",
              "  1.967286229133606,\n",
              "  1.935787320137024,\n",
              "  1.7036420106887817,\n",
              "  1.8114744424819946,\n",
              "  1.842037558555603,\n",
              "  1.8581644296646118,\n",
              "  1.7019891738891602,\n",
              "  1.4201406240463257,\n",
              "  1.7938519716262817,\n",
              "  2.0782430171966553,\n",
              "  2.196321487426758,\n",
              "  1.7540563344955444,\n",
              "  1.5476161241531372,\n",
              "  1.4654101133346558,\n",
              "  1.3653045892715454,\n",
              "  1.532576560974121,\n",
              "  1.6716893911361694,\n",
              "  1.7405604124069214,\n",
              "  1.3485263586044312,\n",
              "  1.5976070165634155,\n",
              "  1.5419096946716309,\n",
              "  1.246409296989441,\n",
              "  1.0963135957717896,\n",
              "  1.4098873138427734,\n",
              "  1.6023117303848267,\n",
              "  1.7700971364974976,\n",
              "  2.299288511276245,\n",
              "  1.8712400197982788,\n",
              "  1.8056567907333374,\n",
              "  2.062560796737671,\n",
              "  1.7028261423110962,\n",
              "  1.8101749420166016,\n",
              "  1.8786729574203491,\n",
              "  2.197453022003174,\n",
              "  1.5905207395553589,\n",
              "  1.768236517906189,\n",
              "  2.1101632118225098,\n",
              "  1.6690362691879272,\n",
              "  1.7053236961364746,\n",
              "  1.371012806892395,\n",
              "  1.9324684143066406,\n",
              "  2.080636978149414,\n",
              "  1.6491554975509644,\n",
              "  1.3445883989334106,\n",
              "  1.9194377660751343,\n",
              "  1.9289374351501465,\n",
              "  1.7088640928268433,\n",
              "  2.2134170532226562,\n",
              "  1.9812192916870117,\n",
              "  1.8215951919555664,\n",
              "  1.6394354104995728,\n",
              "  1.7286587953567505,\n",
              "  1.726300835609436,\n",
              "  1.4200230836868286,\n",
              "  1.6056896448135376,\n",
              "  1.747005581855774,\n",
              "  1.9744309186935425,\n",
              "  2.120180368423462,\n",
              "  1.676985740661621,\n",
              "  1.8011242151260376,\n",
              "  1.8994618654251099,\n",
              "  1.418641448020935,\n",
              "  1.5036519765853882,\n",
              "  1.5854463577270508,\n",
              "  1.8696662187576294,\n",
              "  1.920331358909607,\n",
              "  1.828249454498291,\n",
              "  1.6768351793289185,\n",
              "  1.3778119087219238,\n",
              "  1.3123798370361328,\n",
              "  1.8371343612670898,\n",
              "  1.346221923828125,\n",
              "  1.9421204328536987,\n",
              "  2.1299641132354736,\n",
              "  2.4956321716308594,\n",
              "  1.9173407554626465,\n",
              "  1.5030040740966797,\n",
              "  1.6456714868545532,\n",
              "  2.0543062686920166,\n",
              "  1.676919937133789,\n",
              "  1.5115442276000977,\n",
              "  1.3077284097671509,\n",
              "  1.0597764253616333,\n",
              "  1.1475900411605835,\n",
              "  0.9391739368438721,\n",
              "  1.1649774312973022,\n",
              "  1.0691274404525757,\n",
              "  1.307608962059021,\n",
              "  1.4012447595596313,\n",
              "  1.5533195734024048,\n",
              "  1.4847480058670044,\n",
              "  1.3249106407165527,\n",
              "  1.6380542516708374,\n",
              "  1.7317942380905151,\n",
              "  1.9479738473892212,\n",
              "  1.9484535455703735,\n",
              "  1.7091264724731445,\n",
              "  1.5993963479995728,\n",
              "  1.956831932067871,\n",
              "  2.1056313514709473,\n",
              "  2.6050169467926025,\n",
              "  1.521498680114746,\n",
              "  1.6790657043457031,\n",
              "  1.5195757150650024,\n",
              "  1.3960660696029663,\n",
              "  1.2527132034301758,\n",
              "  1.3040753602981567,\n",
              "  1.8908764123916626,\n",
              "  1.5960068702697754,\n",
              "  1.3947707414627075,\n",
              "  1.6459026336669922,\n",
              "  1.2086929082870483,\n",
              "  1.3168151378631592,\n",
              "  1.4790488481521606,\n",
              "  1.7040966749191284,\n",
              "  1.6748875379562378,\n",
              "  1.859840989112854,\n",
              "  1.992813229560852,\n",
              "  2.164337635040283,\n",
              "  2.2556655406951904,\n",
              "  2.041844129562378,\n",
              "  2.0723628997802734,\n",
              "  2.8352928161621094,\n",
              "  2.2861359119415283,\n",
              "  2.6306545734405518,\n",
              "  2.157515525817871,\n",
              "  2.601158857345581,\n",
              "  2.1004157066345215,\n",
              "  2.1172597408294678,\n",
              "  1.7452205419540405,\n",
              "  1.8521194458007812,\n",
              "  1.8849257230758667,\n",
              "  2.2979557514190674,\n",
              "  1.9104560613632202,\n",
              "  2.281113624572754,\n",
              "  3.05790638923645,\n",
              "  2.4472944736480713,\n",
              "  2.94268536567688,\n",
              "  2.5380947589874268,\n",
              "  1.9851433038711548,\n",
              "  1.8376173973083496,\n",
              "  1.9406757354736328,\n",
              "  2.1667726039886475,\n",
              "  2.8781471252441406,\n",
              "  2.9617350101470947,\n",
              "  2.364828109741211,\n",
              "  3.309983015060425,\n",
              "  3.3551247119903564,\n",
              "  2.877697706222534,\n",
              "  2.271644353866577,\n",
              "  1.848591923713684,\n",
              "  2.0780045986175537,\n",
              "  2.473684072494507,\n",
              "  2.2983930110931396,\n",
              "  2.4089787006378174,\n",
              "  2.4265339374542236,\n",
              "  2.1892921924591064,\n",
              "  2.1256558895111084,\n",
              "  2.0339865684509277,\n",
              "  2.1383681297302246,\n",
              "  1.8309615850448608,\n",
              "  1.7524203062057495,\n",
              "  2.056414842605591,\n",
              "  2.159162759780884,\n",
              "  1.618265986442566,\n",
              "  1.8397397994995117,\n",
              "  1.7469720840454102,\n",
              "  1.6941828727722168,\n",
              "  1.898311734199524,\n",
              "  1.4475177526474,\n",
              "  1.5833988189697266,\n",
              "  1.738572597503662,\n",
              "  1.8121562004089355,\n",
              "  1.5299625396728516,\n",
              "  1.8520554304122925,\n",
              "  1.5766507387161255,\n",
              "  1.6897783279418945,\n",
              "  1.8971236944198608,\n",
              "  2.4126765727996826,\n",
              "  2.409327268600464,\n",
              "  2.5912773609161377,\n",
              "  2.4773595333099365,\n",
              "  2.372737169265747,\n",
              "  2.771888494491577,\n",
              "  2.1837494373321533,\n",
              "  2.6433541774749756,\n",
              "  1.9672657251358032,\n",
              "  2.60058856010437,\n",
              "  2.5063159465789795,\n",
              "  3.473139524459839,\n",
              "  3.749525308609009,\n",
              "  2.796968460083008,\n",
              "  2.769113540649414,\n",
              "  2.526542901992798,\n",
              "  2.5064914226531982,\n",
              "  2.2400500774383545,\n",
              "  2.5724434852600098,\n",
              "  2.3006484508514404,\n",
              "  2.0375421047210693,\n",
              "  2.043851137161255,\n",
              "  2.353644609451294,\n",
              "  2.085059881210327,\n",
              "  2.042799234390259,\n",
              "  2.4805095195770264,\n",
              "  2.26505708694458,\n",
              "  2.405900001525879,\n",
              "  1.8701263666152954,\n",
              "  1.852691650390625,\n",
              "  2.3204662799835205,\n",
              "  1.6602187156677246,\n",
              "  2.0106396675109863,\n",
              "  1.9573745727539062,\n",
              "  1.6880489587783813,\n",
              "  1.1646552085876465,\n",
              "  1.292274832725525,\n",
              "  1.259656548500061,\n",
              "  1.321083664894104,\n",
              "  2.1058905124664307,\n",
              "  2.060901403427124,\n",
              "  2.085080862045288,\n",
              "  2.2745978832244873,\n",
              "  2.2954397201538086,\n",
              "  2.357969284057617,\n",
              "  1.709206223487854,\n",
              "  1.5156044960021973,\n",
              "  2.269714832305908,\n",
              "  1.629948616027832,\n",
              "  2.5836329460144043,\n",
              "  2.4330837726593018,\n",
              "  1.8274065256118774,\n",
              "  1.9857233762741089,\n",
              "  2.0740301609039307,\n",
              "  2.2111237049102783,\n",
              "  2.346916675567627,\n",
              "  2.0761055946350098,\n",
              "  1.676896572113037,\n",
              "  1.7242873907089233,\n",
              "  2.4214632511138916,\n",
              "  2.2738587856292725,\n",
              "  2.2466611862182617,\n",
              "  2.0316855907440186,\n",
              "  1.8666938543319702,\n",
              "  2.207744836807251,\n",
              "  2.354707956314087,\n",
              "  2.2697718143463135,\n",
              "  2.123851776123047,\n",
              "  1.8127756118774414,\n",
              "  1.8624001741409302,\n",
              "  1.7832754850387573,\n",
              "  1.6284900903701782,\n",
              "  1.7822569608688354,\n",
              "  1.7201980352401733,\n",
              "  2.9539196491241455,\n",
              "  2.7209908962249756,\n",
              "  2.3575551509857178,\n",
              "  2.4307215213775635,\n",
              "  2.035919427871704,\n",
              "  2.1940906047821045,\n",
              "  3.4692890644073486,\n",
              "  2.5788111686706543,\n",
              "  2.849714517593384,\n",
              "  2.3658504486083984,\n",
              "  2.017097234725952,\n",
              "  2.875598192214966,\n",
              "  2.312299966812134,\n",
              "  2.4915971755981445,\n",
              "  2.0740644931793213,\n",
              "  1.8358350992202759,\n",
              "  2.0417323112487793,\n",
              "  2.3445966243743896,\n",
              "  3.5079433917999268,\n",
              "  1.9625073671340942,\n",
              "  1.7990089654922485,\n",
              "  2.4220259189605713,\n",
              "  1.7280564308166504,\n",
              "  1.6663289070129395,\n",
              "  1.6128249168395996,\n",
              "  1.7041102647781372,\n",
              "  2.0515353679656982,\n",
              "  2.256969690322876,\n",
              "  1.7795038223266602,\n",
              "  2.7811739444732666,\n",
              "  3.196312189102173,\n",
              "  2.045027494430542,\n",
              "  2.5383212566375732,\n",
              "  3.002408742904663,\n",
              "  2.0532772541046143,\n",
              "  1.4538898468017578,\n",
              "  2.5261971950531006,\n",
              "  2.1725080013275146,\n",
              "  2.497352361679077,\n",
              "  1.8800939321517944,\n",
              "  2.239187002182007,\n",
              "  1.829837679862976,\n",
              "  2.0232532024383545,\n",
              "  1.4617811441421509,\n",
              "  1.574226975440979,\n",
              "  1.312080979347229,\n",
              "  1.6608117818832397,\n",
              "  2.185666561126709,\n",
              "  2.502599000930786,\n",
              "  2.405108690261841,\n",
              "  2.326105833053589,\n",
              "  2.5402543544769287,\n",
              "  2.399040460586548,\n",
              "  2.4929959774017334,\n",
              "  2.4795713424682617,\n",
              "  2.632761001586914,\n",
              "  2.0471115112304688,\n",
              "  1.9276682138442993,\n",
              "  1.8533307313919067,\n",
              "  1.849471092224121,\n",
              "  1.726341724395752,\n",
              "  2.268934965133667,\n",
              "  2.1625587940216064,\n",
              "  2.4367542266845703,\n",
              "  2.072888135910034,\n",
              "  1.9837207794189453,\n",
              "  1.726558804512024,\n",
              "  1.6277555227279663,\n",
              "  2.0069425106048584,\n",
              "  1.9475234746932983,\n",
              "  1.695181965827942,\n",
              "  1.731439232826233,\n",
              "  1.4575201272964478,\n",
              "  1.4843307733535767,\n",
              "  1.6892329454421997,\n",
              "  2.3515329360961914,\n",
              "  1.9526066780090332,\n",
              "  2.311356544494629,\n",
              "  2.0291545391082764,\n",
              "  2.8093502521514893,\n",
              "  2.566681385040283,\n",
              "  2.3736531734466553,\n",
              "  2.59960675239563,\n",
              "  2.7376034259796143,\n",
              "  2.9397895336151123,\n",
              "  2.5465633869171143,\n",
              "  2.0083019733428955,\n",
              "  1.9803968667984009,\n",
              "  1.8528337478637695,\n",
              "  1.5246442556381226,\n",
              "  1.7781215906143188,\n",
              "  1.6868109703063965,\n",
              "  1.7305034399032593,\n",
              "  1.769333004951477,\n",
              "  1.5025763511657715,\n",
              "  1.4606947898864746,\n",
              "  1.2656910419464111,\n",
              "  1.9484739303588867,\n",
              "  1.5529241561889648,\n",
              "  2.1353325843811035,\n",
              "  1.4155019521713257,\n",
              "  1.1592079401016235,\n",
              "  1.6536411046981812,\n",
              "  1.569764494895935,\n",
              "  1.4864641427993774,\n",
              "  1.6646746397018433,\n",
              "  1.8332252502441406,\n",
              "  1.5472897291183472,\n",
              "  1.8927403688430786,\n",
              "  2.5975120067596436,\n",
              "  1.8182083368301392,\n",
              "  1.9159101247787476,\n",
              "  1.5664496421813965,\n",
              "  2.590160846710205,\n",
              "  2.1559438705444336,\n",
              "  2.373955488204956,\n",
              "  2.539746046066284,\n",
              "  2.366387128829956,\n",
              "  2.17311692237854,\n",
              "  2.319788694381714,\n",
              "  1.8993419408798218,\n",
              "  2.1298038959503174,\n",
              "  1.978244423866272,\n",
              "  2.0454139709472656,\n",
              "  1.8139948844909668,\n",
              "  2.144587755203247,\n",
              "  2.057710886001587,\n",
              "  2.1573047637939453,\n",
              "  2.1828062534332275,\n",
              "  2.1341984272003174,\n",
              "  1.625937819480896,\n",
              "  2.2323174476623535,\n",
              "  1.9660733938217163,\n",
              "  2.0904147624969482,\n",
              "  2.100497245788574,\n",
              "  1.7752128839492798,\n",
              "  2.047130584716797,\n",
              "  1.4401935338974,\n",
              "  1.7553480863571167,\n",
              "  1.3926674127578735,\n",
              "  1.5533355474472046,\n",
              "  1.3615740537643433,\n",
              "  2.035569190979004,\n",
              "  1.8331689834594727,\n",
              "  1.6467937231063843,\n",
              "  1.3401414155960083,\n",
              "  1.8374522924423218,\n",
              "  2.043328046798706,\n",
              "  1.9835004806518555,\n",
              "  2.087895393371582,\n",
              "  2.087313413619995,\n",
              "  2.277392625808716,\n",
              "  2.4346249103546143,\n",
              "  2.150644302368164,\n",
              "  2.2745773792266846,\n",
              "  2.6219820976257324,\n",
              "  2.2068464756011963,\n",
              "  2.716050863265991,\n",
              "  2.5577404499053955,\n",
              "  2.077256679534912,\n",
              "  1.8259695768356323,\n",
              "  2.206927537918091,\n",
              "  1.919660210609436,\n",
              "  1.9419450759887695,\n",
              "  2.111724615097046,\n",
              "  2.779088258743286,\n",
              "  2.6871469020843506,\n",
              "  2.6989974975585938,\n",
              "  2.5923385620117188,\n",
              "  2.0817224979400635,\n",
              "  1.6645742654800415,\n",
              "  1.534296989440918,\n",
              "  1.8536921739578247,\n",
              "  1.4469313621520996,\n",
              "  2.126126527786255,\n",
              "  2.0227479934692383,\n",
              "  2.2966556549072266,\n",
              "  2.3653550148010254,\n",
              "  2.0148143768310547,\n",
              "  1.5838136672973633,\n",
              "  1.8733100891113281,\n",
              "  2.345993995666504,\n",
              "  2.6995437145233154,\n",
              "  2.496713876724243,\n",
              "  1.9635733366012573,\n",
              "  1.9980082511901855,\n",
              "  1.8304332494735718,\n",
              "  2.8711330890655518,\n",
              "  2.2357776165008545,\n",
              "  1.7664045095443726,\n",
              "  1.774633526802063,\n",
              "  1.8467553853988647,\n",
              "  2.9119770526885986,\n",
              "  1.8117564916610718,\n",
              "  2.0875141620635986,\n",
              "  2.189307451248169,\n",
              "  2.1484553813934326,\n",
              "  2.213414430618286,\n",
              "  1.5520988702774048,\n",
              "  2.5709686279296875,\n",
              "  2.2172372341156006,\n",
              "  2.2691290378570557,\n",
              "  1.856225609779358,\n",
              "  2.1006052494049072,\n",
              "  2.2607991695404053,\n",
              "  2.3035309314727783,\n",
              "  2.2309203147888184,\n",
              "  1.931253433227539,\n",
              "  2.0349810123443604,\n",
              "  2.1422178745269775,\n",
              "  2.431596279144287,\n",
              "  2.20833420753479,\n",
              "  1.8902662992477417,\n",
              "  2.578253984451294,\n",
              "  1.887753963470459,\n",
              "  2.0698235034942627,\n",
              "  2.186105966567993,\n",
              "  1.981341004371643,\n",
              "  1.9749354124069214,\n",
              "  2.952723741531372,\n",
              "  2.750767469406128,\n",
              "  2.468752145767212,\n",
              "  2.078925848007202,\n",
              "  2.662113904953003,\n",
              "  2.056699514389038,\n",
              "  1.7472009658813477,\n",
              "  2.1492297649383545,\n",
              "  2.225461006164551,\n",
              "  2.215855360031128,\n",
              "  2.206204414367676,\n",
              "  2.2239933013916016,\n",
              "  2.278111219406128,\n",
              "  2.2190186977386475,\n",
              "  1.8128212690353394,\n",
              "  1.9510059356689453,\n",
              "  2.072046995162964,\n",
              "  2.2282021045684814,\n",
              "  1.7457098960876465,\n",
              "  1.6034793853759766,\n",
              "  1.8192001581192017,\n",
              "  2.9898414611816406,\n",
              "  2.52636456489563,\n",
              "  3.343083381652832,\n",
              "  2.8544504642486572,\n",
              "  2.226633071899414,\n",
              "  2.152172803878784,\n",
              "  2.239682197570801,\n",
              "  2.1921896934509277,\n",
              "  1.8362630605697632,\n",
              "  1.9582852125167847,\n",
              "  1.9427889585494995,\n",
              "  2.0180203914642334,\n",
              "  1.9549403190612793,\n",
              "  2.0128285884857178,\n",
              "  1.9084291458129883,\n",
              "  1.6331382989883423,\n",
              "  1.9458484649658203,\n",
              "  1.9578360319137573,\n",
              "  1.791157841682434,\n",
              "  2.1249349117279053,\n",
              "  1.911511778831482,\n",
              "  2.2999155521392822,\n",
              "  2.040534257888794,\n",
              "  1.871724009513855,\n",
              "  1.8864898681640625,\n",
              "  2.109557867050171,\n",
              "  1.9473756551742554,\n",
              "  1.941075325012207,\n",
              "  1.8552583456039429,\n",
              "  1.838694453239441,\n",
              "  1.9650219678878784,\n",
              "  2.3097946643829346,\n",
              "  2.1220219135284424,\n",
              "  2.4810917377471924,\n",
              "  2.3875112533569336,\n",
              "  1.7094663381576538,\n",
              "  1.519569754600525,\n",
              "  1.8627079725265503,\n",
              "  1.9376450777053833,\n",
              "  1.984686017036438,\n",
              "  2.6935501098632812,\n",
              "  2.177358865737915,\n",
              "  2.388239622116089,\n",
              "  2.6477644443511963,\n",
              "  1.9762815237045288,\n",
              "  2.2737514972686768,\n",
              "  2.1637303829193115,\n",
              "  2.1046512126922607,\n",
              "  2.2427730560302734,\n",
              "  2.3151631355285645,\n",
              "  1.9057984352111816,\n",
              "  1.7142425775527954,\n",
              "  1.9449869394302368,\n",
              "  1.6610898971557617,\n",
              "  1.8644548654556274,\n",
              "  2.0593631267547607,\n",
              "  2.319322347640991,\n",
              "  1.8874143362045288,\n",
              "  1.8156909942626953,\n",
              "  2.498634099960327,\n",
              "  2.5048916339874268,\n",
              "  2.1259262561798096,\n",
              "  1.937381625175476,\n",
              "  2.0159912109375,\n",
              "  2.013477087020874,\n",
              "  2.035346031188965,\n",
              "  1.9426392316818237,\n",
              "  2.0982117652893066,\n",
              "  2.345853328704834,\n",
              "  2.377488851547241,\n",
              "  2.99017071723938,\n",
              "  2.302522659301758,\n",
              "  3.5830061435699463,\n",
              "  3.431507110595703,\n",
              "  3.121885061264038,\n",
              "  2.1768569946289062,\n",
              "  2.695887327194214,\n",
              "  2.8226044178009033,\n",
              "  2.1096458435058594,\n",
              "  2.738288164138794,\n",
              "  2.826434373855591,\n",
              "  2.321364641189575,\n",
              "  2.4991164207458496,\n",
              "  2.4926815032958984,\n",
              "  2.0279152393341064,\n",
              "  1.726274847984314,\n",
              "  2.2687108516693115,\n",
              "  1.9238430261611938,\n",
              "  1.8554353713989258,\n",
              "  1.8840047121047974,\n",
              "  2.026209592819214,\n",
              "  2.097640037536621,\n",
              "  1.909793496131897,\n",
              "  2.029416799545288,\n",
              "  2.253136396408081,\n",
              "  2.23934268951416,\n",
              "  2.398973226547241,\n",
              "  2.06209397315979,\n",
              "  1.8932852745056152,\n",
              "  2.067558765411377,\n",
              "  1.88827645778656,\n",
              "  1.631770133972168,\n",
              "  2.1068344116210938,\n",
              "  2.5966567993164062,\n",
              "  2.3451921939849854,\n",
              "  1.982369065284729,\n",
              "  2.6141788959503174,\n",
              "  2.1979587078094482,\n",
              "  2.2147376537323,\n",
              "  2.593864679336548,\n",
              "  2.153201103210449,\n",
              "  2.4425768852233887,\n",
              "  2.2028563022613525,\n",
              "  2.1504786014556885,\n",
              "  2.3890163898468018,\n",
              "  2.389472723007202,\n",
              "  2.2347910404205322,\n",
              "  2.258683204650879,\n",
              "  2.6651549339294434,\n",
              "  1.7495580911636353,\n",
              "  2.7625770568847656,\n",
              "  2.0600175857543945,\n",
              "  1.8932896852493286,\n",
              "  3.2802371978759766,\n",
              "  1.9826730489730835,\n",
              "  2.228296995162964,\n",
              "  2.3823485374450684,\n",
              "  2.354278564453125,\n",
              "  2.493852376937866,\n",
              "  2.8762969970703125,\n",
              "  2.1932742595672607,\n",
              "  2.338693380355835,\n",
              "  1.8445183038711548,\n",
              "  2.4535365104675293,\n",
              "  2.1794626712799072,\n",
              "  2.5032472610473633,\n",
              "  2.9186973571777344,\n",
              "  2.4550135135650635,\n",
              "  2.243631601333618,\n",
              "  2.0835418701171875,\n",
              "  2.1404497623443604,\n",
              "  2.306373357772827,\n",
              "  2.129371404647827,\n",
              "  2.0368058681488037,\n",
              "  2.2627944946289062,\n",
              "  2.3988149166107178,\n",
              "  2.4691243171691895,\n",
              "  2.551830291748047,\n",
              "  2.7042665481567383,\n",
              "  2.5892136096954346,\n",
              "  2.264042854309082,\n",
              "  2.2636444568634033,\n",
              "  2.3301172256469727,\n",
              "  2.5893938541412354,\n",
              "  2.3515446186065674,\n",
              "  2.7905521392822266,\n",
              "  3.3446903228759766,\n",
              "  3.7451016902923584,\n",
              "  3.050586700439453,\n",
              "  2.6980905532836914,\n",
              "  2.619983673095703,\n",
              "  2.279757261276245,\n",
              "  2.696924924850464,\n",
              "  2.706786870956421,\n",
              "  2.3094937801361084,\n",
              "  2.3699371814727783,\n",
              "  1.9782148599624634,\n",
              "  2.1663191318511963,\n",
              "  2.2699828147888184,\n",
              "  2.2985332012176514,\n",
              "  1.9480684995651245,\n",
              "  2.221329689025879,\n",
              "  2.3812901973724365,\n",
              "  2.2504305839538574,\n",
              "  2.247606039047241,\n",
              "  2.281351089477539,\n",
              "  2.115177869796753,\n",
              "  1.7964887619018555,\n",
              "  2.110793352127075,\n",
              "  2.3606927394866943,\n",
              "  2.278032064437866,\n",
              "  2.0907623767852783,\n",
              "  2.2289960384368896,\n",
              "  2.2947089672088623,\n",
              "  2.1189639568328857,\n",
              "  2.368060827255249,\n",
              "  2.2544045448303223,\n",
              "  1.9425052404403687,\n",
              "  2.500096559524536,\n",
              "  2.191195249557495,\n",
              "  2.1069157123565674,\n",
              "  1.9626564979553223,\n",
              "  2.1961004734039307],\n",
              " 'val_acc': [0.625,\n",
              "  0.5,\n",
              "  0.4166666567325592,\n",
              "  0.7291666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.5625,\n",
              "  0.6145833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.59375,\n",
              "  0.5729166865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6875,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.5833333134651184,\n",
              "  0.7395833134651184,\n",
              "  0.7395833134651184,\n",
              "  0.7708333134651184,\n",
              "  0.5416666865348816,\n",
              "  0.625,\n",
              "  0.7291666865348816,\n",
              "  0.5,\n",
              "  0.71875,\n",
              "  0.6979166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.71875,\n",
              "  0.6770833134651184,\n",
              "  0.6145833134651184,\n",
              "  0.71875,\n",
              "  0.65625,\n",
              "  0.625,\n",
              "  0.7604166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.7604166865348816,\n",
              "  0.5729166865348816,\n",
              "  0.7083333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.7291666865348816,\n",
              "  0.6875,\n",
              "  0.6458333134651184,\n",
              "  0.6875,\n",
              "  0.6770833134651184,\n",
              "  0.6875,\n",
              "  0.6458333134651184,\n",
              "  0.71875,\n",
              "  0.65625,\n",
              "  0.7083333134651184,\n",
              "  0.65625,\n",
              "  0.7604166865348816,\n",
              "  0.7395833134651184,\n",
              "  0.6979166865348816,\n",
              "  0.7708333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6979166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6979166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6979166865348816,\n",
              "  0.65625,\n",
              "  0.7291666865348816,\n",
              "  0.6875,\n",
              "  0.6458333134651184,\n",
              "  0.71875,\n",
              "  0.6666666865348816,\n",
              "  0.6979166865348816,\n",
              "  0.71875,\n",
              "  0.65625,\n",
              "  0.7708333134651184,\n",
              "  0.6875,\n",
              "  0.6041666865348816,\n",
              "  0.6875,\n",
              "  0.6458333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.6979166865348816,\n",
              "  0.7291666865348816,\n",
              "  0.7083333134651184,\n",
              "  0.75,\n",
              "  0.6770833134651184,\n",
              "  0.75,\n",
              "  0.7083333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6979166865348816,\n",
              "  0.6770833134651184,\n",
              "  0.625,\n",
              "  0.6770833134651184,\n",
              "  0.6979166865348816,\n",
              "  0.6979166865348816,\n",
              "  0.6979166865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6354166865348816,\n",
              "  0.7083333134651184,\n",
              "  0.75,\n",
              "  0.5416666865348816,\n",
              "  0.6145833134651184,\n",
              "  0.5833333134651184,\n",
              "  0.6145833134651184,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.6770833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.5625,\n",
              "  0.5729166865348816,\n",
              "  0.5729166865348816,\n",
              "  0.4895833432674408,\n",
              "  0.6354166865348816,\n",
              "  0.5729166865348816,\n",
              "  0.59375,\n",
              "  0.625,\n",
              "  0.6354166865348816,\n",
              "  0.6145833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.625,\n",
              "  0.6770833134651184,\n",
              "  0.6145833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6979166865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6979166865348816,\n",
              "  0.6770833134651184,\n",
              "  0.59375,\n",
              "  0.6354166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.5416666865348816,\n",
              "  0.6041666865348816,\n",
              "  0.6875,\n",
              "  0.65625,\n",
              "  0.59375,\n",
              "  0.5104166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6875,\n",
              "  0.6979166865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6145833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.6770833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.625,\n",
              "  0.6458333134651184,\n",
              "  0.6979166865348816,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.65625,\n",
              "  0.7083333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.7291666865348816,\n",
              "  0.7083333134651184,\n",
              "  0.6875,\n",
              "  0.6770833134651184,\n",
              "  0.6979166865348816,\n",
              "  0.7083333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.6666666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.71875,\n",
              "  0.7083333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6979166865348816,\n",
              "  0.71875,\n",
              "  0.6770833134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6041666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.71875,\n",
              "  0.6666666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.7291666865348816,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6979166865348816,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.71875,\n",
              "  0.71875,\n",
              "  0.71875,\n",
              "  0.7083333134651184,\n",
              "  0.6979166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6979166865348816,\n",
              "  0.7708333134651184,\n",
              "  0.71875,\n",
              "  0.7083333134651184,\n",
              "  0.6875,\n",
              "  0.65625,\n",
              "  0.6770833134651184,\n",
              "  0.6875,\n",
              "  0.6145833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.59375,\n",
              "  0.6770833134651184,\n",
              "  0.6041666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6875,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.7395833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.5208333134651184,\n",
              "  0.6145833134651184,\n",
              "  0.59375,\n",
              "  0.59375,\n",
              "  0.625,\n",
              "  0.65625,\n",
              "  0.625,\n",
              "  0.65625,\n",
              "  0.625,\n",
              "  0.65625,\n",
              "  0.5729166865348816,\n",
              "  0.7083333134651184,\n",
              "  0.65625,\n",
              "  0.6979166865348816,\n",
              "  0.65625,\n",
              "  0.625,\n",
              "  0.5729166865348816,\n",
              "  0.5625,\n",
              "  0.5833333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.6770833134651184,\n",
              "  0.65625,\n",
              "  0.7083333134651184,\n",
              "  0.71875,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.625,\n",
              "  0.6979166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6979166865348816,\n",
              "  0.7083333134651184,\n",
              "  0.71875,\n",
              "  0.6354166865348816,\n",
              "  0.6145833134651184,\n",
              "  0.5416666865348816,\n",
              "  0.6145833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.6979166865348816,\n",
              "  0.6354166865348816,\n",
              "  0.7083333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.7083333134651184,\n",
              "  0.6979166865348816,\n",
              "  0.625,\n",
              "  0.6458333134651184,\n",
              "  0.6875,\n",
              "  0.6458333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.5833333134651184,\n",
              "  0.625,\n",
              "  0.6354166865348816,\n",
              "  0.6770833134651184,\n",
              "  0.625,\n",
              "  0.6145833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6875,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.6770833134651184,\n",
              "  0.6770833134651184,\n",
              "  0.5625,\n",
              "  0.6354166865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.5625,\n",
              "  0.6875,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.625,\n",
              "  0.59375,\n",
              "  0.6458333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6041666865348816,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6354166865348816,\n",
              "  0.5833333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6354166865348816,\n",
              "  0.625,\n",
              "  0.6354166865348816,\n",
              "  0.6979166865348816,\n",
              "  0.6770833134651184,\n",
              "  0.7083333134651184,\n",
              "  0.6979166865348816,\n",
              "  0.71875,\n",
              "  0.6770833134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.625,\n",
              "  0.6041666865348816,\n",
              "  0.6041666865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6875,\n",
              "  0.59375,\n",
              "  0.59375,\n",
              "  0.5520833134651184,\n",
              "  0.6041666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6145833134651184,\n",
              "  0.625,\n",
              "  0.6666666865348816,\n",
              "  0.6145833134651184,\n",
              "  0.7083333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.7083333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.65625,\n",
              "  0.6770833134651184,\n",
              "  0.6979166865348816,\n",
              "  0.71875,\n",
              "  0.625,\n",
              "  0.6354166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.625,\n",
              "  0.6145833134651184,\n",
              "  0.5833333134651184,\n",
              "  0.625,\n",
              "  0.5,\n",
              "  0.5416666865348816,\n",
              "  0.5833333134651184,\n",
              "  0.6145833134651184,\n",
              "  0.6145833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6979166865348816,\n",
              "  0.6145833134651184,\n",
              "  0.6145833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.5729166865348816,\n",
              "  0.6041666865348816,\n",
              "  0.6145833134651184,\n",
              "  0.6875,\n",
              "  0.65625,\n",
              "  0.59375,\n",
              "  0.5625,\n",
              "  0.6145833134651184,\n",
              "  0.6145833134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6770833134651184,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.5625,\n",
              "  0.6458333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6041666865348816,\n",
              "  0.6041666865348816,\n",
              "  0.7083333134651184,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.7083333134651184,\n",
              "  0.6875,\n",
              "  0.7291666865348816,\n",
              "  0.6875,\n",
              "  0.7083333134651184,\n",
              "  0.6979166865348816,\n",
              "  0.7083333134651184,\n",
              "  0.6875,\n",
              "  0.7395833134651184,\n",
              "  0.6979166865348816,\n",
              "  0.6979166865348816,\n",
              "  0.7604166865348816,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.625,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.625,\n",
              "  0.6041666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.59375,\n",
              "  0.6145833134651184,\n",
              "  0.6979166865348816,\n",
              "  0.7083333134651184,\n",
              "  0.6979166865348816,\n",
              "  0.6145833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6979166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.625,\n",
              "  0.5833333134651184,\n",
              "  0.625,\n",
              "  0.5625,\n",
              "  0.6770833134651184,\n",
              "  0.6770833134651184,\n",
              "  0.625,\n",
              "  0.5729166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.625,\n",
              "  0.6666666865348816,\n",
              "  0.6041666865348816,\n",
              "  0.6875,\n",
              "  0.6666666865348816,\n",
              "  0.59375,\n",
              "  0.625,\n",
              "  0.7083333134651184,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.7083333134651184,\n",
              "  0.6875,\n",
              "  0.6666666865348816,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.6875,\n",
              "  0.65625,\n",
              "  0.6770833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6458333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6145833134651184,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.71875,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.7083333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.7395833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.625,\n",
              "  0.71875,\n",
              "  0.6770833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.625,\n",
              "  0.6458333134651184,\n",
              "  0.6979166865348816,\n",
              "  0.6354166865348816,\n",
              "  0.5833333134651184,\n",
              "  0.6041666865348816,\n",
              "  0.625,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.5104166865348816,\n",
              "  0.59375,\n",
              "  0.59375,\n",
              "  0.65625,\n",
              "  0.59375,\n",
              "  0.625,\n",
              "  0.6145833134651184,\n",
              "  0.6041666865348816,\n",
              "  0.625,\n",
              "  0.6041666865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6145833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6979166865348816,\n",
              "  0.6875,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6666666865348816,\n",
              "  0.625,\n",
              "  0.6979166865348816,\n",
              "  0.7083333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.7291666865348816,\n",
              "  0.7083333134651184,\n",
              "  0.7083333134651184,\n",
              "  0.6875,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.59375,\n",
              "  0.6354166865348816,\n",
              "  0.6875,\n",
              "  0.71875,\n",
              "  0.625,\n",
              "  0.6770833134651184,\n",
              "  0.625,\n",
              "  0.6145833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6145833134651184,\n",
              "  0.59375,\n",
              "  0.6458333134651184,\n",
              "  0.6875,\n",
              "  0.6666666865348816,\n",
              "  0.6875,\n",
              "  0.65625,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6875,\n",
              "  0.71875,\n",
              "  0.6979166865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6979166865348816,\n",
              "  0.6770833134651184,\n",
              "  0.7291666865348816,\n",
              "  0.7395833134651184,\n",
              "  0.6979166865348816,\n",
              "  0.71875,\n",
              "  0.6666666865348816,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.6666666865348816,\n",
              "  0.71875,\n",
              "  0.625,\n",
              "  0.6458333134651184,\n",
              "  0.6979166865348816,\n",
              "  0.65625,\n",
              "  0.71875,\n",
              "  0.6979166865348816,\n",
              "  0.65625,\n",
              "  0.6770833134651184,\n",
              "  0.6875,\n",
              "  0.65625,\n",
              "  0.6875,\n",
              "  0.6770833134651184,\n",
              "  0.7083333134651184,\n",
              "  0.6041666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.7083333134651184,\n",
              "  0.6875,\n",
              "  0.71875,\n",
              "  0.7395833134651184,\n",
              "  0.7291666865348816,\n",
              "  0.6979166865348816,\n",
              "  0.6770833134651184,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.625,\n",
              "  0.6145833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6875,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6041666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6145833134651184,\n",
              "  0.6875,\n",
              "  0.6354166865348816,\n",
              "  0.6875,\n",
              "  0.65625,\n",
              "  0.625,\n",
              "  0.6458333134651184,\n",
              "  0.5520833134651184,\n",
              "  0.625,\n",
              "  0.5625,\n",
              "  0.5729166865348816,\n",
              "  0.6145833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.6875,\n",
              "  0.6041666865348816,\n",
              "  0.6145833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6875,\n",
              "  0.6354166865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.625,\n",
              "  0.7083333134651184,\n",
              "  0.6875,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.6979166865348816,\n",
              "  0.6979166865348816,\n",
              "  0.6979166865348816,\n",
              "  0.75,\n",
              "  0.71875,\n",
              "  0.6458333134651184,\n",
              "  0.6875,\n",
              "  0.6875,\n",
              "  0.7604166865348816,\n",
              "  0.6875,\n",
              "  0.6979166865348816,\n",
              "  0.71875,\n",
              "  0.6666666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6875,\n",
              "  0.6770833134651184,\n",
              "  0.6770833134651184,\n",
              "  0.65625,\n",
              "  0.6770833134651184,\n",
              "  0.7083333134651184,\n",
              "  0.71875,\n",
              "  0.71875,\n",
              "  0.6770833134651184,\n",
              "  0.6979166865348816,\n",
              "  0.7083333134651184,\n",
              "  0.6875,\n",
              "  0.7083333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6979166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6979166865348816,\n",
              "  0.7291666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6875,\n",
              "  0.6979166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6145833134651184,\n",
              "  0.7083333134651184,\n",
              "  0.71875,\n",
              "  0.6875,\n",
              "  0.6666666865348816,\n",
              "  0.625,\n",
              "  0.625,\n",
              "  0.4895833432674408,\n",
              "  0.6041666865348816,\n",
              "  0.6145833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6875,\n",
              "  0.6770833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.6875,\n",
              "  0.7083333134651184,\n",
              "  0.625,\n",
              "  0.6770833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.6145833134651184,\n",
              "  0.7083333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6875,\n",
              "  0.7083333134651184,\n",
              "  0.7291666865348816,\n",
              "  0.6979166865348816,\n",
              "  0.65625,\n",
              "  0.6145833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.7395833134651184,\n",
              "  0.7083333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.625,\n",
              "  0.59375,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.5833333134651184,\n",
              "  0.65625,\n",
              "  0.5729166865348816,\n",
              "  0.5833333134651184,\n",
              "  0.5208333134651184,\n",
              "  0.5729166865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.6145833134651184,\n",
              "  0.59375,\n",
              "  0.6354166865348816,\n",
              "  0.6145833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.59375,\n",
              "  0.6666666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.65625,\n",
              "  0.7291666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6979166865348816,\n",
              "  0.65625,\n",
              "  0.5833333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6145833134651184,\n",
              "  0.625,\n",
              "  0.6875,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.6145833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.625,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.53125,\n",
              "  0.7083333134651184,\n",
              "  0.6875,\n",
              "  0.7083333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6875,\n",
              "  0.71875,\n",
              "  0.6979166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.5,\n",
              "  0.65625,\n",
              "  0.6770833134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6979166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6979166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.5833333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6145833134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6770833134651184,\n",
              "  0.53125,\n",
              "  0.59375,\n",
              "  0.625,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.625,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.6979166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.6666666865348816,\n",
              "  0.6875,\n",
              "  0.6354166865348816,\n",
              "  0.5833333134651184,\n",
              "  0.59375,\n",
              "  0.6145833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6145833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.5833333134651184,\n",
              "  0.5625,\n",
              "  0.65625,\n",
              "  0.5625,\n",
              "  0.625,\n",
              "  0.59375,\n",
              "  0.5833333134651184,\n",
              "  0.65625,\n",
              "  0.6770833134651184,\n",
              "  0.59375,\n",
              "  0.5625,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.625,\n",
              "  0.5416666865348816,\n",
              "  0.65625,\n",
              "  0.59375,\n",
              "  0.6354166865348816,\n",
              "  0.6770833134651184,\n",
              "  0.625,\n",
              "  0.5833333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6145833134651184,\n",
              "  0.5208333134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6145833134651184,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.6770833134651184,\n",
              "  0.6770833134651184,\n",
              "  0.5416666865348816,\n",
              "  0.65625,\n",
              "  0.6458333134651184,\n",
              "  0.5833333134651184,\n",
              "  0.6041666865348816,\n",
              "  0.6145833134651184,\n",
              "  0.625,\n",
              "  0.5520833134651184,\n",
              "  0.6145833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.6041666865348816,\n",
              "  0.625,\n",
              "  0.6666666865348816,\n",
              "  0.5625,\n",
              "  0.625,\n",
              "  0.6354166865348816,\n",
              "  0.5833333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6979166865348816,\n",
              "  0.625,\n",
              "  0.6041666865348816,\n",
              "  0.65625,\n",
              "  0.6979166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.65625,\n",
              "  0.6041666865348816,\n",
              "  0.625,\n",
              "  0.59375,\n",
              "  0.6041666865348816,\n",
              "  0.6041666865348816,\n",
              "  0.6145833134651184,\n",
              "  0.6041666865348816,\n",
              "  0.5104166865348816,\n",
              "  0.53125,\n",
              "  0.5,\n",
              "  0.6354166865348816,\n",
              "  0.625,\n",
              "  0.6354166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.6041666865348816,\n",
              "  0.5104166865348816,\n",
              "  0.6145833134651184,\n",
              "  0.6041666865348816,\n",
              "  0.59375,\n",
              "  0.65625,\n",
              "  0.7604166865348816,\n",
              "  0.6979166865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6979166865348816,\n",
              "  0.625,\n",
              "  0.65625,\n",
              "  0.65625,\n",
              "  0.6979166865348816,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.65625,\n",
              "  0.5833333134651184,\n",
              "  0.6145833134651184,\n",
              "  0.6875,\n",
              "  0.6354166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6979166865348816,\n",
              "  0.6458333134651184,\n",
              "  0.5625,\n",
              "  0.6041666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.5729166865348816,\n",
              "  0.5833333134651184,\n",
              "  0.6041666865348816,\n",
              "  0.5833333134651184,\n",
              "  0.59375,\n",
              "  0.59375,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6979166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6041666865348816,\n",
              "  0.5416666865348816,\n",
              "  0.65625,\n",
              "  0.5729166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6458333134651184,\n",
              "  0.5104166865348816,\n",
              "  0.71875,\n",
              "  0.7291666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6979166865348816,\n",
              "  0.6041666865348816,\n",
              "  0.625,\n",
              "  0.6770833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.65625,\n",
              "  0.6458333134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6354166865348816,\n",
              "  0.59375,\n",
              "  0.6354166865348816,\n",
              "  0.625,\n",
              "  0.6145833134651184,\n",
              "  0.6145833134651184,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.625,\n",
              "  0.59375,\n",
              "  0.59375,\n",
              "  0.5729166865348816,\n",
              "  0.5520833134651184,\n",
              "  0.6145833134651184,\n",
              "  0.6354166865348816,\n",
              "  0.6354166865348816,\n",
              "  0.59375,\n",
              "  0.6666666865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6041666865348816,\n",
              "  0.625,\n",
              "  0.65625,\n",
              "  0.6354166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6875,\n",
              "  0.6666666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6770833134651184,\n",
              "  0.6770833134651184,\n",
              "  0.5625,\n",
              "  0.59375,\n",
              "  0.6979166865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6979166865348816,\n",
              "  0.6354166865348816,\n",
              "  0.625,\n",
              "  0.6666666865348816,\n",
              "  0.6770833134651184,\n",
              "  0.65625,\n",
              "  0.6770833134651184,\n",
              "  0.6458333134651184,\n",
              "  0.6145833134651184,\n",
              "  0.6041666865348816,\n",
              "  0.5625,\n",
              "  0.59375,\n",
              "  0.6770833134651184,\n",
              "  0.65625,\n",
              "  0.6979166865348816,\n",
              "  0.65625,\n",
              "  0.6458333134651184,\n",
              "  0.6875,\n",
              "  0.6458333134651184,\n",
              "  0.6666666865348816,\n",
              "  0.6354166865348816,\n",
              "  0.6770833134651184,\n",
              "  0.6458333134651184]}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot training performance"
      ],
      "metadata": {
        "id": "54D1y5fZcRUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_x = range(len(acc))\n",
        "\n",
        "plt.plot(epochs_x, acc, 'co', label='Training acc')\n",
        "plt.plot(epochs_x, val_acc, 'k', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'co', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'k', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aig-yHOBcNrP",
        "outputId": "ec257dce-6d29-4a66-e9a3-24b4c7d18ad4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3hUxfrHv282jRJagCBFilRBagABUZSr0hRFRZErIBcQ7GK9YsGCepUriqiAclEUhQt6vTZ+XGlSlSagoaMgzYBRQk1IsvP7Y3cOs7On79lsNpnP8+TJ7tk5c+a073nPO++8Q4wxKBQKhSL+SYh1AxQKhULhDUrQFQqFopSgBF2hUChKCUrQFQqFopSgBF2hUChKCUrQFQqFopSgBF2hCxEtIKKhXpeNJUS0l4j+EoV6GRE1Dn6eSkRP2inrYjuDieh/bttpUm8PIjrgdb2K4icx1g1QeAcRnRS+lgeQD6Ao+P0Oxthsu3UxxnpHo2xphzE22ot6iKgBgF8AJDHGCoN1zwZg+xwqyh5K0EsRjLGK/DMR7QUwgjG2SC5HRIlcJBQKRelBuVzKAPyVmogeJaLfAMwkoqpE9CURHSWiP4Of6wrrLCOiEcHPw4hoJRFNDJb9hYh6uyzbkIiWE9EJIlpERG8S0YcG7bbTxueIaFWwvv8RUXXh99uIaB8R5RDROJPj05mIfiMin7DseiLaEvzciYjWENExIjpMRFOIKNmgrveI6Hnh+8PBdQ4R0XCpbF8i+oGIjhPRfiIaL/y8PPj/GBGdJKIu/NgK63clonVElBv839XusTGDiFoE1z9GRFlEdK3wWx8i2hqs8yARPRRcXj14fo4R0R9EtIKIlL4UM+qAlx1qAagGoD6AUQic+5nB7+cDOANgisn6nQHsAFAdwMsAZhARuSj7EYC1ANIBjAdwm8k27bTxVgC3A6gJIBkAF5gLAbwdrL92cHt1oQNj7HsApwBcIdX7UfBzEYAHgvvTBUBPAHeatBvBNvQKtudKAE0AyP77UwCGAKgCoC+AMUR0XfC3S4P/qzDGKjLG1kh1VwPwFYDJwX17FcBXRJQu7UPYsbFocxKALwD8L7jePQBmE1GzYJEZCLjv0gC0ArAkuPxBAAcA1ACQAeBxACqvSDGjBL3s4AfwNGMsnzF2hjGWwxj7hDF2mjF2AsAEAJeZrL+PMfYOY6wIwPsAzkPgxrVdlojOB9ARwFOMsbOMsZUAPjfaoM02zmSM7WSMnQHwbwBtg8tvBPAlY2w5YywfwJPBY2DExwAGAQARpQHoE1wGxtgGxth3jLFCxtheANN02qHHwGD7fmKMnULgASbu3zLG2I+MMT9jbEtwe3bqBQIPgF2MsQ+C7foYwHYA1whljI6NGRcDqAjgpeA5WgLgSwSPDYACABcSUSXG2J+MsY3C8vMA1GeMFTDGVjCVKKrYUYJedjjKGMvjX4ioPBFNC7okjiPwil9FdDtI/MY/MMZOBz9WdFi2NoA/hGUAsN+owTbb+Jvw+bTQptpi3UFBzTHaFgLW+AAiSgEwAMBGxti+YDuaBt0JvwXb8QIC1roVIW0AsE/av85EtDToUsoFMNpmvbzufdKyfQDqCN+Njo1lmxlj4sNPrPcGBB52+4joWyLqElz+CoDdAP5HRD8T0WP2dkPhJUrQyw6ytfQggGYAOjPGKuHcK76RG8ULDgOoRkTlhWX1TMpH0sbDYt3BbaYbFWaMbUVAuHoj1N0CBFw32wE0CbbjcTdtQMBtJPIRAm8o9RhjlQFMFeq1sm4PIeCKEjkfwEEb7bKqt57k/9bqZYytY4z1R8Ad8xkClj8YYycYYw8yxhoBuBbAWCLqGWFbFA5Rgl52SUPAJ30s6I99OtobDFq86wGMJ6LkoHV3jckqkbRxPoB+RHRJsAPzWVhf7x8BuA+BB8c8qR3HAZwkouYAxthsw78BDCOiC4MPFLn9aQi8seQRUScEHiScowi4iBoZ1P01gKZEdCsRJRLRzQAuRMA9EgnfI2DNP0JESUTUA4FzNCd4zgYTUWXGWAECx8QPAETUj4gaB/tKchHodzBzcSmigBL0sstrAMoB+B3AdwD+r5i2OxiBjsUcAM8DmItAvLwertvIGMsCcBcCIn0YwJ8IdNqZwX3YSxhjvwvLH0JAbE8AeCfYZjttWBDchyUIuCOWSEXuBPAsEZ0A8BSC1m5w3dMI9BmsCkaOXCzVnQOgHwJvMTkAHgHQT2q3YxhjZxEQ8N4IHPe3AAxhjG0PFrkNwN6g62k0AucTCHT6LgJwEsAaAG8xxpZG0haFc0j1WyhiCRHNBbCdMRb1NwSForSjLHRFsUJEHYnoAiJKCIb19UfAF6tQKCJEjRRVFDe1AHyKQAflAQBjGGM/xLZJCkXpQLlcFAqFopSgXC4KhUJRSoiZy6V69eqsQYMGsdq8QqFQxCUbNmz4nTFWQ++3mAl6gwYNsH79+lhtXqFQKOISIpJHCGsol4tCoVCUEpSgKxQKRSlBCbpCoVCUEkpUHHpBQQEOHDiAvLw868KKmJKamoq6desiKSkp1k1RKBRBSpSgHzhwAGlpaWjQoAGM505QxBrGGHJycnDgwAE0bNgw1s1RKBRBSpTLJS8vD+np6UrMSzhEhPT0dPUmpVCUMEqUoANQYh4nqPOkUJQ8SpygKxQlgezsbPznP/+JdTMUccLx48fx22+/WReMMkrQBXJyctC2bVu0bdsWtWrVQp06dbTvZ8+eNV13/fr1uPfeey230bVrV8sydli2bBn69evnSV2KcK688koMGDAAp0+fti6sKPO0aNEC5513XqybUbI6RZ0yOzsb437+Gb/m5+P8lBRMaNQIgzOM5i22Jj09HZs2bQIAjB8/HhUrVsRDD52bKL2wsBCJifqHLDMzE5mZmZbbWL16tev2KYqPnTt3xroJijji0KFDsW4CgDi20GdnZ2PUjh3Yl58PBmBffj5G7diB2dnZnm5n2LBhGD16NDp37oxHHnkEa9euRZcuXdCuXTt07doVO3bsABBqMY8fPx7Dhw9Hjx490KhRI0yePFmrr2LFilr5Hj164MYbb0Tz5s0xePBg8MyXX3/9NZo3b44OHTrg3nvvtbTE//jjD1x33XVo3bo1Lr74YmzZsgUA8O2332pvGO3atcOJEydw+PBhXHrppWjbti1atWqFFStWeHq8SguFhYWxboJC4Zi4tdDH/fwzTvtDpyw87fdj3M8/R2Sl63HgwAGsXr0aPp8Px48fx4oVK5CYmIhFixbh8ccfxyeffBK2zvbt27F06VKcOHECzZo1w5gxY8Jitn/44QdkZWWhdu3a6NatG1atWoXMzEzccccdWL58ORo2bIhBgwZZtu/pp59Gu3bt8Nlnn2HJkiUYMmQINm3ahIkTJ+LNN99Et27dcPLkSaSmpmL69Om4+uqrMW7cOBQVFSmXggFFRUUAAL9fTYupiB/iVtB/zdefhtJoeSTcdNNN8Pl8AIDc3FwMHToUu3btAhGhoKBAd52+ffsiJSUFKSkpqFmzJrKzs1G3bt2QMp06ddKWtW3bFnv37kXFihXRqFEjLb570KBBmD59umn7Vq5cqT1UrrjiCuTk5OD48ePo1q0bxo4di8GDB2PAgAGoW7cuOnbsiOHDh6OgoADXXXcd2rZtG9GxKe2o+QIU8UTculzOT0lxtDwSKlSooH1+8skncfnll+Onn37CF198YRiLnSK0w+fz6b7C2ykTCY899hjeffddnDlzBt26dcP27dtx6aWXYvny5ahTpw6GDRuGWbNmebrN0oay0BXxRNwK+oRGjVA+IbT55RMSMKFRo6huNzc3F3Xq1AEAvPfee57X36xZM/z888/Yu3cvAGDuXOsJ5rt3747Zs2cDCPjmq1evjkqVKmHPnj246KKL8Oijj6Jjx47Yvn079u3bh4yMDIwcORIjRozAxo0bPd+H0oSy0BXxRNwK+uCMDExv1gz1U1JAAOqnpGB6s2ae+89lHnnkEfz9739Hu3btotJxVq5cObz11lvo1asXOnTogLS0NFSuXNl0nfHjx2PDhg1o3bo1HnvsMbz//vsAgNdeew2tWrVC69atkZSUhN69e2PZsmVo06YN2rVrh7lz5+K+++7zfB9KE8pCV8QTMZtTNDMzk8kTXGzbtg0tWrSISXtKEidPnkTFihXBGMNdd92FJk2a4IEHHoh1s8IozeeLj4T9/fffkZ6eHuPWKNyyc+dOfP/997jtttuiuh1+vRSHnhLRBsaYbox03FropZl33nkHbdu2RcuWLZGbm4s77rgj1k0qsygLPb7p2LEjhgwZUmZcZ5ZRLkSUCmA5gJRg+fmMsaelMsMAvALgYHDRFMbYu942tezwwAMPlEiLvCxSVoSgtHL8+HEAgdTcycnJMW5N9LETtpgP4ArG2EkiSgKwkogWMMa+k8rNZYzd7X0TFYrYoSz0+Mbn86GoqAhnzpwpE4Ju6XJhAU4GvyYF/5TZoigTRNNCnzZtGnJycqJWvyIQZACgzAygs+VDJyIfEW0CcATAN4yx73WK3UBEW4hoPhHV87SVCkWMiJaFvmXLFowePTrqnXVlndTUVABK0ENgjBUxxtoCqAugExG1kop8AaABY6w1gG8AvK9XDxGNIqL1RLT+6NGjkbRboSgWvLTQN2zYgB9++AEAtAFpv//+u2f1K8LhFvqZM2di3JLiwVGUC2PsGIClAHpJy3MYY3zM/bsAOhisP50xlskYy6xRo4ab9kaVyy+/HAsXLgxZ9tprr2HMmDGG6/To0QM8/LJPnz44duxYWJnx48dj4sSJptv+7LPPsHXrVu37U089hUWLFjlpvi4qzW5keGmhZ2Zmon379p7Vp7BGWegSRFSDiKoEP5cDcCWA7VIZMRHwtQC2ednI4mLQoEGYM2dOyLI5c+bYSpAFBLIkVqlSxdW2ZUF/9tln8Ze//MVVXQrvUFEu8Y2y0MM5D8BSItoCYB0CPvQviehZIro2WOZeIsoios0A7gUwLDrNjS433ngjvvrqK20yi7179+LQoUPo3r07xowZg8zMTLRs2RJPP/207voNGjTQXqEnTJiApk2b4pJLLtFS7AKBGPOOHTuiTZs2uOGGG3D69GmsXr0an3/+OR5++GG0bdsWe/bswbBhwzB//nwAwOLFi9GuXTtcdNFFGD58OPKDCcgaNGiAp59+Gu3bt8dFF12E7du3hzdKQKXZdU60fOjqQVE88HxJxSXosT6vlmGLjLEtANrpLH9K+Px3AH/3smH333+/NtmEV7Rt2xavvfaa4e/VqlVDp06dsGDBAvTv3x9z5szBwIEDQUSYMGECqlWrhqKiIvTs2RNbtmxB69atdevZsGED5syZg02bNqGwsBDt27dHhw4BL9SAAQMwcuRIAMATTzyBGTNm4J577sG1116Lfv364cYbbwypKy8vD8OGDcPixYvRtGlTDBkyBG+//Tbuv/9+AED16tWxceNGvPXWW5g4cSLefdc4/F+l2XVOtG9QNTdrdOFZUo2yonqN3+/XthkL1EhRCdHtIrpb/v3vf6N9+/Zo164dsrKyQtwjMitWrMD111+P8uXLo1KlSrj22mu133766Sd0794dF110EWbPno2srCzT9uzYsQMNGzZE06ZNAQBDhw7F8uXLtd8HDBgAAOjQoYOW0MuIlStXalEVeml2J0+ejGPHjiExMREdO3bEzJkzMX78ePz4449IS0szrbu0ouLQ4xs+w1hxTVgS6+ulxOZDN7Oko0n//v3xwAMPYOPGjTh9+jQ6dOiAX375BRMnTsS6detQtWpVDBs2zDBtrhXDhg3DZ599hjZt2uC9997DsmXLImovf6WMJP3uY489hr59++Lrr79Gt27dsHDhQi3N7ldffYVhw4Zh7NixGDJkSERtjRVHjx7FoUOH0KZNG8frxvoVWhEZXNCLy0IvKioKm8imOFEWukTFihVx+eWXY/jw4Zp1fvz4cVSoUAGVK1dGdnY2FixYYFrHpZdeis8++wxnzpzBiRMn8MUXX2i/nThxAueddx4KCgq0lLcAkJaWhhMnToTV1axZM+zduxe7d+8GAHzwwQe47LLLXO1bWU2z27x5c9cTeSgfevHxxhtv4Ouvv/a0zuIWdGWhl0AGDRqE66+/XnO98HSzzZs3R7169dCtWzfT9du3b4+bb74Zbdq0Qc2aNdGxY0ftt+eeew6dO3dGjRo10LlzZ03Eb7nlFowcORKTJ0/WOkOBQNjVzJkzcdNNN6GwsBAdO3bE6NGjXe0Xn+u0devWKF++fEia3aVLlyIhIQEtW7ZE7969MWfOHLzyyitISkpCxYoV43oijD/++MP1usqHXnzce++9ALw95tyfXVZcLmCMxeSvQ4cOTGbr1q1hyxQll3g5XwikqnC1TlZWVlTasWbNGgaAde7c2bP64x0358mKvn37MgDsnXfe8bReGd723NzcqG4nuK31zEBXlctFoTCBKddIXBNplMumTZuQm5tru3ysLXQl6AqFCcqHHt9EKujt2rXD1Vdfbbu8EnQJdaHHB2XlPEV7P5UPPbp4Ebb4/fd6uQj1UYIukJqaipycnDIjFvEKYww5OTlanozSjLLQ45tIolzcnKNYC3qJinKpW7cuDhw4AJWJseSTmpqKunXrxroZjvD7/UhIcGbDKAs9vlGCHkOSkpLQsGHDWDdDUUopKipyLOjFZaEvXLgQf//737F27VpNhBSRE0nYYjwKeolyuSgU0cTNzVZcFvrtt9+OH374AUeOHInq9soa/PxF00IXyxUVFTnejpcoQVeUGdzcbMVlcfG2OX2DUJhTHIIuXlfKQlcoigk3gl5cnZdcCGKZqa80ws9fNF0uStAVihhQkix0WSz4dlQnqbfw4+rGQrd77pWgKxQxoCQJOocLeKx9r6UV/uCM5tuZaP0rQVcoiol4cLmo+HRv4cfTzXFVLheFogTAGMO8efPCBLwkWujydpSgews/ntGMcIorQSeiVCJaS0Sbg/OGPqNTJoWI5hLRbiL6nogaRKOxCoUdZs2ahYEDB+KNN94IWV6SLHQjH3qsBaG0UdwWeqxdZ3Ys9HwAVzDG2gBoC6AXEV0slfkbgD8ZY40BTALwD2+bqVDY57fffgMAHDp0KGR5SbTQZR+6stC9pbgt9BIv6MEUvCeDX5OCf/Ke9gfwfvDzfAA9SXXXK2KE0aVXkix0GS44CxcuLJbtlRWK20Ivrok0jLDlQyciHxFtAnAEwDeMMTn9WB0A+wGAMVYIIBdAuk49o4hoPRGtV/laFNHGyK3hhOL2od9+++3Fsr2SxooVKzyv8/Tp05g3bx6A4rPQ40LQGWNFjLG2AOoC6ERErdxsjDE2nTGWyRjLrFGjhpsqFApLuIXOGMPp06e15SXJQvfiYVOa6N+/v+d1ig+JaAq6KOJxIegcxtgxAEsB9JJ+OgigHgAQUSKAygByvGigHmvWrHE0i4iibMEF/eDBgxgwYIC2PFY+9DfffBNXXXWVbr1eeibPnj1b4h8M999/P2bMmBG2PNoe2uJyuRTXZNRG2IlyqUFEVYKfywG4EsB2qdjnAIYGP98IYAmLkmlz+vRpdO3aFdddd100qleUIubMmRPik46VhX733Xfjm2++8bxeub6UlBTcfffdntbrNa+//jpGjBgRtjwagi4eY+VyOcd5AJYS0RYA6xDwoX9JRM8S0bXBMjMApBPRbgBjATwWneaeewJu3LgxWptQlFJKUpSL14LOheTtt9/2tF4nMMbw4osv4uDBgzFrgxFeWOgFBQUgIrz55pshy+NK0BljWxhj7RhjrRljrRhjzwaXP8UY+zz4OY8xdhNjrDFjrBNj7OdoN5wxhl27dqmoAEUY8RDlwuv1yjLNy8sDENtsjVlZWXj88ccxcODAmLVBxGsL/cSJEwCAJ554ImR5XAl6SUO8AZo2bYpevWR3vkKhT0mx0O+77z4cP34cgL6gu3mI5OfnA0DMJsfYu3cvVq1aBQA4efKkReniJ1ILffTo0dr4BvmclSRBj9upUdQADIURJd1Cnzx5MtasWWP4u9/vd5xGlwt6rNLvRjrTWLQ7RSO10KdNm4bdu3frlitJgh63FroSdAXH7/dj6tSpmtvBiJJioQPm0RDxaKFHSrTvZy986FysS7KFHreCrvCO3NxcnD17NtbNcM2nn36KMWPG4KmnngJQ8i10wLwtTh8ifr9fC+NVE2ToM3/+fBAR/vWvf9lex27O+riNQy8JKMvcOevWrcNrr71m+HuVKlVw7bXXGv5e0uH+6N9//x2AsaCXpJGiZnHoTq/xUaNGoX379gDiV9CjHbbIeffdd12vb3TO4ioOvaSihN0+nTp1wgMPPGBapjREC1ldE3Ys9KKiopC3lT/++CPidulh9qBw+hARB+pE0+Vy5swZvPPOO3F971WqVMl2WXk/+fWjXC4eEs8XkyI6yP0qkbhcrr76aqSkpGjfvRjvoGc1m13HkbwVRFPQn3/+eYwaNQrz58/3vO7iuq/T0tJsl33vvfdCvtux0JWgO0QJukLG7uu6HUFfvHhxyPdjx465apNIUlKSo7Y4vcbFB0Y0XS48HFFOS1xS0TuOTgR93LhxId/jQdDjrktcCbrCCC9cLjJedBbrWc1mPnSnFnpSUpK2b9EQdH4MuBjyPgsvKa5ghwoVKrhe1+j6KUmCHncWOkcJuzeU9GROdvDS5SLjhaDrWehmx93ptS3Wn5qa6mhdO2RkZCAjI0PzP+/bty/k9z179kS8jeIS9Ei2Iz+ET5w4gVmzZoVcV++++y7+7//+L7JGRkDcCboScm+JtUXhBXbHJsRK0PWsZi87RUVBF/3/XnHs2DEcO3ZMs9DFTtjt27ejcePGjuuM9D7+8ccfQUT45Zdfim27cqfonXfeiaFDh2L16tVama1bt6J3796utxEpStDLOLGeMssLSrqFrifoXvrQRUFPTk52tK4T9NwVBw4ccFXX4cOHQ76L+2xn/3k8+X/+8x/DMpFoRU5OePZv2ULnfQle9LN4RdwJOkcJuzeUJgvd6DunJAm61z50TjQsdI7ePec2qqZOnTqGv9k5T/y4zZkzx7BMJO7Eyy67zLA+vm1+XkvSoLy4E3S9OQKVuLunNFjonE8++cT091gJul4GRKc+9Pfff99wrIAoqnr+eq/Qa7NXYZLig82JoK9btw7ffy/PiBlAr712tSIrK8uwPlnQrVJOFCelIsqFMaZSAugwY8YMZGdnG/7+xx9/oHr16sXYosjIzc1FampqmBXKzz2/sbwcKVpSfOjDhg0DoH/9iyLu9QP6/fff1z7Ls9v7fL6oxL07PU9nzpyJuJ6CggI8+uijePzxxw3vCbk+fl6jEfXjlriz0DmR5jouC4wYMSIsllbk22+/jau3mypVquDSSy8NWy4K+AsvvGC4vpXY8fSoIrEQ9Eh86EYutG3btqF///746quvHA1Pnz17tvZZbDOPSY9G/nUnFjpg/JagV4/esfX7/ahXrx4mTZqEGjVqGJ5zuT5+XtetW2fZ3uIi7gTd6IQorJGPXSwnQ3DL2rVrTX8fN26coSBaCYXew2L79u2WN+z27dtBRGGDkjjF6UM3EvQ77rgDn3/+Ofr16xc2QYNdxON36tSpsGUcIsLmzZtD0iZ89NFHmDx5sq3t2Nl/O4Ju1+Wyffv2kDfZKVOmmNbHt83vn6NHj4a1KVaUCpeLEnR7yK6pknABeoG8H24FfdeuXbrLO3XqZGo187C12bNno2fPnmG/OxV0pxa6KGh2rNtt27bZrltsi+ja4NsxsoLbtm2LZs2aYfv2wPTDgwcPtr1Nryx0u7ogGzanT582rY9vW354JiYmlvzkXERUj4iWEtFWIsoiovt0yvQgolwi2hT8eyo6zT2H6hR1jnycSqugG93I0eoA5oN5jDrHIvGhM8bw0EMPmW5fDFW0E7X0xRdfaNOpWSFeM+L+8WNptr0dO3bY2gbgvlMUMB4da9flYnd0rSzoPAc9J5od0nax885dCOBBxtiFAC4GcBcRXahTbgVjrG3w71lPWymgLHT3lAaXix3cWuhuO/isBN1plEuTJk20z0eOHME///lP0+276RS1qpMjHkvR+jQTdDeGgpgF0anLxUknuJeCLneGloTJRexMEn2YMbYx+PkEgG0AjINIo4wSdPfYtdCzs7Oxd+/eYmhRKHv37tXtmLTCKwvd7aAcNxY6t+70zkF+fr42sMWsk5fj1OUCuBt/IHYWmrlc3CC6qpzWKZ7vXbt24corr8Tu3bt163n33XcN85xzrMYx8N/5pCIcqweD3+/HP//5T63/IRo4MtGIqAGAdgD0Aj+7ENFmIlpARC0N1h9FROuJaD3vSHCLinJxjnycjCz0WrVqRTxHpBsaNmyI8847z/F6dgU9NzfX9FqRX5mnTZumff7111/x6aef6q7nRtCtfK188nM7HYnRDFs0stD5A8GrgWnidsz24ejRo5g+fXrIORfP6ZNPPolFixZh/fr1uue6qKgIixYtCltmB9lCl0eTWlno8+fPx0MPPWQaeRYptt8RiKgigE8A3M8YkwMvNwKozxg7SUR9AHwGoIlcB2NsOoDpAJCZmenK8W0Uh66wprT60GWMRHv8+PE4ffo0/vGPf+j+Lgt6gwYNtM9du3bFwYMH4ff7w44bv5GN4qHdZEDcsmWL7bJiu+0aN3bvGSuXi1cPELsG2tChQ7FgwQL069cvrC3yukb1/PnnnyHf5X0wui/E5YyxsMlPrASdXx/RmjQFsGmhE1ESAmI+mzEWZqYwxo4zxk4GP38NIImIojJiRblc3FNaBV0+/2bXgzxpgYjschEHMB08eBCAvkXKj6sTC92KhISEsP245JJLwBgLEyQ3Lhe7uPGhR7ods33gbg7R3SEeJ/7WWVRUZFiP3Ga53MqVK3XXE70Kubm5YetZCTq/DqKpV3aiXAjADADbGGOvGpSpFSwHIuoUrDc8u40HKEF3j1NBL0lvPk5m+DG7Ho4cOWI4ela20PVS0eoNOuHb44K+cuXKENEw63xevHixrvAkJCSEuWWKiorw0ksvoVq1atoDBrAn6PLxs3Nuv/32W3z77bfad7cW+k2m1FQAACAASURBVMsvv2xZxo5lDQAVK1YEgBA/tFieX9NFRUWG9VgJ+oIFCyzbqpe8y0rQxbZFCzsWejcAtwG4QghL7ENEo4lodLDMjQB+IqLNACYDuIVFWQ2UD9058nGyurCi2XnjFCdD5a2uh9dffz1s2ZkzZ8LyfOsJup4rhF+LRUVFWLt2Lbp3746nn35a+93MQvf7/bouoISEhDDh8fv9mh9fFPRo3QtyJkO9TlGzNxbOo48+arktuxY6z/hoJOj84VlYWGjLQj9+/LhjgS0sLNQVdKs3Md62aOqVpQ+dMbYSgKkpxxibAkB/eJXH6CXnUoJuD6NJb404fvy4ZhHFGrO2OhX0jIyMsGWiQHL0BL1r166GURKMMe1G37Bhg/a71Y2uN6DJyELXSxFsVwydIou13SgXN7ac3X2wstBFQbeKdlqzZg26du2KJ5980lFbCwsLdf3g8hvvkSNHkJ6erp1//j+atm7cBSKrTlH3mAm63ghHoxFzscCJoFtdD3bzk9ud/Ufcnl7EixsfOhGFCbrf79f2VXTjiPtvV9Dt3DNyXbLL5fDhw7qjTt0YWHYNNN6vIT5cjATdKJKIP6hWrVoFIHweWSuKioq0XDZGHDhwABkZGXj44Yc198/cuXO19aNF7CPhXaIsdOeYCXpCQgKGDx+OKlWqaMtKUq50Ly10Mz+4iJ3c4gMHDsRXX30FIBA/zi3zXbt2YeDAgZg5c6aleBrlcxGPf40aNVBUVKSV5f+/+eYbbfutWrWyHcdvJ+mYmaAXFhaidu3auus5ibTh+2H3oaR3LOXrmLdv9+7dunXw48r3x+kAu8LCQl13pHgeH3vsMQCBUMVJkyahdevWmrsupp2iJQ3VKeoeM+Hz+/2YOXMmJk2apC2LdV4KkUgEXfbh6okZr79GjRraMjtDuefNm6e9yezfvx8PP/wwgMBsNvPmzcN//vMfV2+Qfr8/5Pjn5+fD7/eHuVyuuuoqFBQUoH379rjssssM7wW5DXZyeFtZ6EbY3V+xDrsGmuje0isv5lnZuXOnbh2RCvrZs2ctBV2eRUvse1GCHmR2dja6rl8PABAvmdIm6EePHsUzzzzj+X7l5uaiXLly+OabbwAY31Cc0iLo4lsHYC7o4s1tNHL0zTffNG+sABdip8gWer169UJ86HKdZ86cgc/ns/06/9Zbb8FqcF+0BV3OWWOn7u+++y5s3fXr12PSpElYsmQJZs6cqdVh9NCSB0U5Dd89depU2NR78+bNC6mH+/r1zr0SdATEfNSOHTjIb8ZS7HIZM2YMxo8fjyVLlnha7+bNm5GXl4dnnnkGQOiNozc3pBuXy+rVq9G3b1/P/YROBF0eLi9bYGaCLvq7jSz0u+++G4A94crLy3PtcuEC+uKLL6Jly5YhPnT53Gzbts1U0PXasHz5ctN22ekU1UPPYrYqZyToBQUF6Nq1K5YtW4b8/HzNZy+uO27cOIwdOxb3339/SNut8pq7tdCBwDkRqVmzZsj3qVOnhu0LRwk6gHE//4zTNl8n4x3+Cv/SSy/hjz/+wP333+/pvIVimB1H7/XUjYV+88034+uvv9aNGokEpzP8iDgRdLGslcuFp4Y1Iz8/3/L6NHIj8uN/wQUXICEhAUVFRVp0hd7D1ufzORILq9w10bbQT548ifvuuw9//vmnYRz6/v37sWbNGtx+++0h+2zlS+eCLrrQOEeOHAnZH7dJ6sSHf82aNXWvF70xD0rQAfzKU1WWAR86v8AWL16MRx99FK+//rrWQ+4F/HhZWdFuLHTZd+i0TUaYtfXf//636bpuBd0qOsXO8PwzZ85YHgu9KcwYY9rxT0xM1KxvPtO83rnhom8Xq07faAv6e++9h8mTJ2PChAmGFjofrFNUVORK0K+44oqwctw9s3XrVgDuR0yL69WsWROffvopbrjhBsv1lKADON/k4ittgi4KCRcfL10YsoUupmsVcWOhu705rB4eRvu/efPmkNGMetgRdL1wQCvsdCzaEXS9ya3FsLukpKSwuHQjC93JdcIt9JMnT+rGVcvbELdv9sZo937koX/yW8yHH36ofRajVqwEXWwTP35GbyFr1qzRRoRGIugLFizAxIkTUa1aNTRu3Fh30JqMEnQAExo1QvmEhDJloQPRybciC7rRkGWvwxazs7MN63Qr6HaydsoiLU9MINYvl61cubJhvXYE/fTp066vTx4CmZSUBJ/PZ0vQnWyLn/dGjRohPT097HczC90oERlg77gA0PpypkyZEvKWxf3PYhvsWOjieeUWupHbTDxObt2ZjDH06tULDz74oLbMTk70WA/9LxEMzsjA9GbNUFvnBJU2H7qbgSgy+/fvN/xNdrkYXfRWFjpjLMztIOarWLFihbb81KlTqFWrltahKONW0M2EhePW5QIEZveR4ftoV9DdXp+8/sTERFsWOk/o5TRs0OihKB/zn376SftsdtwPHz5sa/tm8H4kvp9y1I/e9aAn6EYWOmNMi37Se8DbQa8Ndu7dpUuX2up/cUPcCDoQEPVv27ULW17aLHS9i8KpKLRo0cLwN9lCNxJ0fgPl5ubq5gKfNGkS2rRpo82pCZwTu5dffhmXXnqplnuai5ORv7s4Bd3MQpePvd6xKS5B58fEicsFsH8/FBYWYtOmTYa/m1mSdo67U8Q0Ezzqiu9nUVGR5f7LUThWgi4nVXOK3nm1O2uRWdbPSIgrQTeCn5hTp05hypQpcW+xe+Fy0Rv4IOfB4cfNyKrgN9Dw4cNxww03hEXCcCHnHXUimzdvBnAuR4rRxLoct4JuJ4GYLOh//vknsrOzQ0I1ef3yICQ9QTCaU1KPSARdfMjYdbmI69mpv52OgWS2DY5bERRJTExE/fr1te9iRAqvX0wCJrZHr21GFvqFF4bPmCmGgNo5j3qdq3rYFXS3s2NZEXeCbjZS9NFHH8U999yDzz//vLib5SleWOh6yKPs+M3yyy+/6JYvLCzE1q1bNetcjsbgVlq5cuXC1uXWkpifGjB247gVdKucGkDoTVa7dm3s2LEDtWrVQr169bTl/NjIbzZmoYvRttBFQU9ISAjr9JMxy+an1waj884x2z8vBL1q1aoh50a00OXBP7IPXW8f5ePDBX3dunVhZf1+v3Z87Qh6y5aBSdj69u1rWs6uoNtJK+GGUiXoPNNdSUoq5QY3FvqxY8cs/XJcTGVB5z7UO++8M6w8v5CBcBHRE3TeXjnGl58jvjw7O9tSoETsuFyMps0Tb546dero9i8Y+dBj6XIR36DkCBY7Fjqfqu3EiRMh5caPHw8AGD16NMzgx3b27Nlhv/E3sEgoV65ciPGSkJCA22+/HcC5/fvxxx8BhAu6FWfPnkVhYSGSkpJQvnz5sN8ZY9r1J57HGTNm4K677sI999wTUp5b1FYJ2+z2fykL3QQ3yftLAjNnzsSsWbPCln/wwQeO6+rcubOp3xw4d5PoDSwCoOUhkcsbfecPTlH0ZEEfN25ciDXEh6/XqlULQ4YM0dazijSwI+hGFqd489SqVcu0fvmGNHK5FBUV4f333zdtM2+f2z4e2UIXsSPoEydOBBDuEqtbt67u9uQHVF5eHm644Qa0bt06rOxnn31mZxdMSUhICLFoiQi33HILgHP799e//hVAwIrmKSvswF1xRsIp5ksXLfQ+ffpgypQpaNy4cVhbAWvLWgm6Q8ws9HibUm348OEYOnSorbJWDynZv/2Xv/wlrIyVoMuuE9k9In+X/ZzAuXPABXrfvn1Yv359iADxG2j+/Plhy4wwss7sdM6JN2GlSpV0yzjtFP34449tzQ1ZWFjo2sDgx5Bb6HK9MkadokZzoMpcffXV2me/34+srKwwK9pLiChM0Pl3vf0bO3as7br5Wwk/fxMnTgzpHxEnFRGvPaN9lbNcGmF3HINyuQQp7dkWvZr3U0+4uGgZjRSVBV32mcuCzsVUz3oWX/M7d+6MK6+8UvvOHwSiWFq5L4zcaKKgGw3qEK0hI5+4UwvdrkgXFBRELOgJCQm2LHS5v8Jou0bHQMztIka/iMfES6OJiELqEy32SMdAHDt2DMC5a/rBBx/ESy+9pFtW7IexEmSzcQlOUBa6CaVJ0PWS9QPhN+cFF1yAAQMGGNajJ7JGPnSOLOhyxIeRC0ZvW3Jss/gGwUVYtM5EQZ8yJXzyK76OfCOIgl6nTp2w9eR1jHygTi10ORmTEZEIOrcc9Sx0vegeoygX+QFk1tHL1+X9USNHjgwp74Vlyc8BEYW42kQLPdJMn3wibT3/uRlGgs4fPHXr1g2bqtAOS5cutbWdSLEzSXQ9IlpKRFuJKIuI7tMpQ0Q0mYh2E9EWImofldai9Fvo4s0ohlbJ/Pzzz2FzPhrVw7FyuVglozKymvRcLmZw8RaFVhR0uUMKOGehyw8dUdCNXAmiCBkJulEIp5GFzvfTSti9crnI+8YtUBFZ0J1a6AC0rIVXXXUVgMCbnngMvLAse/XqpX0W3R1WLhcn8AABvQgsM+wI7fnnn++4PfIxj9ZoUTuPiUIADzLGLgRwMYC7iEgO7OwNoEnwbxSAtz1tpUC8TEH34YcfYsSIEY7XM7KCx40bZ2s/iQi//fab7gWj53Jx4h+VrSbeHrHNTgRdvMitfOhc0GVB5oKelZVlKOiiCPEoChmjKJcKFSrgoYcewrRp00K2yY+h1c1dUFAAv9+Piy66SOvwswvfps/n0yZH5hw+fDjkfPDRpIC1D91M0CdPnhziupIF3QsLndfHGDMU9H79+uGNN96IeFtOLXR+P8jHTE469/LLL+N///uf7XpLjKAzxg4zxjYGP58AsA2A/G7bH8AsFuA7AFWI6DzPW2tASbTQb7vtNsyYMcPxesOHDw/5zm/aI0eOmI7qE1m9erVtC92JoD/77LMh3/V88XYE3crlooeZoLdv3x4XXnihLQu9YcOGaNq0aVgZI5cLEeGVV17BqFGj0KBBA205PxdWccfc5dK4cWN8/PHHWLhwoW7UiBkJCQlhk3W/++676NKli/Z9y5Yt2n5yd4xoANh1ucgUt6CLbbv33nsj3pZTC90I+bp++OGHQ/qFrJCPebSmd3TkyCGiBgDaAfhe+qkOADG49wDCRR9ENIqI1hPRejtJlfQojS4XMRxLTpPrVCz5OlaCfvbsWfj9fke+PDlvi5Hrxgorl4seRoOYzpw5oy2zY6EbuQyMBF1EvPZ46lUrceSCLk4ZN336dNN1ZHw+X5igA+eSd/F2tGrVCoB1jLgTQU9LS/Pchy6OWBYFXQ5j9AK7E33bxa03QL7uYi7oRFQRwCcA7meMhSdwtgFjbDpjLJMxlqmXeN5mHWHLilPQmzVrZitFphXifnB/pR5GnaRmWAn6zp07kZKSgrVr18Ln8+GXX37RHU0nk5qaigULFoCI8Ouvv3rqcjETdMaYFongRtBFEfJK0Hlst5UAcR+6eFyc+qF9Ph/S0tIsy/HYaT5wSk71wLErmomJiUhJSbEVJeQEPl7CzOVih0aNGgEABg0aZFjGiQC/8MILug/OUaNGOcrzf9NNN4Utk49btNzEthSCiJIQEPPZjLHwLE3AQQD1hO91g8uKheIU9J07d4ZMdeUWuxneRGF2KugXXXRRyHLZKvj222/h8/nQoEEDZGZmWtbbrVs3zJs3DwCwYMECXQvdicvFrg995MiRePLJJwGEi1FeXp4jC92ofaKgb9u2LcT61YPHoNt1uUQq6HpCI1K1alXNz37q1CkUFBRoA63k+8OuKPORw+JDjk+24Za5c+eie/fuAEJHawLOBb1p06ZgjBn2iwDOMik+8sgjusv79OnjKFxTLwFdUlKSVseoUaMwcuRI2/U5wU6UCwGYAWAbY+xVg2KfAxgSjHa5GEAuYyzyHJo62OkUjdbTz8tJk+3mwtixY4f2+auvvrK1TlFREfx+P2rUqBFyg5gNRrFDYWGhZhX9+uuvtmc+khHTwu7Zsweff/552EhR8RyKfRHyubVjodt57RYnuGjevDnatzcP1OLl7Qq6ncmnjTAT9LS0NFx//fVIT0/XLOqTJ0+GhDW6FXT+ZiOKWY0aNSIS9PLly2vb1zPEnAg6z8VjtM7gwYNx2WWX2a7PyGASHwp2tSUrKyskC2lSUhKWL1+OUaNGYerUqY47a+1ix+TrBuA2AFcQ0abgXx8iGk1EPBnE1wB+BrAbwDsA7jSoK2JiOVLUTmY/u1j50LjFLOZneeyxx2zVzYfay0PGIxV0sRPVaMowJy6XzZs3o3Hjxujfv3/YzW301uVG0I1GhwLA999/H5KawK7LxWpyEE5hYSH8fn9EFnpCQgKqVq1q+JsYaVOhQgWcOnUq5LzLxzKSBFLPPvusY0FftWqV9jk1NVUTdD1DzE7b+JtI9erVARgns/vwww8d+fzFcyR+zsvLc6wtF154YUindVJSEi655BJMmzYtqjplJ8plJWOMGGOtGWNtg39fM8amMsamBsswxthdjLELGGMXMcbWR6vBsfShe5H0a8OGDRg3bpylVRvJwAPucpEHpHgh6HoztTvt4NEbri+/sRjV6ff7MXDgQNx6661aXXqCLlpHZjf1xRdfjIkTJ9oSdL32WQkQP2aRulzEGYXE/SkqKgq5Vrigi/eJfH/Y7SjUO26XXHKJo2smLS0tRNhSU1O1Y+ZW0Pl6/Ji4jY1/+OGHbd1n4sPU7du/F30PdlAjRR2gl4zKKZmZmXjhhRcsRdDsKS6uq+e6EQXdawudu52OHz+uHQ+3FrqInBfF6PgwxjBv3jx8/PHHAIwF3U6fACcrK0t7rbZr0dm10IHABCHicTHahtGxkwVdFOSzZ8+GnOOKFSvi5MmTIfeE+AYCBK5fO8Jk1E75mjG7hhITE0FEeOaZZ3DNNdegS5cuhha63++3dTz5dcctdLeRLA888IDlqM/69eujX79+ric/5yhBNyCWA4usMrg5wSq7oJkwir361113XdjvRoLudsosTmFhoSZ8b775JnJzc8Pqteq8A/TfdPhQc3FbesgPbyNBd3ID/fTTT5o7y0zQO3TooH3mx8FIgC655BLtt6NHj9qy0M0E3Sh9gSzo3EKXBd1NtJTRseDnnWMmwvzB+tRTT+Hzzz+Hz+dzJeh66SB4XhW3oZTnnXeeYeZJDhfzSN0k0crdIlMqBL24LXQnJ8foYWMl6GY3nZilcOHChWG/89SgsstFr1NXFnQza6eoqEg3akAUXz5PoxmyIACBVMJGdYqIx7OgoABFRUWaoJs9nN555x3DKIaNGzdqn83qEKcN48fB6MExf/78kFluREFwaq3xa8FIwMQ2p6SkID8/P+Se2LVrV8g+2hUnu9e5UX033XRTyLXKsSPoopuDiHDXXXcZts+Nhf799/JQGn14G6+99loAAYF3g7LQHSALerQsdr1wOyuMHjZuLXR534xma7frcpEfHLt27QrxP8v1ms3HadZuEZ44yQw7gi4PNpKtuz179mgDgEaMGKGlTHVrLYmZ9vQsdLFz0ufzhQ2a4aSkpOjmEzez0IFzceby9SfWnZSUpHXEcsRUsXJ5o20BoftWt25d3HDDDbrrEFHY5ChA4C1Fr0PazIderlw5fPnll1iyZEnIciAwKYoYJMCPgxsLvVOnTo7Kd+jQAYyxkLc0J0QrGVfYdoplKx4SzSiXY8eOhaWMFRGTJRlx8OBBvPrquehOo85PK0E3Qs7pYSXoTjtF69atG9KJJddrJeh2Qhj1EkvJ2HG5cEHnFpos6I0aNdKd9MMs6sUuvB9AvFGzsrK0zwkJCSEjQuVrs3///mFtE8uI4snPEd9PWRzE74mJiVr+GI78Zsa389FHH4VNsCKeP/Ha2L9/v661zesTr3kusFbJwRhjWLRokTavKN9e3759Ubt27bD1atasiWbNmoXsKxBqoV933XVYuXKl7nZjQXHP0VCqBD1SqlatimrVqhn+Ls+TqcdVV12FBx980LJtbl0ucoSIXn7maHWKFhYW6nZoivV6JehG9dix0K3yd9gZdWkFf7CJx1f+3LRpU+34/ve//w2rY9myZSHfxQmT9erlscuyQItljx8/jjVr1uDxxx/XlsnXGi8/aNAg9OnTx2gXbV8bCQkJIWXvuOMOAMaCLoa+9uzZU+u/EOsQ30KMZvDSE/RevXqhW7duttpdHGRlZYW5E6NJ3An6AqnzDPDWxSIKSU5ODr777jvtO78xzJ66/BVfrz6RSHzoIlWqVEF2dnbYNvfs2QMAji10M+xY6HZCGKPlcuHnxWrQhheCztthNPmDPNmEXr9BzZo1tSnx0tPTQyY3F7P+8Xr5fsoPVfFaWbt2LYBQf798rYntNDv/dq8NItLKduzY0bZVKh9Do05tPg2dDC8vulxK2qxlLVq0wLBhw4pte3El6LOzs/H83r1hy7+VQt68EvjLLrssxP1gR9BljAT9wIEDpuvZ3UZqairatWsXsoz7Zz/66CPXFrqelWMk6AcPHkSDBg2wdetWWyNgxdGvRhhNnqHncuFCx0Xg8ssvN63byVRmVphZ6E7WHzx4MDIyMrTl/LyI58dI0MUyelFGRi4Xq3Y6EXQiwsGDB7FmzRrLED/uNrzmmmsAnBNmcXt2+jn4+ZYnmvaCrl27AoDpG0xJJK4EfdzPPyNfx4Ux57ffAHj/dBZ9ooA9l4uMkcvFyOrgiFbmM888Y1iuoKAAhw+HZln4/ffftc/ixa73cDES4C+++CIsle++ffvCZl4BArPC79u3D5MnT7Yl6E4sdLk+Mwu9WrVqWL9+ve7E2yJdunTx7EYVrwW7QinCxUzOkcLXF5fxdATnnReamVrclt7bh5mLxisLHQBq164Nn89nKejly5fH/v37tXzv/PyJ94qdwAO9EEcrDbA7OUX79u1x9uxZ9O3b11b5kkJcCfqvBol2fnfZwWgXfqHxG4PfEJs3b9aSVRnhNM8J733nfkjA3IWgZ3UbzZGoF7YoTy7NqVq1qm6Mrlg3h3ckV6tWzXaOGiv4fsmuClEk+INO9Jl36NDBVg5sryYYEEVFPNZyG95+W3/OFz7jkdzfoWehP/TQQ1i+fHlY6JzVm4GZhe6FoBs9vMzelOvWratZ4fz6dhot5UbQN27ciB9//NGybqD4Qg29JK4E/fyUFEDnIkmP0qzkHC4ussulbdu2GDhwoOm6fr8fS5YsCZufU+bbb78FELiIrrjiipCOHrPBOnoifeTIEQDAc889Z+lDd0NycrJuJAwX9FtvvVU3I2WbNm1sb4O3Ve5A3b17t/aZ55F382bm5bHgGOUCAUKnXRPhaaTT0tJ0hVm22rt37x4W6WQ29yxQfBa60XcruKA7PSeioA8ZMgSA9ZtRenq6lje+NBJXgj6hUSOk6FwsN7nMrW4XbjkYuVxq164dlqpWXLdnz554+eWXTbfRo0cPrbxsrZl14hlZ2ADQu3fviEaKGt2YvXr10p2tpUqVKsjLy0PlypXx3HPPhf1ulcFQhIuQnYgYo8mhzfBK0MUOuYSEBPTs2VPXh2/0lsUt+SpVqoScC/5ZT6BkQRcfKnouvhMnToR8j0anqB52+7LcCrpoQfNtFVe8d0klrvZ+cEYG/q7jA7vYg6gFM2QLXb5oDh8+jJ9++kl3Xaev9nrx42Zx07L/XCQ5OTkiC93oRk1JSdG9cRISEpCXl4fU1FTLV3+rjkve+aoXHSLSsmVLLaWvE9wK+sMPPxzyXY6wWLRoUcigGI6RG4iLWYUKFSwtdI4s6GIb7Iio3c5bty4X/t2uoPO3Uaf3imihP/7442jVqpXrkZylhbgSdAC4WidOfNiwYXA7pZ0drCx0M/SyE4pcfPHFYeVlQXcbZpecnBzSVj6TjYjZTWS0nykpKbpiz+PUU1NTdcXgwgvPzS1ulYSJ++Jl61LG7bFxK+gvv/yy9jYFWA875wmkrAQ9Pz9f10LX8+PKqXTFNtgZk2HXJRKpy8WuoPNrQe+c6A2ck9cDgObNm+PHH380HUdSFog7QTe6SIwsZC/gFxq3MJ0MZLIaRSnfnDwdqiimPFbZKUlJSSH17Nq1y9H6Rjd+amqq7m8HDx5EQUGBoYVevXp1vPnmmwCsxYILutWkIm6H8etNE2a3Ljl3CmAs7KtXr8a0adMMH2B85GPVqlV1fdt6691444144oknwtoA2BNR+dw98cQTWLNmTVi54nK5GAn6tm3bsG3bNsv1ImXXrl1h8+XGK6VG0KOJbKHLqV45ehe22RBsIPRmNEqq1bx5c9N5R41ITk52NAWXXYws9GeffRYADAWdMaYbO6yHXUF3m2lv7NixYda/3SRPYtv5Okav+k2aNMGoUaMM67rvvvswb9483HzzzbquED3RSkxMDOnwdirocpnnnnsu7E0RKL5O0QYNGqBixYp48cUXQ5Y3b94cZnMPexWF0rhxY8M+sHgj7gQ9Fsg+9N9//z3Ml2qEaJXriZPYoZefn68r6EDAKnNKUlKS7mQSImYiZuZDt6rT5/OFTabt9/u1m9Cuy4UfM6POX7cWOhGFRQ9dcsklttaVE20B7qcn9Pl8uPHGG8OO9cGDgSl5zfJ1c2NBPB923h7t+qrtCjp/kMs46RQ9ceIErr/+elvlOV5Z6KWJuBN0o4tEvCG8tuJlCx0AJk6caGtd8QbT8xGOGDFCyxl95swZQ0F3c/EmJycbzrLUv39/AOZ5T8wE3cwK4w+Je++9N2S5OCONHUE/dOiQZt0a+crdWuh6zJ0711Y5PQvdbbI1IxYvXmxZhl/nTiz0mjVr2k5OZqeviDEWNuExf2hHO+JECXo4diaJ/hcRHSEiXSc1EfUgolxhvtGnvG/mOYwuWHlmdT2ys7NdWVKyhe4EMUpDb9tJSUmaaOXl5WmdovLN4Ob1MikpydDlwuszs9AHDx6sfRYHOtmx0PXg+wbYc7mIIz6N9t/LiQPsTM4B6PvQvZxAHHDWaWtX0CdPnozsE7UR4gAAHKVJREFU7GzbQmh0jozmN+U8+uijuPvuu3H33Xfb2o5blKCHY+cR+h4A/VER51ghzDeq//4VYwoLC1GrVq2w4ex24Ba6m5tWHGLesmXLsN8TExNDcnTwTlF+M/EoCDczrScnJ1umMDXLC12/fn3teHXv3h1jxowBYNwpyjESdMaY7bk78/LyQsLzkpKSQrIRcryy0G+55RbbZcW28zZ6ParQybUmHoOhQ4calrvnnnsctcHoHBnly+ekpaXhjTfeiNrM9q1btwagBF0PO5NELweg3wsYA+y4XPTg1rXVUH09IrHQxanV5GnWgMBNwwVQz+WiF4Y1btw4FBQU6EZqiJiJTEZGBpYsWYIPP/zQtI6XX34ZDzzwAAYOHKgdY7suFxlR0O24XERBT0xMDJl1h+OVhc7nKLWD+PZ0ySWX4KmnnsKMGTM8aQfHraC/+uqrpjn9nWAk6M2bN/ekfrcsXrwYS5YsKfODiPTw6oh0IaLNRLSAiMLN0CBENIqI1hPRerdx427941xI3FwEej50rxDnWORTqvl8Pu2G5q+34n4TkS3rxOohd/nll1u6GdLT0/Hqq68iKSlJq0/8rIeZy4U/HI3av27dOm22H3Gfk5KSUK1atTArvbjmahSRZ/V55plnwhJmRYqTQTaiJezz+XT7G4wmLTHDzVthcVC9enXLgWllFS8EfSOA+oyxNgDeABA+t1YQxth0xlgmYyzTLBzJjG8MQgatcCPoXLTuu+8+HD9+HGfPnrX0HzolMTExRNALCgqQnJyshdTx5E1m7QOg646Qy4i4eTCKA0b4Zz3xNupoZYxpgm6URzwzMxPlypVDXl5eSIQOfwDIr/F2EnF5jVHbvcRJOoOGDRuGLZMf1HqjV60oqYKuMCZiQWeMHWeMnQx+/hpAEhFVj7hlOszOzsYrv/7qaJ1vvvkGubm5ukJiBb9ZeXKtgoIC24LOh6NbzSouWuhnz57F2bNnteRX99xzD95//33Ddoli8sorr9hqlxeIkTt6MyaJIi9HH+m5XOSHbGpqKvLy8kIidHg98rB3NxMEy/DRnHbRm3zCKxo1aoTx48frpimW+eSTT/Diiy86Hk1qF6t7xWvjRhE5EfcqEFEtANmMMUZEnRB4SIQ7iz3AKB96sB1hy44cOYKrrroKvXr10qaBEsXjmmuuQUJCgu70YHKdOTk5jiz0mjVrokuXLiEzHunh8/k0t0FBQYEm6ImJiZg8ebJWTna5yO1z2kHkxkLXy/1duXLlsBmTZEHn2zITdNHFwAVdL4ZetsgjtdD37dvnOH2AWeKsSOEzTdmJchkwYIDhb246aS+66KKQ1LJm+7d7925UqVLF8TYU0cVSBYjoYwA9AFQnogMAngaQBACMsakAbgQwhogKAZwBcAuL0nBOo3zoRnBByMrK0m4Q8SL98ssvAQBff/215YQHSUlJWLx4se35ColIEyYzRAt91qxZOHXqlO7NqCfocvucEKnLZfTo0di0aROuv/76sBTCcgZCbtH7/X4MGzYMq1evxpNPPokqVaogOTkZL7zwQkgnoJ6Frlc3ELmg253wQIRfQ9Gc7ixSd4ebvoXbbrsNjzzyiPbd7Bq54IILXLVLEV0sBZ0xNsji9ykApnjWIhOq+XzIMbjIFixYILYJQOgNpyfonNdff11X0MX1V61aFfLfCiJCSkqK5dB70YfO3StWYqsnJLKFznM+ix2ZkcZKi4JeqVIlfPTRR7ozrIsPFzlXd1pamhZRwlPsvvTSSyHrp6SkIC8vTzcxl+w6iKUPPZpRFpE+LNxY6HK+eq8mX1cUH/EV90OkO8EFAPzjH/8wzfJmJuhGN6Z4U5kNwdajYcOGji10jlWEgx2Xy/Tp00O+2x00Y2e74vHV882K+yPGuRsJhGxN8uOmlzq3rAh6pLix0OV8QUrQ44+Se0Xq8EdhoaGgA8ZWzf79+7WEWm4F3SlTp071VNCdulzkG1oWdDculzFjxqB27doh8e/t27fHgw8+aNiWL7/8EpdeeqnpNuVYe37c9Ca3UIJuD7cDnZ5//nntcywS4Skio+RekTqcH8GowAceeACAfUGfO3eu67jzZs2aoWLFikhJSdHt3BJDEUWXC0dP0MVsfnYsdNkH64WF3qRJExw8eDAkcichIQETJ04MeSMQ21K5cmXMnTsXPXr0wN/+9jfdenmUyZ133gnA3EL32ofuBn69lGRBdxufP27cOLzzzjsArCO0FCWPkntF6jDBxcw0HC4O/CYU/cl6Fq/RHKDiJA0yI0aMAHBuAJJRqNjjjz+ufdaz0PVedatWraqtZ2ShiwmdrATda+vr1ltvDWmLSK1atbB06VLDVKgtWrQAANx8880AAsctPz9fs9D5fJH8N5FYWugleRLhSNo2YsQIMMZUWGIcEleCPjgjA/0czEgiihYfkMIFXQyJc2JpffDBB7rLu3btiqeffhqAtaCLYXIJCQm2feiyCMsW+hVXXKENy5Yt9mgLn7gPTsVkypQpeOutt9C9e3cA5zpFT506hb/+9a+6sfgcu5kDvYRH3/z5559R3c6XX37peuKFkvywUUSPuBJ0ALjXxmsgFz7R0uWCxsVbdIXIFu/JkycNLVijZFD/+Mc/tN94ZMvevXt1y4rWMhE5FnQzH7o8gOrqq68GgLBJqr220MV9cBoTX7FiRYwZM0bbr+TkZC0mXz7e8uQibqegiwS9nDLRoG/fvq4nXlCCXjaJO0E3EyI57E8erALoC3pCQkJIvY888oipoMv+ydGjR6NLly6atXjXXXcB0J/mDAgf7ehllAufmowPHf/ggw+wc+dOdO7cOaQOrwU9kkFOMsnJySGjZkXk2PRYCLoXo1OjjdksSYrSS9zln3QyxZbe5BJGFrpY9ujRo4bbSU5ORoUKFUI6TN9++20AAavY7/dr4maUEEl2f8iC3rhxY9P9MrPQhw4dGpJCNTU1FU2aNNGtL1pEGkNtJuivvfYaatWqhWnTpgGIjcuFu+s+/fTTYt+2Xfr162drjgBF6SLuLHQ76Ak6T07EBV20gsXRjEBA7M0sdN5ZNHHiRGzevDnkd/kG4n5hEdnCkwXdaHq7Dh06ADg3AMSpVfzf//5Xy1JXkkPSzAS9QYMGmDp1qvbdi+gdp/BQVKMHr0IRK+JO0BeLPtRnntEtoyfoHDsWekFBgVaHHC2SkpKCBQsWYOzYsRg7dqyWbN8IPimEXIfR9zZt2hgO+77pppuwe/du9O7dO+w3Oz7Ta6+9VgsNdJLNr7hJSkrSBN2oz2L06NEAYhM6yAU9WhM4KBRuiTtBf//w4XNfMjJ0yzgVdNmHLlroDRs2xMKFC7XfUlJS0LRpU/zzn/+09TqrJziyYBORFiZpJVBiDg1x+7Jf3ogbbrgBc+bMwWOPPWarfCxITk7GmTNn4Pf7DeOp33777ZiNZOQul1iETCoUZsSdoB8R85EYWLJ+vx+nTp3SZk4X4SIoulw2btwYEhonju70+XwhVqLTARt2BB0Axo4dCwC4/fbbbdftRtCJCDfffHOJnr6LR7kA5q6hWPmHeZ9ELNw9CoUZJfeuNqBmYiKO8C8GN/Thw4dx2WWXYcOGDWG/cSETLfSdO3dqr/AAsHz5ctSqVQtAQJBF8XOaBU8WzkaNGun6XmvWrImCggJX+dqB0hWmdurUKe3zyZMnY9gSfT799FNs2LAhJh2yTnn77bcNB3QpSh9xJ+i3ZWTgn/yLgfg9+6zxPNV6gi7DGNMsQ1nQndK3b1+MHDlSG07N812bta2sc1hwq0Vj2r9ISU9PD0tkVVIRDRVF6SfuXC49gkn1Uzt0AAymXTODi6ZVrLfYKRpJburk5OSwzIdeUVpD0sRzozfJhUKh0CfuBJ2zYupUMBcTxdqx0IFQQVeWsz0uvvhiT+oR8+woQVco7BN3gs6FdkFODhqsWeN4/VWrVuHjjz8O8dOKcH+jOIS+pAp6x44dY92EEFatWmVr6jQrxNGgdjt7FQqFDUEnon8R0REi+sngdyKiyUS0m4i2EFF775t5Di7oL/z6K/Y5nJKOc+utt6Jnz566v/GIFi74kbpcREaOHOlJPZzRo0dj7dq1UU8SZZeEhARPjpU40lXOQaNQKIyxY6G/B6CXye+9ATQJ/o0C8HbkzbImj4ezNW3qab08tpgn2Iq0U5TDGAvxpe/YsQPr1q2LqE4iQseOHUvdZL2DBw/WPsciV4tCEa9YCjpjbDmAP0yK9AcwiwX4DkAVIjrPqwbqtCd0wfjxntYvv+J7JegyTZs2RWZmpuf1KhSKsosXPvQ6APYL3w8El0WFZdy9wCM8PHKHcGRB99LlolAoFNGkWDtFiWgUEa0novVHjx51Vcfs337jlQX+16wJeNhxJo/+a9myZYntFFUoFAoRLwT9IIB6wve6wWVhMMamM8YyGWOZbkevHRWH/nOE+TYjRbTQy5Urh0mTJml+9eJOQ6tQKBRO8ELQPwcwJBjtcjGAXMbYYauV3FJDz1r2MOOeKOh9+vTR0uUuXrwYa9eu9Ww7CoVC4TWWvgQi+hhADwDViegAgKcBJAEAY2wqgK8B9AGwG8BpAPazS7ng1owMvB5o2LmFHgq6UX6UK664wrNtKKxZtWqVcnUpFA6xvGMYY4MsfmcA7vKsRRZ0r1wZryPwaqElT7Uh6OPGjcOECRMsy4m5Q0pTwqt4o2vXrrFugkIRd8TtSNEJF1yA8lzIbQj6888/H7ZMnPmGky8MVlIWokKhiCfiTtA5/apXx/RmzVA/JcUwja4VVrnKlYWuUCjiibgTdHFg0eCMDOzt0gXPu5jbsWnTpiFDzDn33HOP9lkJukKhiCfiTtBbtWqFF154ASuI0GDNGiQsW4aJBw6YrqM30cXf/va3kNmH+ATMqamp6NKlCwAl6AqFIr6IO0Fv0aIFzh8+HPcdPYp9+flgAI5ZzGCvN5lvtWrVAAA//PAD9uzZg+rVqwMI5C/nEyxY5UxXKBSKkkTcCToA3LdzJ0KGFxnMDM/hkwlv2LAB559/Pt577z0MHz4cANC2bVs0atQIs2bNwqRJk9CmTRvs3bsXAPDdd99FofUKhUIRHeIyjCNHtpx1LHARnl62ffv22Ldvn26ZmjVr4v777w9ZpnK4KBSKeCIuLfQwgkPzjXA6EUTt2rUBALm5ua6bpFAoFMVNqRV00W8udn7agQ/xV7PlKBSKeCIuXS7piYnIEac6kwT9zz//hM/nQ6VKlVzVX6dOHbz11lvo52HSL4VCoYg2cWmhvy5nPZQEvUqVKkhLS8Mdd9yBhQsXutrGmDFjUK9ePeuCCoVCUUKISwt9cEYG/rpt27kF55+vW05vaL9CoVCUVuLSQgeAkPgTaQDQnTt3FmtbFAqFoiQQt4I+KhiJosfUQ4cwOzu7GFujUCgUsSduBf2tpk31f3juOTAA437+uVjbo1AoFLEmbgUdMGh8ZiYA4FchDa5CoVCUBeJa0P16C4OjO6upUZ4KhaKMEdeCrktQyHOKilTnqEKhKFPYEnQi6kVEO4hoNxE9pvP7MCI6SkSbgn8jvG9qOOl6VrgwaYXqHFUoFGUJS0EnIh+ANwH0BnAhgEFEdKFO0bmMsbbBv3c9bqcurxt1jAZRnaMKhaIsYcdC7wRgN2PsZ8bYWQBzAPSPbrPsMTgjw7LMPtU5qlAoygh2BL0OgP3C9wPBZTI3ENEWIppPRLpj5oloFBGtJ6L1R48eddHccDSny9SpwJgxumWU20WhUJQFvOoU/QJAA8ZYawDfAHhfrxBjbDpjLJMxllmjRg1PNqxlRm/WDBg4ULfM8O3blagrFIpSjx1BPwhAtLjrBpdpMMZyGGPct/EugA7eNM+a+hazFQHAWcaUL12hUJR67Aj6OgBNiKghESUDuAXA52IBIjpP+HotgG0oJiY0amSr3L78fGWlKxSKUo2loDPGCgHcDWAhAkL9b8ZYFhE9S0TXBovdS0RZRLQZwL0AhkWrwTKDMzJs+43+um0bqq9cqYRdoVCUSogxFpMNZ2ZmsvXr13tS1507d+LtQ4dsl08mwr+aNwcQCGv8NT8f56ekYEKjRrYiZxQKhSJWENEGxlim3m9xmQ9d5q2mTfFBdjZOypNHG3CWMQzftg2JCQk47Q8kENiXn49RO3YAsBcOqVAoFCWNUjP0f2rTpkiyLqZxFtDEnHPa71edpwqFIm4pNYI+OCMDM1u0iHiH9uXno8GaNcrPbsLs7Gw0WLMGCcuWqWOlUJQgSo2gAwFRn9WiRcT17MvPx1+3bUPaihWuRau0it7s7GyM2rED+/LzwXDOVVVa9k+hiGdKRaeoDC1bFpV6jUhPTMTAmjXxdU4Ofs3PRzWfDyf8fpwVjm35hARMb9ZM1z8/OzvbUees0/Je0mDNGt10CvWD7SgJncxGxyeS41Ycxzxa24jl9eKUeGprrDDrFC2Vgl59xQrk2OwgLU7SfT783r07gHMX7r78fBACicRk6utc0NxCFv3/8sNCvin6pKdrDxu973o3jVhHNZ8PIMIfhYW67eQkEqFQup4SEMhbz/cF8DaySG9f3//tt7DjM7RWrbDlQOBh/HqTJpYPUL1jPrRWLcvjqNfOaj4f8vx+nAoeqwpEAKB9F7dhZASYHQOxHXauFzv1eInZA9duW+MRr45vmRP02dnZuH3bNhREpfbI4Na8nrgYkQygEAYTevB6fT5UTEw0fUAYwctz0V2Vm4uphw45qsMOSQCIyPDNxc2biiwARvggpImQsBI4owRv8nEmAKNr1w6bHtFJO83aXoEIZxgLuQ7Sdd4GkwBUSkzEH4WFIOhfN77g8vOFB61eG3tWqYJFbdu6ajcQ2Pf7du7UDKwKRCgAwq6BLpUqYfGxY7p11E9Jwd4uXVy3wahdbsRV3h9uEADhhoq4zOlbuxllTtCB8AOvKLmYPYDMbhgzsXWK+PYERCbCYyRRT1uxwnZIbSwgABV8PsM2yvvDsRJFrwwrAuDv0cN02/ytc19+vvYArG/wNmpmsMhvbHZ0RO/61TNe9HDzsCqTgs65c+fOqFibitIJf4OafuiQoUVvuy6fDyeKinDWk5bFlg9btNAEtDxRmHtIpqLPhxQi5BQWetaG+ikpaFyuHJYcOxb1+9nIDRYNmPSwsqJMCzpg/eqsUCgUsUDv7cNyHRNBL1Vhi0YMzsjA3i5dMKZ27Vg3RaFQKDS8NqfLhKBz3mraVIm6QqEotZQpQQcCov5hixYon1Dmdl2hUJQw0hO9TadVJlVtcEYGpjdrhvopKSAEOq/4geVT2tVPSUHPKlVi1kaFQlH64RFcXlEqsi26YXBGhuNRgmYhUdV8PhUiqVAoHLEqN9fTQVNlVtCdYFf8q69c6WmYlkKhKN1MPXQI3SpX9kzUy6TLJVq83qRJmG++fEICPmzRAqxHj7A/O/OhihACI/f00gSrE6lQxB8M8DRlt9IBD5F98/VTUkyH9k5o1CjsAZCEQEeJ6NvndX3QogUWtW2LmS1ahGzjwxYtUBR8SHzYogXSfT6tvmQYn2QK/q+fkoIxtWvDZ1DODhV9Pq09PatU0epWKBTm/Orh+BhbA4uIqBeA1xHoM3yXMfaS9HsKgFkAOgDIAXAzY2yvWZ3FObCoJFOc2eXsDNWW826k+nzICeYEka8Us8RWVgmj9PJe6OXI0MuvIrcRODeiT0wGJvdx8ARj/HNOYWFIn4hR8jC+TO5DMcuBIsLbpJd3xQx+zOVtisfE6fSLVvDj5iTXEIcbEqovyRlOh/9HNFKUiHwAdgK4EsABAOsADGKMbRXK3AmgNWNsNBHdAuB6xtjNZvUqQY8vVFpTY5wcG6cPOicd91ZJo+Qsj3YeyOJDrILJkH85yZqdPDiiwcDxARgl5I6xmw+GJ0bj+VycYmW8iFT0+TA12D67I9B5AjI5bYGbBF2RCnoXAOMZY1cHv/8dABhjLwplFgbLrCGiRAC/AajBTCpXgq5QxB9GEV96b3t2ytndpvywEucfsErBK2afdPJQE/fD7sNa70EmbsMLwyhSQb8RQC/G2Ijg99sAdGaM3S2U+SlY5kDw+55gmd+lukYBGAUA559/fod9+/Y52hGFQqGwIpZvk8WxbTNBL9awRcbYdPx/e/cXYlUVxXH8+6MpJQOdqYhJJZWkGIJSopR6iAoziXopUIKGGuilyCIIhyCptyAygxClLJCwyKRkkKRGn6eUwvybI0YqmkpmEAQKq4e9rl7nT94/M3M6+64PXOaefTbcve4a1r1nn3PPhnWQvqFP5GuHEFpDPb8xyem1obarXI4DM6u2Z3jbiH18ymUq6eRoCCGECVJLQf8BmCtptqRrgKXAliF9tgDd/vxJYPt/zZ+HEEIYe1eccjGzC5JeBLaRTkKvN7O9kt4CdprZFuAjYIOkQeAPUtEPIYQwgWqaQzezrcDWIW1vVD3/B3hqbIcWQgihHvFL0RBCyERhS9BJOg00et3iDcCZK/bKS8TcGiLm1tBMzLeY2Y0j7SisoDdD0s7RrsPMVcTcGiLm1jBeMceUSwghZCIKegghZKKsBX1d0QMoQMTcGiLm1jAuMZdyDj2EEMJwZf2GHkIIYYgo6CGEkInSFXRJiyUdlDQoaUXR4xkrkmZK2iFpn6S9kpZ7e4ekbyUd8r/t3i5J7/v7sFvS/GIjaIykqyT9KKnPt2dLGvC4Pvf7ByFpkm8P+v5ZRY67GZKmSdok6YCk/ZIW5pxnSa/4//QeSRslTc4xz5LWSzrltxOvtNWdV0nd3v+QpO6RXms0pSrovnrSB8CjQBewTFJXsaMaMxeAV82sC1gAvOCxrQD6zWwu0O/bkN6Duf54Hlgz8UMeE8uB/VXbbwOrzOxW4CzQ4+09wFlvX+X9ymo18I2Z3Q7cSYo/yzxLmg68BNxtZneQ7ge1lDzz/AmweEhbXXmV1AGsBO4F7gFWVj4EamJmpXkAC4FtVdu9QG/R4xqnWL8mLft3EOj0tk7goD9fS1oKsNL/Yr+yPEi3Yu4HHgT6SCuJnQHahuabdHO4hf68zfup6BgaiHkqcGTo2HPNMzAdOAp0eN76gEdyzTMwC9jTaF6BZcDaqvbL+l3pUapv6Fz656g45m1Z8cPMecAAcJOZnfBdJ4HK3fNzeC/eA14jraEMcD3wp5lVFpmsjulivL7/nPcvm9nAaeBjn2r6UNIUMs2zmR0H3gF+A06Q8raL/PNcUW9em8p32Qp69iRdB3wJvGxmf1Xvs/SRncV1ppIeA06Z2a6ixzLB2oD5wBozmwf8zaXDcCC7PLcDT5A+yG4GpjB8WqIlTERey1bQa1k9qbQkXU0q5p+a2WZv/l1Sp+/vBE55e9nfi/uAxyX9CnxGmnZZDUzzVa/g8phyWRXrGHDMzAZ8exOpwOea54eBI2Z22szOA5tJuc89zxX15rWpfJetoNeyelIpSRJpoZD9ZvZu1a7q1aC6SXPrlfZn/Gz5AuBc1aHd/56Z9ZrZDDObRcrjdjN7GthBWvUKhsdb+lWxzOwkcFTSbd70ELCPTPNMmmpZIOla/x+vxJt1nqvUm9dtwCJJ7X50s8jbalP0SYQGTjosAX4BDgOvFz2eMYzrftLh2G7gJ38sIc0f9gOHgO+ADu8v0hU/h4GfSVcRFB5Hg7E/APT58znA98Ag8AUwydsn+/ag759T9LibiPcuYKfn+iugPec8A28CB4A9wAZgUo55BjaSzhOcJx2J9TSSV+A5j38QeLaeMcRP/0MIIRNlm3IJIYQwiijoIYSQiSjoIYSQiSjoIYSQiSjoIYSQiSjoIYSQiSjoIYSQiX8BHJbylSxMeHAAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./content/drive/My Drive/new', exist_ok=True)\n",
        "model.save('./content/drive/My Drive/new/UNfreeze2Class.h5')"
      ],
      "metadata": {
        "id": "f_Xlg2XSuT9L"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "HuBli3HajIR2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"./content/drive/My Drive/new/UNfreeze2Class.h5\")"
      ],
      "metadata": {
        "id": "zblzwvRGkSCR",
        "outputId": "2a0857b0-db0b-46b1-f550-43d3181ae218",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2297ec22-136c-4d23-ad3f-1a9bbd656208\", \"UNfreeze2Class.h5\", 19737224)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MwNk3sNbc2Qx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}